<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>运维人</title>
  
  
  <link href="https://www.langxw.com/atom.xml" rel="self"/>
  
  <link href="https://www.langxw.com/"/>
  <updated>2024-06-26T08:10:54.555Z</updated>
  <id>https://www.langxw.com/</id>
  
  <author>
    <name>运维人</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>K8S证书更新</title>
    <link href="https://www.langxw.com/2024/06/26/K8S%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/"/>
    <id>https://www.langxw.com/2024/06/26/K8S%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/</id>
    <published>2024-06-26T07:57:19.000Z</published>
    <updated>2024-06-26T08:10:54.555Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>公司自建的K8S集群证书过期，导致集群无法提供服务。首先需要确认一下内容：</p><ul><li><p>K8s 版本：V1.20</p></li><li><p>部署方式：kubeadm</p></li><li><p>部署架构：几个master节点+几个worker几点</p></li></ul><h2 id="二、操作步骤"><a href="#二、操作步骤" class="headerlink" title="二、操作步骤"></a>二、操作步骤</h2><h3 id="1、查看证书过期情况"><a href="#1、查看证书过期情况" class="headerlink" title="1、查看证书过期情况"></a>1、查看证书过期情况</h3><p><code>kubeadm certs check-expiration</code></p><h3 id="2、备份-etc-kubernetes"><a href="#2、备份-etc-kubernetes" class="headerlink" title="2、备份/etc/kubernetes/"></a>2、备份/etc/kubernetes/</h3><pre><code class="bash">cp -rp /etc/kubernetes /etc/kubernetes.bakcp -r /var/lib/etcd /var/lib/etcd-$(date +%Y%m%d).bakcp -r ~/.kube/config ~/.kube/config.bak</code></pre><h3 id="3、生成新的证书"><a href="#3、生成新的证书" class="headerlink" title="3、生成新的证书"></a>3、生成新的证书</h3><h4 id="3-1-方法1：（默认选择）"><a href="#3-1-方法1：（默认选择）" class="headerlink" title="3.1 方法1：（默认选择）"></a>3.1 方法1：（默认选择）</h4><p><code>kubeadm certs renew all #  使用该命令不用提前删除过期证书</code></p><h4 id="3-2-方法2：（单个更新）"><a href="#3-2-方法2：（单个更新）" class="headerlink" title="3.2 方法2：（单个更新）"></a>3.2 方法2：（单个更新）</h4><pre><code class="bash">kubeadm certs renew apiserver kubeadm certs renew admin.conf kubeadm certs renew apiserver-etcd-clientkubeadm certs renew apiserver-kubelet-clientkubeadm certs renew controller-manager.confkubeadm certs renew etcd-healthcheck-clientkubeadm certs renew etcd-peerkubeadm certs renew etcd-serverkubeadm certs renew front-proxy-client  kubeadm certs renew scheduler.conf</code></pre><p><strong>如果你运行了一个 HA 集群，这个命令需要在所有控制面板节点上执行。</strong><br>(除了kubelet.conf都会更新生成)</p><h3 id="4、验证集群状态"><a href="#4、验证集群状态" class="headerlink" title="4、验证集群状态"></a>4、验证集群状态</h3><h4 id="4-1-验证集群证书有效期是否更新（等待5分钟再查看）："><a href="#4-1-验证集群证书有效期是否更新（等待5分钟再查看）：" class="headerlink" title="4.1 验证集群证书有效期是否更新（等待5分钟再查看）："></a>4.1 验证集群证书有效期是否更新（等待5分钟再查看）：</h4><p><code>echo | openssl s_client -showcerts -connect 127.0.0.1:6443 -servername api 2&gt;/dev/null | openssl x509 -noout -enddate</code><br>如果证书有效期一直未更新，需要重启控制面的服务（详见步骤6）<br><code>kubeadm certs check-expiration</code></p><h4 id="4-2-验证集群状态"><a href="#4-2-验证集群状态" class="headerlink" title="4.2 验证集群状态"></a>4.2 验证集群状态</h4><ul><li>查询状态</li></ul><p><code>kubectl get nodes</code><br><code>kubectl get pods -A</code></p><ul><li><p>重启pod验证<br>kubectl rollout restart deployment <deployment-name> -n <namespace></p></li><li><p>业务验证</p></li></ul><p><strong>如果集群状态正常，服务正常，则下面的操作应该都不用进行下去。</strong></p><h3 id="5、生成配置文件（可能不需要）"><a href="#5、生成配置文件（可能不需要）" class="headerlink" title="5、生成配置文件（可能不需要）"></a>5、生成配置文件（可能不需要）</h3><h4 id="5-1-方法1：（默认选择）"><a href="#5-1-方法1：（默认选择）" class="headerlink" title="5.1 方法1：（默认选择）"></a>5.1 方法1：（默认选择）</h4><p><code>kubeadm init phase kubeconfig all</code></p><h4 id="5-2-方法2："><a href="#5-2-方法2：" class="headerlink" title="5.2 方法2："></a>5.2 方法2：</h4><p>// 重新生成 admin 配置文件<br><code>kubeadm init phase kubeconfig admin</code><br>// 重新生成 kubelet 配置文件<br><code>kubeadm init phase kubeconfig kubelet</code></p><h3 id="6、重启服务（可能不需要）"><a href="#6、重启服务（可能不需要）" class="headerlink" title="6、重启服务（可能不需要）"></a>6、重启服务（可能不需要）</h3><pre><code class="bash">cd /etc/kubernetes/ &amp;&amp; mv manifests/* /tmp/mv /tmp/kube-* /etc/kubernetes/manifests/ &amp;&amp; mv /tmp/etcd.yaml /etc/kubernetes/manifests/</code></pre><h3 id="7、重启kubelet（可能不需要）"><a href="#7、重启kubelet（可能不需要）" class="headerlink" title="7、重启kubelet（可能不需要）"></a>7、重启kubelet（可能不需要）</h3><p><code>systemctl restart kubelet &amp;&amp; systemctl status kubelet</code></p><h3 id="8、更新集群连接配置（视情况而定）"><a href="#8、更新集群连接配置（视情况而定）" class="headerlink" title="8、更新集群连接配置（视情况而定）"></a>8、更新集群连接配置（视情况而定）</h3><p><code>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></p><h3 id="9、其他办法将证书时间调整为10年"><a href="#9、其他办法将证书时间调整为10年" class="headerlink" title="9、其他办法将证书时间调整为10年"></a>9、其他办法将证书时间调整为10年</h3><p>需要修改kubeadm源代码，待测试赛</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、背景&quot;&gt;&lt;a href=&quot;#一、背景&quot; class=&quot;headerlink&quot; title=&quot;一、背景&quot;&gt;&lt;/a&gt;一、背景&lt;/h2&gt;&lt;p&gt;公司自建的K8S集群证书过期，导致集群无法提供服务。首先需要确认一下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;K8s 版本：V</summary>
      
    
    
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
    <category term="证书" scheme="https://www.langxw.com/tags/%E8%AF%81%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.langxw.com/2024/04/03/hello-world/"/>
    <id>https://www.langxw.com/2024/04/03/hello-world/</id>
    <published>2024-04-03T04:55:50.032Z</published>
    <updated>2021-01-29T02:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Nacos集群搭建</title>
    <link href="https://www.langxw.com/2024/04/03/Nacos%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>https://www.langxw.com/2024/04/03/Nacos%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</id>
    <published>2024-04-03T04:55:50.020Z</published>
    <updated>2021-02-01T06:53:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、预备环境说明："><a href="#一、预备环境说明：" class="headerlink" title="一、预备环境说明："></a>一、预备环境说明：</h2><ol><li>64位JDK ，这里使用 JDK 1.8.0_261</li><li>下载编译好的压缩包，nacos-server-1.4.0.tar.gz</li><li>准备三台主机A(192.168.1.1)、B(192.168.1.2)、C(192.168.1.3)</li><li>准备高可用Nginx集群、MySQL集群</li></ol><h2 id="二、集群架构："><a href="#二、集群架构：" class="headerlink" title="二、集群架构："></a>二、集群架构：</h2><pre class="mermaid">graph LRA[微服务] --> B(Nginx集群)    B --> D[Nacos1]    B --> E[Nacos2]    B --> F[Nacos3]    D --> G(MySQL集群)    E --> G    F --> G</pre><ul><li><p><strong>高可用Nginx集群</strong></p></li><li><p><strong>高可用Nacos集群（至少三个实例）</strong></p></li><li><p><strong>高可用MySQL集群</strong></p></li></ul><h2 id="三、部署步骤："><a href="#三、部署步骤：" class="headerlink" title="三、部署步骤："></a>三、部署步骤：</h2><h3 id="1、解压："><a href="#1、解压：" class="headerlink" title="1、解压："></a>1、解压：</h3><p><code>tar -zxvf nacos-server-1.4.0.tar.gz -C /data</code></p><h3 id="2、导入数据文件："><a href="#2、导入数据文件：" class="headerlink" title="2、导入数据文件："></a>2、导入数据文件：</h3><p>创建数据库nacos_test，将<code>/data/nacos/conf/nacos-mysql.sql</code>导入数据库，并创建应用程序访问账号。</p><h3 id="3、修改数据源："><a href="#3、修改数据源：" class="headerlink" title="3、修改数据源："></a>3、修改数据源：</h3><p>这里我们使用外置MySQL集群作为数据存储。</p><p>修改<code>conf/application.properties</code>中关于MySQL数据源的配置。</p><pre><code class="yaml">#*************** Config Module Related Configurations ***************#### If use MySQL as datasource:spring.datasource.platform=mysql### Count of DB:db.num=1### Connect URL of DB:db.url.0=jdbc:mysql://192.168.1.1:3306/nacos_test?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user=nacos_test_rwdb.password=POUZV#Of</code></pre><h3 id="4、修改集群配置："><a href="#4、修改集群配置：" class="headerlink" title="4、修改集群配置："></a>4、修改集群配置：</h3><p>修改<code>conf/cluster.conf</code>文件的末尾关于IP和端口配置（若没有cluster.conf就用cluster.conf.example复制）</p><pre><code>192.168.1.1:8848192.168.1.2:8848192.168.1.3:8848</code></pre><p>这里写上集群内三台主机的IP和nacos监听端口</p><h3 id="5、修改内存占用："><a href="#5、修改内存占用：" class="headerlink" title="5、修改内存占用："></a>5、修改内存占用：</h3><p>如果机器内存本身不大，那么不建议使用默认配置，根据具体情况修改<code>nacos/bin/startup.sh</code>中关于java内存占用的配置，</p><p>比如将集群的内存占用从2G修改为512M（standalone配置可以保持不变），集群模式是else中内容：</p><pre><code class="shell">#===========================================================================================# JVM Configuration#===========================================================================================if [[ &quot;$&#123;MODE&#125;&quot; == &quot;standalone&quot; ]]; then    JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -Xms512m -Xmx512m -Xmn256m&quot;    JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -Dnacos.standalone=true&quot;else    if [[ &quot;$&#123;EMBEDDED_STORAGE&#125;&quot; == &quot;embedded&quot; ]]; then        JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -DembeddedStorage=true&quot;    fi    JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;    JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:-OmitStackTraceInFastThrow -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;BASE_DIR&#125;/logs/java_heapdump.hprof&quot;</code></pre><h3 id="6、启动："><a href="#6、启动：" class="headerlink" title="6、启动："></a>6、启动：</h3><p>依次启动，先启动的是主节点。</p><p>执行<code>bin/startup.sh</code>即可，默认是集群模式。</p><p>启动后可以查看日志文件和端口监听来确定集群是否启动成功 <code>tail -f nacos/logs/start.out</code>。</p><p>注意：<code>./startup.sh -m standalone</code> 是启动单实例模式。</p><h3 id="7、配置Nginx代理Upstream"><a href="#7、配置Nginx代理Upstream" class="headerlink" title="7、配置Nginx代理Upstream"></a>7、配置Nginx代理Upstream</h3><p>修改Nginx配置文件default.conf，加入负载均衡和反向代理的配置。</p><pre><code class="shell"># 在server外部upstream nacos-cluster &#123;  server 192.168.1.1:8848;  server 192.168.1.2:8848;  server 192.168.1.3:8848;&#125;# 在server内部.location / 之前server &#123;    # 省略。。。。。    location /nacos &#123;        proxy_redirect off;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_pass http://nacos-cluster;    &#125;&#125;</code></pre><p><strong>注意：nginx upstream 的name中不要带下划线，不然springboot会报400错误。</strong></p><h3 id="8、验证："><a href="#8、验证：" class="headerlink" title="8、验证："></a>8、验证：</h3><p>通过Nginx代理来访问整个集群（建议Nginx集群使用一个域名来访问）。</p><p>可通过<a href="http://192.168.1.1/nacos%E8%AE%BF%E9%97%AEnacos%E9%9B%86%E7%BE%A4%EF%BC%8C%E9%BB%98%E8%AE%A4%E5%B8%90%E5%8F%B7nacos%E3%80%81%E5%AF%86%E7%A0%81nacos%E3%80%82">http://192.168.1.1/nacos访问nacos集群，默认帐号nacos、密码nacos。</a></p><p>或者使用域名访问<a href="http://nacos.xxx.com/nacos%E3%80%82">http://nacos.xxx.com/nacos。</a></p><h2 id="四、其他"><a href="#四、其他" class="headerlink" title="四、其他"></a>四、其他</h2><ol><li><a href="https://nacos.io/zh-cn/docs/what-is-nacos.html">Nacos官方文档 https://nacos.io/zh-cn/docs/what-is-nacos.html</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、预备环境说明：&quot;&gt;&lt;a href=&quot;#一、预备环境说明：&quot; class=&quot;headerlink&quot; title=&quot;一、预备环境说明：&quot;&gt;&lt;/a&gt;一、预备环境说明：&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;64位JDK ，这里使用 JDK 1.8.0_261&lt;/li&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="中间件" scheme="https://www.langxw.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="Nacos" scheme="https://www.langxw.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Nacos/"/>
    
    
    <category term="nacos" scheme="https://www.langxw.com/tags/nacos/"/>
    
  </entry>
  
  <entry>
    <title>K8s 1.28版本二进制部署</title>
    <link href="https://www.langxw.com/2024/03/13/K8s-1-28%E7%89%88%E6%9C%AC%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.langxw.com/2024/03/13/K8s-1-28%E7%89%88%E6%9C%AC%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/</id>
    <published>2024-03-13T10:49:37.000Z</published>
    <updated>2024-03-13T10:56:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、高可用架构"><a href="#一、高可用架构" class="headerlink" title="一、高可用架构"></a>一、高可用架构</h2><h3 id="1、架构图"><a href="#1、架构图" class="headerlink" title="1、架构图"></a>1、架构图</h3><p><img src="https://s2.loli.net/2024/03/12/7qYdlwvnbEOCI2p.jpg" alt="K8s高可用架构png"></p><h3 id="2、架构说明"><a href="#2、架构说明" class="headerlink" title="2、架构说明"></a>2、架构说明</h3><p>k8s分为Master节点和Node节点，Master节点是控制节点，Node节点是工作节点。其中，Master节点包含kube-apiserver、kube-scheduler、kube-controller-manager、etcd、kubelet、kube-proxy；Node节点包含kubelet、kube-proxy。</p><p>关于Master节点和Node节点介绍如下:</p><ol><li><p>Master节点-控制节点<br>指容器编排层，它暴露 API 和接口来定义、 部署容器和管理容器的生命周期。管理集群中的工作节点和 Pod。在生产环境中，控制平面通常跨多台计算机运行， 一个集群通常运行多个节点，提供容错性和高可用性。</p></li><li><p>Node节点-工作节点<br>托管 Pod，而 Pod 就是作为应用负载的组件,是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。每个集群至少有一个工作节点。</p></li></ol><h3 id="3、组件介绍"><a href="#3、组件介绍" class="headerlink" title="3、组件介绍"></a>3、组件介绍</h3><h4 id="3-1-Master组件"><a href="#3-1-Master组件" class="headerlink" title="3.1 Master组件"></a>3.1 Master组件</h4><h5 id="3-1-1-kube-apiserver"><a href="#3-1-1-kube-apiserver" class="headerlink" title="3.1.1 kube-apiserver"></a>3.1.1 kube-apiserver</h5><p>API 服务器是 Kubernetes 控制平面的组件， 该组件负责公开了 Kubernetes API，负责处理接受请求的工作。 API 服务器是 Kubernetes 控制平面的前端。Kubernetes API 服务器的主<br>要实现是 kube-apiserver。 <code>kube-apiserver</code> 设计上考虑了水平扩缩，也就是说，它可通过部署多个实例来进行扩缩。 你可以运行 <code>kube-apiserver</code> 的多个实例，并在这些实例之间平衡流量。</p><h5 id="3-1-2-etcd"><a href="#3-1-2-etcd" class="headerlink" title="3.1.2 etcd"></a>3.1.2 etcd</h5><p>一致且高可用的键值存储，用作 Kubernetes 所有集群数据的后台数据库。一般可以安装在Master节点上，如果资源充足，也可安装到单独的服务器上。</p><h5 id="3-1-3-kube-scheduler"><a href="#3-1-3-kube-scheduler" class="headerlink" title="3.1.3 kube-scheduler"></a>3.1.3 kube-scheduler</h5><p><code>kube-scheduler</code> 是控制平面的组件， 负责监视新创建的、未指定运行节点node的 Pods， 并选择节点来让 Pod 在上面运行。</p><p>调度决策考虑的因素包括单个 Pod 及 Pods 集合的资源需求、软硬件及策略约束、 亲和性及反亲和性规范、数据位置、工作负载间的干扰及最后时限。</p><h5 id="3-1-4-kube-controller-manager"><a href="#3-1-4-kube-controller-manager" class="headerlink" title="3.1.4 kube-controller-manager"></a>3.1.4 kube-controller-manager</h5><p>kube-controller-manager 是控制平面的组件， 负责运行控制器进程。</p><p>从逻辑上讲， 每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在同一个进程中运行。</p><p>这些控制器包括：</p><ul><li>节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应</li><li>任务控制器（Job Controller）：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成</li><li>端点分片控制器（EndpointSlice controller）：填充端点分片（EndpointSlice）对象（以提供 Service 和 Pod 之间的链接）。</li><li>服务账号控制器（ServiceAccount controller）：为新的命名空间创建默认的服务账号（ServiceAccount）</li></ul><h5 id="3-1-5-kubelet"><a href="#3-1-5-kubelet" class="headerlink" title="3.1.5 kubelet"></a>3.1.5 kubelet</h5><p><code>kubelet</code> 会在集群中每个节点(node)上运行。 它保证容器containers都运行在 Pod 中。这里之所以放在Master节点组件里面，客观上说Master也属于node。</p><p>kubelet 接收一组通过各类机制提供给它的 PodSpecs， 确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。</p><h5 id="3-1-6-kube-proxy"><a href="#3-1-6-kube-proxy" class="headerlink" title="3.1.6 kube-proxy"></a>3.1.6 kube-proxy</h5><p>kube-proxy是集群中每个节点(node)上所运行的网络代理， 实现 Kubernetes 服务(Service)概念的一部分。</p><p>kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。</p><p>如果操作系统提供了可用的数据包过滤层，则 kube-proxy 会通过它来实现网络规则。 否则，kube-proxy 仅做流量转发。</p><h4 id="3-2-Node组件"><a href="#3-2-Node组件" class="headerlink" title="3.2 Node组件"></a>3.2 Node组件</h4><h5 id="3-2-1-kubelet"><a href="#3-2-1-kubelet" class="headerlink" title="3.2.1 kubelet"></a>3.2.1 kubelet</h5><p><code>kubelet</code> 会在集群中每个节点(node)上运行。 它保证容器containers都运行在 Pod 中。这里之所以放在Master节点组件里面，客观上说Master也属于node。</p><p>kubelet 接收一组通过各类机制提供给它的 PodSpecs， 确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。</p><h5 id="3-2-2-kube-proxy"><a href="#3-2-2-kube-proxy" class="headerlink" title="3.2.2 kube-proxy"></a>3.2.2 kube-proxy</h5><p>kube-proxy是集群中每个节点(node)上所运行的网络代理， 实现 Kubernetes 服务(Service)概念的一部分。</p><p>kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。</p><p>如果操作系统提供了可用的数据包过滤层，则 kube-proxy 会通过它来实现网络规则。 否则，kube-proxy 仅做流量转发。</p><h3 id="4、k8s高可用架构分析"><a href="#4、k8s高可用架构分析" class="headerlink" title="4、k8s高可用架构分析"></a>4、k8s高可用架构分析</h3><p>这里负载均衡采用KeepAlived和HAProxy实现高可用，当然了，你也可以使用公有云的负载均衡和F5实现高可用。当采用KeepAlived和HAProxy实现高可用时，会虚拟出一个VIP，用于组件之间的交互。</p><p>当kube-scheduler、kube-controller-manager、kubelet、kube-proxy组件想访问kube-apiserver时，必须要经过负载均衡再进行访问kube-apiserver，从而实现了高可用。</p><p>针对etcd组件，只有kube-apiserver组件才能与其进行交互，kube-scheduler、kube-controller-manager、kubelet、kube-proxy组件不能直接与etcd组件进行交互。</p><h2 id="二、二进制部署"><a href="#二、二进制部署" class="headerlink" title="二、二进制部署"></a>二、二进制部署</h2><h3 id="2-1-环境"><a href="#2-1-环境" class="headerlink" title="2.1 环境"></a>2.1 环境</h3><h4 id="2-1-1-高可用kubernetes集群规划"><a href="#2-1-1-高可用kubernetes集群规划" class="headerlink" title="2.1.1 高可用kubernetes集群规划"></a>2.1.1 高可用kubernetes集群规划</h4><table><thead><tr><th>主机名</th><th>IP地址</th><th>说明</th></tr></thead><tbody><tr><td>k8s-master01</td><td>10.200.16.10</td><td>master节点</td></tr><tr><td>k8s-master02</td><td>10.200.16.11</td><td>master节点</td></tr><tr><td>k8s-master03</td><td>10.200.16.12</td><td>master节点</td></tr><tr><td>k8s-worker01</td><td>10.200.16.13</td><td>worker节点</td></tr><tr><td>k8s-worker02</td><td>10.200.16.14</td><td>worker节点</td></tr><tr><td>k8s-worker03</td><td>10.200.16.2</td><td>worker节点</td></tr><tr><td>apiserver-lb</td><td>10.200.0.34</td><td>负载均衡IP</td></tr></tbody></table><h4 id="2-1-2-网段规划"><a href="#2-1-2-网段规划" class="headerlink" title="2.1.2 网段规划"></a>2.1.2 网段规划</h4><table><thead><tr><th>网段名称</th><th>网段划分</th></tr></thead><tbody><tr><td>宿主机网段</td><td>10.200.0.0/16</td></tr><tr><td>Pod网段</td><td>172.16.0.0/16</td></tr><tr><td>Service网段</td><td>10.100.0.0/16</td></tr></tbody></table><p><strong>注意事项：</strong></p><ol><li><p><strong>一般情况下，宿主机网段、K8s Service网段、Pod网段不能重复。</strong></p></li><li><p><strong>K8s Service网段比较特殊，必定不能和Pod网段或者宿主机网段重复。</strong></p></li><li><p>特殊情况下，宿主机网段和Pod网段可以重复（阿里云Terway组件）。</p></li></ol><h4 id="2-1-3-云主机规划"><a href="#2-1-3-云主机规划" class="headerlink" title="2.1.3 云主机规划"></a>2.1.3 云主机规划</h4><p>采用三Master三Worker+云负载均衡，4核8G磁盘100G</p><h4 id="2-1-4-配置信息"><a href="#2-1-4-配置信息" class="headerlink" title="2.1.4 配置信息"></a>2.1.4 配置信息</h4><table><thead><tr><th>配置信息</th><th>备注</th></tr></thead><tbody><tr><td>系统版本</td><td>Centos7.9</td></tr><tr><td>containerd版本</td><td>1.6.28</td></tr><tr><td>Kubernetes版本</td><td>v1.28.7</td></tr><tr><td>kubectl版本</td><td>v1.28.7</td></tr></tbody></table><h3 id="2-2-K8s基础环境配置"><a href="#2-2-K8s基础环境配置" class="headerlink" title="2.2 K8s基础环境配置"></a>2.2 K8s基础环境配置</h3><h4 id="1-所有节点安装常用软件包"><a href="#1-所有节点安装常用软件包" class="headerlink" title="1. 所有节点安装常用软件包"></a>1. 所有节点安装常用软件包</h4><ol><li>所有节点CentOS 7安装yum源如下</li></ol><pre><code class="bash">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repocurl -s -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.reposed -i -e &#39;/mirrors.cloud.aliyuncs.com/d&#39; -e &#39;/mirrors.aliyuncs.com/d&#39; /etc/yum.repos.d/CentOS-Base.repocurl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</code></pre><ol start="2"><li>所有节点安装常用的软件包 <code>yum -y install bind-utils expect rsync wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git ntpdate bash-completion</code></li></ol><h4 id="2-设置主机名"><a href="#2-设置主机名" class="headerlink" title="2. 设置主机名"></a>2. 设置主机名</h4><pre><code class="bash">hostnamectl set-hostname k8s-master01hostnamectl set-hostname k8s-master02hostnamectl set-hostname k8s-master03hostnamectl set-hostname k8s-worker01hostnamectl set-hostname k8s-worker02hostnamectl set-hostname k8s-worker03</code></pre><h4 id="3-关闭防火墙和Selinux"><a href="#3-关闭防火墙和Selinux" class="headerlink" title="3. 关闭防火墙和Selinux"></a>3. 关闭防火墙和Selinux</h4><pre><code class="bash">systemctl disable --now firewalld# systemctl disable --now NetworkManager (如果有就关闭)setenforce 0sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config</code></pre><h4 id="4-关闭交换分区"><a href="#4-关闭交换分区" class="headerlink" title="4. 关闭交换分区"></a>4. 关闭交换分区</h4><pre><code class="bash">sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstabswapoff -a &amp;&amp; sysctl -w vm.swappiness=free -h</code></pre><h4 id="5-时间同步"><a href="#5-时间同步" class="headerlink" title="5. 时间同步"></a>5. 时间同步</h4><ul><li>手动同步时区和时间</li></ul><pre><code class="bash">ln -svf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimentpdate ntp.aliyun.com</code></pre><ul><li>定期任务同步</li></ul><pre><code class="bash">echo &quot;*/5 * * * * /usr/sbin/ntpdate ntp.aliyun.com&quot; &gt; /var/spool/cron/rootcrontab -l</code></pre><h4 id="6-配置limit"><a href="#6-配置limit" class="headerlink" title="6. 配置limit"></a>6. 配置limit</h4><pre><code class="bash">cat &gt;&gt; /etc/security/limits.conf &lt;&lt;&#39;EOF&#39;* soft nofile 655360* hard nofile 131072* soft nproc 655350* hard nproc 655350* soft memlock unlimited* hard memlock unlimitedEOF</code></pre><h4 id="7-内核调优"><a href="#7-内核调优" class="headerlink" title="7. 内核调优"></a>7. 内核调优</h4><pre><code class="bash">cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf# 以下3个参数是containerd所依赖的内核参数net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1fs.may_detach_mounts = 1vm.overcommit_memory=1vm.panic_onoom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384net.ipv6.conf.all.disable_ipv6 = 0net.ipv6.conf.default.disable_ipv6 = 0net.ipv6.conf.lo.disable_ipv6 = 0net.ipv6.conf.all.forwarding = 1EOFsysctl --system# 这些是Linux系统的一些参数设置，用于配置和优化网络、文件系统和虚拟内存等方面的功能。以下是每个参数的详细解释：# # 1. net.ipv4.ip_forward = 1#    - 这个参数启用了IPv4的IP转发功能，允许服务器作为网络路由器转发数据包。# # 2. net.bridge.bridge-nf-call-iptables = 1#    - 当使用网络桥接技术时，将数据包传递到iptables进行处理。#   # 3. fs.may_detach_mounts = 1#    - 允许在挂载文件系统时，允许被其他进程使用。#   # 4. vm.overcommit_memory=1#    - 该设置允许原始的内存过量分配策略，当系统的内存已经被完全使用时，系统仍然会分配额外的内存。# # 5. vm.panic_on_oom=0#    - 当系统内存不足（OOM）时，禁用系统崩溃和重启。# # 6. fs.inotify.max_user_watches=89100#    - 设置系统允许一个用户的inotify实例可以监控的文件数目的上限。# # 7. fs.file-max=52706963#    - 设置系统同时打开的文件数的上限。# # 8. fs.nr_open=52706963#    - 设置系统同时打开的文件描述符数的上限。# # 9. net.netfilter.nf_conntrack_max=2310720#    - 设置系统可以创建的网络连接跟踪表项的最大数量。# # 10. net.ipv4.tcp_keepalive_time = 600#     - 设置TCP套接字的空闲超时时间（秒），超过该时间没有活动数据时，内核会发送心跳包。# # 11. net.ipv4.tcp_keepalive_probes = 3#     - 设置未收到响应的TCP心跳探测次数。# # 12. net.ipv4.tcp_keepalive_intvl = 15#     - 设置TCP心跳探测的时间间隔（秒）。# # 13. net.ipv4.tcp_max_tw_buckets = 36000#     - 设置系统可以使用的TIME_WAIT套接字的最大数量。# # 14. net.ipv4.tcp_tw_reuse = 1#     - 启用TIME_WAIT套接字的重新利用，允许新的套接字使用旧的TIME_WAIT套接字。# # 15. net.ipv4.tcp_max_orphans = 327680#     - 设置系统可以同时存在的TCP套接字垃圾回收包裹数的最大数量。# # 16. net.ipv4.tcp_orphan_retries = 3#     - 设置系统对于孤立的TCP套接字的重试次数。# # 17. net.ipv4.tcp_syncookies = 1#     - 启用TCP SYN cookies保护，用于防止SYN洪泛攻击。# # 18. net.ipv4.tcp_max_syn_backlog = 16384#     - 设置新的TCP连接的半连接数（半连接队列）的最大长度。# # 19. net.ipv4.ip_conntrack_max = 65536#     - 设置系统可以创建的网络连接跟踪表项的最大数量。# # 20. net.ipv4.tcp_timestamps = 0#     - 关闭TCP时间戳功能，用于提供更好的安全性。# # 21. net.core.somaxconn = 16384#     - 设置系统核心层的连接队列的最大值。# # 22. net.ipv6.conf.all.disable_ipv6 = 0#     - 启用IPv6协议。# # 23. net.ipv6.conf.default.disable_ipv6 = 0#     - 启用IPv6协议。# # 24. net.ipv6.conf.lo.disable_ipv6 = 0#     - 启用IPv6协议。# # 25. net.ipv6.conf.all.forwarding = 1#     - 允许IPv6数据包转发。</code></pre><h4 id="8-配置免密登录"><a href="#8-配置免密登录" class="headerlink" title="8. 配置免密登录"></a>8. 配置免密登录</h4><pre><code class="bash">yum install -y sshpassssh-keygen -f /root/.ssh/id_rsa -P &#39;&#39;export IP=&quot;10.200.16.11 10.200.16.12 10.200.16.13 10.200.16.14 10.200.16.2&quot;export SSHPASS=111111for HOST in $IP;do     sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $HOSTdone</code></pre><p>方便传输文件</p><h4 id="9-升级Linux内核至4-18以上"><a href="#9-升级Linux内核至4-18以上" class="headerlink" title="9. 升级Linux内核至4.18以上"></a>9. 升级Linux内核至4.18以上</h4><pre><code class="bash"># 安装最新的内核# 我这里选择的是稳定版kernel-ml   如需更新长期维护版本kernel-lt  yum -y --enablerepo=elrepo-kernel  install  kernel-ml# 查看已安装那些内核rpm -qa | grep kernel# 查看默认内核grubby --default-kernel# 若不是最新的使用命令设置grubby --set-default $(ls /boot/vmlinuz-* | grep elrepo)# 重启生效reboot</code></pre><h4 id="10-安装ipvasadm"><a href="#10-安装ipvasadm" class="headerlink" title="10. 安装ipvasadm"></a>10. 安装ipvasadm</h4><pre><code class="bash">yum install ipvsadm ipset sysstat conntrack libseccomp -ycat &gt;&gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOF ip_vsip_vs_rrip_vs_wrrip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipipEOFsystemctl restart systemd-modules-load.servicelsmod | grep -e ip_vs -e nf_conntrack#ip_vs_sh               16384  0#ip_vs_wrr              16384  0#ip_vs_rr               16384  0#ip_vs                 180224  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr#nf_conntrack          176128  1 ip_vs#nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs#nf_defrag_ipv4         16384  1 nf_conntrack#libcrc32c              16384  3 nf_conntrack,xfs,ip_vs# 参数解释## ip_vs# IPVS 是 Linux 内核中的一个模块，用于实现负载均衡和高可用性。它通过在前端代理服务器上分发传入请求到后端实际服务器上，提供了高性能和可扩展的网络服务。## ip_vs_rr# IPVS 的一种调度算法之一，使用轮询方式分发请求到后端服务器，每个请求按顺序依次分发。## ip_vs_wrr# IPVS 的一种调度算法之一，使用加权轮询方式分发请求到后端服务器，每个请求按照指定的权重比例分发。## ip_vs_sh# IPVS 的一种调度算法之一，使用哈希方式根据源 IP 地址和目标 IP 地址来分发请求。## nf_conntrack# 这是一个内核模块，用于跟踪和管理网络连接，包括 TCP、UDP 和 ICMP 等协议。它是实现防火墙状态跟踪的基础。## ip_tables# 这是一个内核模块，提供了对 Linux 系统 IP 数据包过滤和网络地址转换（NAT）功能的支持。## ip_set# 这是一个内核模块，扩展了 iptables 的功能，支持更高效的 IP 地址集合操作。## xt_set# 这是一个内核模块，扩展了 iptables 的功能，支持更高效的数据包匹配和操作。## ipt_set# 这是一个用户空间工具，用于配置和管理 xt_set 内核模块。## ipt_rpfilter# 这是一个内核模块，用于实现反向路径过滤，用于防止 IP 欺骗和 DDoS 攻击。## ipt_REJECT# 这是一个 iptables 目标，用于拒绝 IP 数据包，并向发送方发送响应，指示数据包被拒绝。## ipip# 这是一个内核模块，用于实现 IP 封装在 IP（IP-over-IP）的隧道功能。它可以在不同网络之间创建虚拟隧道来传输 IP 数据包。</code></pre><h4 id="11-修改网卡名称为eth0"><a href="#11-修改网卡名称为eth0" class="headerlink" title="11. 修改网卡名称为eth0"></a>11. 修改网卡名称为eth0</h4><p>如果网卡名称不是eth0需修改网卡名称</p><pre><code class="bash">1.使用命令 ip addr 查看 linux 是否有 th0 网卡2.vi /etc/sysconfig/network-scripts/ifcfg-ens33 将里面的 NAME 和 DEVICE 项修改为 eth0，ONBOOT需修改为yes3.重命名网卡名称cd /etc/sysconfig/network-scripts/mv ifcfg-ens33 ifcfg-eth04.编辑 /etc/default/grub，加入net.ifnames=0 biosdevname=0 到 GRUB_CMALINE_LINUX 变量中（注意空格别遗漏）5.执行命令： grub2-mkconfig -o /boot/grub2/grub.cfg ，重新生成GRUB配置并更新内核参数6.执行命令：reboot 重启系统7.使用命令 ip addr 查看 eth0 网卡是否已经变更成功</code></pre><h3 id="2-3-基本组件安装"><a href="#2-3-基本组件安装" class="headerlink" title="2.3 基本组件安装"></a>2.3 基本组件安装</h3><h4 id="1-安装containerd（所有节点）"><a href="#1-安装containerd（所有节点）" class="headerlink" title="1. 安装containerd（所有节点）"></a>1. 安装containerd（所有节点）</h4><ol><li>安装</li></ol><pre><code class="bash">yum -y install containerd.io</code></pre><ol start="2"><li>配置containerd所需的模块</li></ol><pre><code class="bash"># 1.临时手动加载模块modrobe -- overlaymodprobe -- br_netfilter# 2.开机自动加载所需的内核模块cat &gt; /etc/modules-load.d/containerd.conf &lt;&lt;EOFoverlaybr_netfilterEOF# 1. overlay：overlay是容器d默认使用的存储驱动，它提供了一种轻量级的、可堆叠的、逐层增量的文件系统。它通过在现有文件系统上叠加文件系统层来创建容器的文件系统视图。每个容器可以有自己的一组文件系统层，这些层可以共享基础镜像中的文件，并在容器内部进行修改。使用overlay可以有效地使用磁盘空间，并使容器更加轻量级。# 2. br_netfilter：br_netfilter是Linux内核提供的一个网络过滤器模块，用于在容器网络中进行网络过滤和NAT转发。当容器和主机之间的网络通信需要进行DNAT或者SNAT时，br_netfilter模块可以将IP地址进行转换。它还可以提供基于iptables规则的网络过滤功能，用于限制容器之间或容器与外部网络之间的通信。</code></pre><ol start="3"><li>修改containerd的配置文件</li></ol><pre><code class="bash"># 1.重新初始化containerd的配置文件containerdconfig default | tee /etc/containerd/config.toml # 2.修改Cgroup的管理者为systemd组件sed -ri &#39;s#(SystemdCgroup = )false#\1true#&#39; /etc/containerd/config.toml grep SystemdCgroup /etc/containerd/config.toml# 3.修改pause的基础镜像名称sed -i &#39;s#registry.k8s.io/pause:3.6#registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7#&#39; /etc/containerd/config.tomlgrep sandbox_image /etc/containerd/config.toml# 配置加速器（不是必须）mkdir /etc/containerd/certs.d/docker.io -pvcat &gt; /etc/containerd/certs.d/docker.io/hosts.toml &lt;&lt; EOFserver = &quot;https://docker.io&quot;[host.&quot;https://hub-mirror.c.163.com&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOF</code></pre><ol start="4"><li>所有节点启动containerd</li></ol><pre><code class="bash"># 1.启动containerd服务systemctl daemon-reload &amp;&amp; systemctl enable --now containerd# 2.查看containerd的版本ctr version# ctr是containerd的一个客户端工具。</code></pre><ol start="5"><li>配置crictl客户端连接的运行时位置</li></ol><pre><code class="bash"># 下载对应版本wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.28.0/crictl-v1.28.0-linux-amd64.tar.gztar -xf crictl-v1.28.0-linux-amd64.tar.gz -C /usr/local/bin/  cat &gt; /etc/crictl.yaml &lt;&lt;EOFruntime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: falseEOF# crictl 是 CRI 兼容的容器运行时命令行接口，可以使用它来检查和调试 Kubernetes 节点上的容器运行时和应用程序。crictl 则直接对应了命名空间 k8s.io，即”crictl image list“等同于“ctr -n=k8s.io image list“crictl --version</code></pre><h4 id="2-安装etcd组件"><a href="#2-安装etcd组件" class="headerlink" title="2. 安装etcd组件"></a>2. 安装etcd组件</h4><p>下载配置后，将软件包下发到所有节点（master节点）</p><pre><code class="bash"># 下载wget https://github.com/etcd-io/etcd/releases/download/v3.5.10/etcd-v3.5.10-linux-amd64.tar.gz# 解压，跳过1层目录，只解压etcd和etcdctltar -xf etcd-v3.5.10-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.10-linux-amd64/etcd&#123;,ctl&#125;# 验证etcdctl version# 下发软件包MasterNodes=&#39;k8s-master02 k8s-master03&#39;for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done</code></pre><h4 id="3-安装k8s组件"><a href="#3-安装k8s组件" class="headerlink" title="3. 安装k8s组件"></a>3. 安装k8s组件</h4><pre><code class="bash"># 在github挑选合适的二进制版本 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md#server-binarieswget https://dl.k8s.io/v1.28.7/kubernetes-server-linux-amd64.tar.gz# 解压，这个命令的作用是将kubernetes-server-linux-amd64.tar.gz文件中的kubelet、kubectl、kube-apiserver、kube-controller-manager、kube-scheduler# 和kube-proxy六个文件提取到/usr/local/bin目录下，同时忽略文件路径中的前三级目录结构。                   tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125;# 查看版本kubelet --version# 下发软件包到其他节点MasterNodes=&#39;k8s-master02 k8s-master03&#39;WorkNodes=&#39;k8s-worker01 k8s-worker02 k8s-worker03&#39;for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; $NODE:/usr/local/bin/; donefor NODE in $WorkNodes; do echo $NODE; scp /usr/local/bin/kube&#123;let,-proxy&#125; $NODE:/usr/local/bin/; done</code></pre><h3 id="2-4-生成K8s和etcd证书"><a href="#2-4-生成K8s和etcd证书" class="headerlink" title="2.4 生成K8s和etcd证书"></a>2.4 生成K8s和etcd证书</h3><h4 id="1-安装cfssl证书管理工具（master01）"><a href="#1-安装cfssl证书管理工具（master01）" class="headerlink" title="1. 安装cfssl证书管理工具（master01）"></a>1. 安装cfssl证书管理工具（master01）</h4><pre><code class="bash">wget https://ghproxy.com/https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssl_1.6.4_linux_amd64 -O /usr/local/bin/cfsslwget https://ghproxy.com/https://github.com/cloudflare/cfssl/releases/download/v1.6.4/cfssljson_1.6.4_linux_amd64 -O /usr/local/bin/cfssljson</code></pre><p>或者用其他方式下载</p><h4 id="2-生成etcd证书（master01）"><a href="#2-生成etcd证书（master01）" class="headerlink" title="2. 生成etcd证书（master01）"></a>2. 生成etcd证书（master01）</h4><ol><li>创建etcd证书存储目录</li></ol><pre><code class="bash">mkdir -pv /yinzhengjie/certs/&#123;etcd,pki&#125;/ &amp;&amp; cd /yinzhengjie/certs/pki/</code></pre><ol start="2"><li>生成etcd证书的自建ca证书</li></ol><pre><code class="bash"># 1.生成证书的CSR文件cat &gt; etcd-ca-csr.json &lt;&lt; EOF&#123;  &quot;CN&quot;: &quot;etcd&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;etcd&quot;,      &quot;OU&quot;: &quot;Etcd Security&quot;    &#125;  ],  &quot;ca&quot;: &#123;    &quot;expiry&quot;: &quot;876000h&quot;  &#125;&#125;EOF# 这是一个用于生成证书签名请求（Certificate Signing Request，CSR）的JSON配置文件。JSON配置文件指定了生成证书签名请求所需的数据。# - &quot;CN&quot;: &quot;etcd&quot; 指定了希望生成的证书的CN字段（Common Name），即证书的主题，通常是该证书标识的实体的名称。# - &quot;key&quot;: &#123;&#125; 指定了生成证书所使用的密钥的配置信息。&quot;algo&quot;: &quot;rsa&quot; 指定了密钥的算法为RSA，&quot;size&quot;: 2048 指定了密钥的长度为2048位。# - &quot;names&quot;: [] 包含了生成证书时所需的实体信息。在这个例子中，只包含了一个实体，其相关信息如下：#   - &quot;C&quot;: &quot;CN&quot; 指定了实体的国家/地区代码，这里是中国。#   - &quot;ST&quot;: &quot;Beijing&quot; 指定了实体所在的省/州。#   - &quot;L&quot;: &quot;Beijing&quot; 指定了实体所在的城市。#   - &quot;O&quot;: &quot;etcd&quot; 指定了实体的组织名称。#   - &quot;OU&quot;: &quot;Etcd Security&quot; 指定了实体所属的组织单位。# - &quot;ca&quot;: &#123;&#125; 指定了生成证书时所需的CA（Certificate Authority）配置信息。#   - &quot;expiry&quot;: &quot;876000h&quot; 指定了证书的有效期，这里是876000小时。# # 生成证书签名请求时，可以使用这个JSON配置文件作为输入，根据配置文件中的信息生成相应的CSR文件。然后，可以将CSR文件发送给CA进行签名，以获得有效的证书。# 2.生成etcd CA证书和CA证书的keycfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /root/certs/etcd/etcd-ca# cfssl是一个用于生成TLS/SSL证书的工具，它支持PKI、JSON格式配置文件以及与许多其他集成工具的配合使用。# gencert参数表示生成证书的操作。-initca参数表示初始化一个CA（证书颁发机构）。CA是用于签发其他证书的根证书。etcd-ca-csr.json是一个JSON格式的配置文件，其中包含了CA的详细信息，如私钥、公钥、有效期等。这个文件提供了生成CA证书所需的信息。# | 符号表示将上一个命令的输出作为下一个命令的输入。# cfssljson是cfssl工具的一个子命令，用于格式化cfssl生成的JSON数据。 -bare参数表示直接输出裸证书，即只生成证书文件，不包含其他格式的文件。# 所以，这条命令的含义是使用cfssl工具根据配置文件etcd-ca-csr.json生成一个CA证书，并将证书文件保存在/root/certs/etcd/etcd-ca路径下。# 3.查看结果ls /root/certs/etcd/etcd-ca.csretcd-ca-key.pemetcd-ca.pem</code></pre><ol start="3"><li>基于自建ca证书颁发etcd证书</li></ol><pre><code class="bash"># 1.写入生成证书所需的配置文件cat &gt; ca-config.json &lt;&lt; EOF &#123;  &quot;signing&quot;: &#123;    &quot;default&quot;: &#123;      &quot;expiry&quot;: &quot;876000h&quot;    &#125;,    &quot;profiles&quot;: &#123;      &quot;kubernetes&quot;: &#123;        &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ],        &quot;expiry&quot;: &quot;876000h&quot;      &#125;    &#125;  &#125;&#125;EOF# 这段配置文件是用于配置加密和认证签名的一些参数。# 在这里，有两个部分：`signing`和`profiles`。# `signing`包含了默认签名配置和配置文件。# 默认签名配置`default`指定了证书的过期时间为`876000h`。`876000h`表示证书有效期为100年。# `profiles`部分定义了不同的证书配置文件。# 在这里，只有一个配置文件`kubernetes`。它包含了以下`usages`和过期时间`expiry`：# 1. `signing`：用于对其他证书进行签名# 2. `key encipherment`：用于加密和解密传输数据# 3. `server auth`：用于服务器身份验证# 4. `client auth`：用于客户端身份验证# 2.生成证书的CSR文件： 证书签发请求文件，配置了一些域名，公司，单位cat &gt; etcd-csr.json &lt;&lt; EOF &#123;  &quot;CN&quot;: &quot;etcd&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;etcd&quot;,      &quot;OU&quot;: &quot;Etcd Security&quot;    &#125;  ]&#125;EOF# 3.基于自建的ectd ca证书生成etcd的证书cfssl gencert \  -ca=/root/certs/etcd/etcd-ca.pem \  -ca-key=/root/certs/etcd/etcd-ca-key.pem \  -config=ca-config.json \  --hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,10.200.16.10,10.200.16.11,10.200.16.12 \  --profile=kubernetes \  etcd-csr.json  | cfssljson -bare /root/certs/etcd/etcd-server# 这是一条使用cfssl生成etcd证书的命令，下面是各个参数的解释：# -ca=/etc/etcd/ssl/etcd-ca.pem：指定用于签名etcd证书的CA文件的路径。# -ca-key=/etc/etcd/ssl/etcd-ca-key.pem：指定用于签名etcd证书的CA私钥文件的路径。# -config=ca-config.json：指定CA配置文件的路径，该文件定义了证书的有效期、加密算法等设置。# -hostname=xxxx：指定要为etcd生成证书的主机名和IP地址列表。# -profile=kubernetes：指定使用的证书配置文件，该文件定义了证书的用途和扩展属性。# etcd-csr.json：指定etcd证书请求的JSON文件的路径，该文件包含了证书请求的详细信息。# | cfssljson -bare /etc/etcd/ssl/etcd：通过管道将cfssl命令的输出传递给cfssljson命令，并使用-bare参数指定输出文件的前缀路径，这里将生成etcd证书的.pem和-key.pem文件。# 查看  ll /root/certs/etcd/# total 24# -rw-r--r-- 1 root root 1050 Jan  7 15:36 etcd-ca.csr# -rw------- 1 root root 1679 Jan  7 15:36 etcd-ca-key.pem# -rw-r--r-- 1 root root 1318 Jan  7 15:36 etcd-ca.pem# -rw-r--r-- 1 root root 1131 Jan  7 15:40 etcd-server.csr# -rw------- 1 root root 1679 Jan  7 15:40 etcd-server-key.pem# -rw-r--r-- 1 root root 1464 Jan  7 15:40 etcd-server.pem</code></pre><ol start="4"><li>将etcd证书拷贝至其他两个节点</li></ol><pre><code class="bash">MasterNodes=&#39;k8s-master02 k8s-master03&#39;for NODE in $MasterNodes; do     echo $NODE; ssh $NODE &quot;mkdir -pv /root/certs/etcd/&quot;     for FILE in etcd-ca-key.pem etcd-ca.pem etcd-server-key.pem etcd-server.pem; do       scp /root/certs/etcd/$&#123;FILE&#125; $NODE:/root/certs/etcd/$&#123;FILE&#125;     donedone</code></pre><h4 id="3-生成k8s组件证书（master01）"><a href="#3-生成k8s组件证书（master01）" class="headerlink" title="3. 生成k8s组件证书（master01）"></a>3. 生成k8s组件证书（master01）</h4><ol><li><p>所有节点创建证书保存目录 <code>mkdir -p /root/certs/kubernetes/</code></p></li><li><p>master01节点生成kubernetes自建ca证书</p></li></ol><pre><code class="bash"># 1.生成证书的CSR文件： 证书签发请求文件，配置了一些域名，公司，单位cat &gt; k8s-ca-csr.json &lt;&lt; EOF &#123;  &quot;CN&quot;: &quot;kubernetes&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;Kubernetes&quot;,      &quot;OU&quot;: &quot;Kubernetes-manual&quot;    &#125;  ],  &quot;ca&quot;: &#123;    &quot;expiry&quot;: &quot;876000h&quot;  &#125;&#125;EOF# 2.生成kubernetes证书cfssl gencert -initca k8s-ca-csr.json | cfssljson -bare /root/certs/kubernetes/k8s-ca# 3.查看ll /root/certs/kubernetes/# total 12# -rw-r--r-- 1 root root 1070 Jan  7 15:47 k8s-ca.csr# -rw------- 1 root root 1675 Jan  7 15:47 k8s-ca-key.pem# -rw-r--r-- 1 root root 1363 Jan  7 15:47 k8s-ca.pem</code></pre><ol start="3"><li>基于自检ca证书颁发apiserver相关证书</li></ol><pre><code class="bash"># 1.生成k8s证书的有效期为100年cat &gt; k8s-ca-config.json &lt;&lt; EOF&#123;  &quot;signing&quot;: &#123;    &quot;default&quot;: &#123;      &quot;expiry&quot;: &quot;876000h&quot;    &#125;,    &quot;profiles&quot;: &#123;      &quot;kubernetes&quot;: &#123;        &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ],        &quot;expiry&quot;: &quot;876000h&quot;      &#125;    &#125;  &#125;&#125;EOF# 2.生成apiserver证书的CSR文件： 证书签发请求文件，配置了一些域名，公司，单位cat &gt; apiserver-csr.json &lt;&lt; EOF &#123;  &quot;CN&quot;: &quot;kube-apiserver&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;Kubernetes&quot;,      &quot;OU&quot;: &quot;Kubernetes-manual&quot;    &#125;  ]&#125;EOF# 3.基于自检ca证书生成apiServer的证书文件cfssl gencert \  -ca=/root/certs/kubernetes/k8s-ca.pem \  -ca-key=/root/certs/kubernetes/k8s-ca-key.pem \  -config=k8s-ca-config.json \  --hostname=10.100.0.1,10.200.0.34,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,10.200.16.10,10.200.16.11,10.200.16.12,10.200.16.13,10.200.16.14,10.200.16.2 \  --profile=kubernetes \   apiserver-csr.json  | cfssljson -bare /root/certs/kubernetes/apiserver# 这个命令是使用cfssl工具生成Kubernetes API Server的证书。# 命令的参数解释如下：# - `-ca=/etc/kubernetes/pki/ca.pem`：指定证书的颁发机构（CA）文件路径。# - `-ca-key=/etc/kubernetes/pki/ca-key.pem`：指定证书的颁发机构（CA）私钥文件路径。# - `-config=ca-config.json`：指定证书生成的配置文件路径，配置文件中包含了证书的有效期、加密算法等信息。# - `-hostname=10.100.0.1,10.200.0.34,kubernetes`：指定证书的主机名或IP地址列表。# - &quot;10.100.0.1&quot;为咱们的svc网段的第一个地址,&quot;10.0.0.240&quot;是负载均衡器的VIP地址。&quot;kubernetes,...,kubernetes.default.svc.cluster.local&quot;对应的是apiServer解析的A记录。#    &quot;10.200.16.10,...,10.0.0.245&quot;对应的是K8S集群的地址。如果后续集群要扩容，需要多加入一些IP地址。# - `-profile=kubernetes`：指定证书生成的配置文件中的配置文件名。# - `apiserver-csr.json`：API Server的证书签名请求配置文件路径。# - `| cfssljson -bare /etc/kubernetes/pki/apiserver`：通过管道将生成的证书输出到cfssljson工具，将其转换为PEM编码格式，并保存到 `/etc/kubernetes/pki/apiserver.pem` 和 `/etc/kubernetes/pki/apiserver-key.pem` 文件中。# 4.验证 ll /root/certs/kubernetes/apiserver*#-rw-r--r-- 1 root root 1314 Jan  7 17:03 /root/certs/kubernetes/apiserver.csr#-rw------- 1 root root 1679 Jan  7 17:03 /root/certs/kubernetes/apiserver-key.pem#-rw-r--r-- 1 root root 1712 Jan  7 17:03 /root/certs/kubernetes/apiserver.pem</code></pre><ol start="4"><li><p>生成第三方组件与apiServer通信的聚合证书</p><p><strong>聚合证书的作用就是让第三方组件(比如metrics-server等)能够拿这个证书文件和apiServer进行通信。</strong></p></li></ol><pre><code class="bash"># 1.生成聚合证书的用于自建ca的CSR文件cat &gt; front-proxy-ca-csr.json &lt;&lt; EOF &#123;  &quot;CN&quot;: &quot;kubernetes&quot;,  &quot;key&quot;: &#123;     &quot;algo&quot;: &quot;rsa&quot;,     &quot;size&quot;: 2048  &#125;&#125;EOF# 2.生成聚合证书的自检ca证书cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare /root/certs/kubernetes/front-proxy-ca[root@k8s-master01 pki]# [root@k8s-master01 pki]# ll /root/certs/kubernetes/front-proxy-ca*-rw-r--r-- 1 root root  891 Jan  7 17:05 /root/certs/kubernetes/front-proxy-ca.csr-rw------- 1 root root 1675 Jan  7 17:05 /root/certs/kubernetes/front-proxy-ca-key.pem-rw-r--r-- 1 root root 1094 Jan  7 17:05 /root/certs/kubernetes/front-proxy-ca.pem[root@k8s-master01 pki]# # 3.生成聚合证书的用于客户端的CSR文件cat &gt; front-proxy-client-csr.json &lt;&lt; EOF &#123;  &quot;CN&quot;: &quot;front-proxy-client&quot;,  &quot;key&quot;: &#123;     &quot;algo&quot;: &quot;rsa&quot;,     &quot;size&quot;: 2048  &#125;&#125;EOF# 4.基于聚合证书的自建ca证书签发聚合证书的客户端证书cfssl gencert \  -ca=/root/certs/kubernetes/front-proxy-ca.pem \  -ca-key=/root/certs/kubernetes/front-proxy-ca-key.pem \  -config=k8s-ca-config.json \  -profile=kubernetes \  front-proxy-client-csr.json | cfssljson -bare /root/certs/kubernetes/front-proxy-client# 5.查看ll /root/certs/kubernetes/front-proxy-client*#-rw-r--r-- 1 root root  903 Jan  7 17:06 /root/certs/kubernetes/front-proxy-client.csr#-rw------- 1 root root 1679 Jan  7 17:06 /root/certs/kubernetes/front-proxy-client-key.pem#-rw-r--r-- 1 root root 1188 Jan  7 17:06 /root/certs/kubernetes/front-proxy-client.pem</code></pre><ol start="5"><li>生成controller-manager证书及kubeconfig文件</li></ol><pre><code class="bash">#    1.生成controller-manager的CSR文件cat &gt; controller-manager-csr.json &lt;&lt; EOF&#123;  &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;system:kube-controller-manager&quot;,      &quot;OU&quot;: &quot;Kubernetes-manual&quot;    &#125;  ]&#125;EOF#    2.生成controller-manager证书文件cfssl gencert \  -ca=/root/certs/kubernetes/k8s-ca.pem \  -ca-key=/root/certs/kubernetes/k8s-ca-key.pem \  -config=k8s-ca-config.json \  -profile=kubernetes \  controller-manager-csr.json | cfssljson -bare /root/certs/kubernetes/controller-manager# 验证ll /root/certs/kubernetes/controller-manager*#-rw-r--r-- 1 root root 1082 Nov  5 11:31 /root/certs/kubernetes/controller-manager.csr#-rw------- 1 root root 1675 Nov  5 11:31 /root/certs/kubernetes/controller-manager-key.pem#-rw-r--r-- 1 root root 1501 Nov  5 11:31 /root/certs/kubernetes/controller-manager.pem#    3.创建一个kubeconfig目录mkdir -pv /root/certs/kubeconfig#    4.设置一个集群kubectl config set-cluster root-k8s \  --certificate-authority=/root/certs/kubernetes/k8s-ca.pem \  --embed-certs=true \  --server=https://10.200.0.34:6443 \  --kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig# kubectl config set-cluster命令用于配置集群信息，root-k8s是集群名称。# --certificate-authority选项指定了集群的证书颁发机构（CA）的路径，这个CA会验证kube-apiserver提供的证书是否合法。# --embed-certs选项用于将证书嵌入到生成的kubeconfig文件中，这样就不需要在kubeconfig文件中单独指定证书文件路径。# --server选项指定了kube-apiserver的地址，负载均衡或者keepalived的地址，apiserver安全端口默认6443。# --kubeconfig选项指定了生成的kubeconfig文件的路径和名称。# 综上所述，kubectl config set-cluster命令的作用是在kubeconfig文件中设置集群信息，包括证书颁发机构、证书、kube-apiserver地址等。#    5.设置一个用户项kubectl config set-credentials system:kube-controller-manager \  --client-certificate=/root/certs/kubernetes/controller-manager.pem \  --client-key=/root/certs/kubernetes/controller-manager-key.pem \  --embed-certs=true \  --kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig# 上述命令是用于设置 Kubernetes 的 controller-manager 组件的客户端凭据。下面是每个参数的详细解释：# - `kubectl config`: 是使用 kubectl 命令行工具的配置子命令。# - `set-credentials`: 是定义一个新的用户凭据配置的子命令。# - `system:kube-controller-manager`: 是设置用户凭据的名称，`system:` 是 Kubernetes API Server 内置的身份验证器使用的用户标识符前缀，它表示是一个系统用户，在本例中是 kube-controller-manager 组件使用的身份。# - `--client-certificate=/etc/kubernetes/pki/controller-manager.pem`: 指定 controller-manager.pem 客户端证书的路径。# - `--client-key=/etc/kubernetes/pki/controller-manager-key.pem`: 指定 controller-manager-key.pem 客户端私钥的路径。# - `--embed-certs=true`: 表示将证书和私钥直接嵌入到生成的 kubeconfig 文件中，而不是通过引用外部文件。# - `--kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig`: 指定生成的 kubeconfig 文件的路径和文件名，即 controller-manager.kubeconfig。#    6.设置一个上下文环境kubectl config set-context system:kube-controller-manager@kubernetes \  --cluster=root-k8s \  --user=system:kube-controller-manager \  --kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig# 这个命令用于配置 Kubernetes 控制器管理器的上下文信息。下面是各个参数的详细解释：# 1. `kubectl config set-context system:kube-controller-manager@kubernetes`: 设置上下文的名称为 `system:kube-controller-manager@kubernetes`，这是一个标识符，用于唯一标识该上下文。# 2. `--cluster=kubernetes`: 指定集群的名称为 `kubernetes`，这是一个现有集群的标识符，表示要管理的 Kubernetes 集群。# 3. `--user=system:kube-controller-manager`: 指定使用的用户身份为 `system:kube-controller-manager`。这是一个特殊的用户身份，具有控制 Kubernetes 控制器管理器的权限。# 4. `--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig`: 指定 kubeconfig 文件的路径。kubeconfig 文件是一个用于管理 Kubernetes 配置的文件，包含了集群、用户和上下文的相关信息。# 通过运行这个命令，可以将这些配置信息保存到 `/root/certs/kubeconfig/kube-controller-manager.kubeconfig` 文件中，以便在后续的操作中使用。#    7.使用默认的上下文kubectl config use-context system:kube-controller-manager@kubernetes \  --kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig# 这个命令是用来指定kubectl使用指定的上下文环境来执行操作。上下文环境是kubectl用来确定要连接到哪个Kubernetes集群以及使用哪个身份验证信息的配置。# 在这个命令中，`kubectl config use-context`是用来设置当前上下文环境的命令。 `system:kube-controller-manager@kubernetes`是指定的上下文名称，它告诉kubectl要使用的Kubernetes集群和身份验证信息。 # `--kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig`是用来指定使用的kubeconfig文件的路径。kubeconfig文件是存储集群连接和身份验证信息的配置文件。# 通过执行这个命令，kubectl将使用指定的上下文来执行后续的操作，包括部署和管理Kubernetes资源。</code></pre><ol start="6"><li><p>生成scheduler证书及kubeconfig文件</p><p>kubeconfig 文件是一个用于管理 Kubernetes 配置的文件，包含了集群、用户和上下文的相关信息。</p></li></ol><pre><code class="bash">#    1.生成scheduler的CSR文件cat &gt; scheduler-csr.json  &lt;&lt; EOF&#123;  &quot;CN&quot;: &quot;system:kube-scheduler&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;system:kube-scheduler&quot;,      &quot;OU&quot;: &quot;Kubernetes-manual&quot;    &#125;  ]&#125;EOF#    2.生成scheduler证书文件cfssl gencert \  -ca=/root/certs/kubernetes/k8s-ca.pem \  -ca-key=/root/certs/kubernetes/k8s-ca-key.pem \  -config=k8s-ca-config.json \  -profile=kubernetes \  scheduler-csr.json | cfssljson -bare /root/certs/kubernetes/schedulerll /root/certs/kubernetes/scheduler*#-rw-r--r-- 1 root root 1058 Jan  7 18:56 /root/certs/kubernetes/scheduler.csr#-rw------- 1 root root 1679 Jan  7 18:56 /root/certs/kubernetes/scheduler-key.pem#rw-r--r-- 1 root root 1476 Jan  7 18:56 /root/certs/kubernetes/scheduler.pem#    3.设置一个集群kubectl config set-cluster root-k8s \  --certificate-authority=/root/certs/kubernetes/k8s-ca.pem \  --embed-certs=true \  --server=https://10.200.0.34:6443 \  --kubeconfig=/root/certs/kubeconfig/kube-scheduler.kubeconfig#   4.设置一个用户项kubectl config set-credentials system:kube-scheduler \  --client-certificate=/root/certs/kubernetes/scheduler.pem \  --client-key=/root/certs/kubernetes/scheduler-key.pem \  --embed-certs=true \  --kubeconfig=/root/certs/kubeconfig/kube-scheduler.kubeconfig#  5.设置一个上下文环境kubectl config set-context system:kube-scheduler@kubernetes \  --cluster=root-k8s \  --user=system:kube-scheduler \  --kubeconfig=/root/certs/kubeconfig/kube-scheduler.kubeconfig#  6.使用默认的上下文kubectl config use-context system:kube-scheduler@kubernetes \  --kubeconfig=/root/certs/kubeconfig/kube-scheduler.kubeconfig</code></pre><ol start="7"><li>配置k8s集群管理员证书及kubeconfig文件</li></ol><pre><code class="bash">#    1.生成管理员的CSR文件cat &gt; admin-csr.json &lt;&lt; EOF&#123;  &quot;CN&quot;: &quot;admin&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;system:masters&quot;,      &quot;OU&quot;: &quot;Kubernetes-manual&quot;    &#125;  ]&#125;EOF# 这段代码是一个JSON格式的配置文件，用于创建和配置一个名为&quot;admin&quot;的Kubernetes凭证。#   - &quot;O&quot;: &quot;system:masters&quot;：这是证书的组织字段，这里是system:masters，表示系统的管理员组。# 通过这个配置文件创建的凭证将具有管理员权限，并且可以用于管理Kubernetes集群。#    2.生成k8s集群管理员证书cfssl gencert \  -ca=/root/certs/kubernetes/k8s-ca.pem \  -ca-key=/root/certs/kubernetes/k8s-ca-key.pem \  -config=k8s-ca-config.json \  -profile=kubernetes \  admin-csr.json | cfssljson -bare /root/certs/kubernetes/adminll /root/certs/kubernetes/admin*-rw-r--r-- 1 root root 1025 Jan  7 19:00 /root/certs/kubernetes/admin.csr-rw------- 1 root root 1675 Jan  7 19:00 /root/certs/kubernetes/admin-key.pem-rw-r--r-- 1 root root 1444 Jan  7 19:00 /root/certs/kubernetes/admin.pem#    2.设置一个集群kubectl config set-cluster root-k8s \  --certificate-authority=/root/certs/kubernetes/k8s-ca.pem \  --embed-certs=true \  --server=https://10.200.0.34:6443 \  --kubeconfig=/root/certs/kubeconfig/kube-admin.kubeconfig#   4.设置一个用户项kubectl config set-credentials kube-admin \  --client-certificate=/root/certs/kubernetes/admin.pem \  --client-key=/root/certs/kubernetes/admin-key.pem \  --embed-certs=true \  --kubeconfig=/root/certs/kubeconfig/kube-admin.kubeconfig#  5.设置一个上下文环境kubectl config set-context kube-admin@kubernetes \  --cluster=root-k8s \  --user=kube-admin \  --kubeconfig=/root/certs/kubeconfig/kube-admin.kubeconfig#  6.使用默认的上下文kubectl config use-context kube-admin@kubernetes \  --kubeconfig=/root/certs/kubeconfig/kube-admin.kubeconfig</code></pre><ol start="8"><li>创建ServiceAccount Key ——secret</li></ol><pre><code class="bash">#     1.ServiceAccount是k8s一种认证方式，创建ServiceAccount的时候会创建一个与之绑定的secret，这个secret会生成一个tokenopenssl genrsa -out /root/certs/kubernetes/sa.key 2048#    2.基于sa.key创建sa.pubopenssl rsa -in /root/certs/kubernetes/sa.key -pubout -out /root/certs/kubernetes/sa.publl /root/certs/kubernetes/sa*#-rw-r--r-- 1 root root 1679 Jan  7 19:02 /root/certs/kubernetes/sa.key#-rw-r--r-- 1 root root  451 Jan  7 19:03 /root/certs/kubernetes/sa.pub# 这两个命令是使用OpenSSL工具生成RSA密钥对。# # 命令1：openssl genrsa -out /etc/kubernetes/pki/sa.key 2048# 该命令用于生成私钥文件。具体解释如下：# - openssl：openssl命令行工具。# - genrsa：生成RSA密钥对。# - -out /etc/kubernetes/pki/sa.key：指定输出私钥文件的路径和文件名。# - 2048：指定密钥长度为2048位。# # 命令2：openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub# 该命令用于从私钥中导出公钥。具体解释如下：# - openssl：openssl命令行工具。# - rsa：与私钥相关的RSA操作。# - -in /etc/kubernetes/pki/sa.key：指定输入私钥文件的路径和文件名。# - -pubout：指定输出公钥。# - -out /etc/kubernetes/pki/sa.pub：指定输出公钥文件的路径和文件名。# 总结：通过以上两个命令，我们可以使用OpenSSL工具生成一个RSA密钥对，并将私钥保存在/etc/kubernetes/pki/sa.key文件中，将公钥保存在/etc/kubernetes/pki/sa.pub文件中。</code></pre><ol start="9"><li>将除etcd外其他证书拷贝到其他两个master节点</li></ol><pre><code class="bash">for NODE in k8s-master02 k8s-master03; do        echo $NODE; ssh $NODE &quot;mkdir -pv /root/certs/&#123;kubernetes,kubeconfig&#125;&quot;     for FILE in $(ls /root/certs/kubernetes); do         scp /root/certs/kubernetes/$&#123;FILE&#125; $NODE:/root/certs/kubernetes/$&#123;FILE&#125;;     done;      for FILE in kube-admin.kubeconfig  kube-controller-manager.kubeconfig  kube-scheduler.kubeconfig; do         scp /root/certs/kubeconfig/$&#123;FILE&#125; $NODE:/root/certs/kubeconfig/$&#123;FILE&#125;;     done;done</code></pre><p>验证三台证书数量是否一致 <code>ls /root/certs/kubernetes | wc -l</code></p><ol start="10"><li><p>生成kube-proxy证书及kubeconfig文件</p><p><strong>这个证书不用同步到其他节点，连接使用kubeconfig文件。</strong></p></li></ol><pre><code class="bash">#     1.生成kube-proxy的csr文件cat &gt; kube-proxy-csr.json  &lt;&lt; EOF&#123;  &quot;CN&quot;: &quot;system:kube-proxy&quot;,  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Beijing&quot;,      &quot;L&quot;: &quot;Beijing&quot;,      &quot;O&quot;: &quot;system:kube-proxy&quot;,      &quot;OU&quot;: &quot;Kubernetes-manual&quot;    &#125;  ]&#125;EOF#    2.创建kube-proxy需要的证书文件cfssl gencert \  -ca=/root/certs/kubernetes/k8s-ca.pem \  -ca-key=/root/certs/kubernetes/k8s-ca-key.pem \  -config=k8s-ca-config.json \  -profile=kubernetes \  kube-proxy-csr.json | cfssljson -bare /root/certs/kubernetes/kube-proxyll /root/certs/kubernetes/kube-proxy*#-rw-r--r-- 1 root root 1045 Jan  9 09:43 /root/certs/kubernetes/kube-proxy.csr#-rw------- 1 root root 1679 Jan  9 09:43 /root/certs/kubernetes/kube-proxy-key.pem#-rw-r--r-- 1 root root 1464 Jan  9 09:43 /root/certs/kubernetes/kube-proxy.pem#    3.设置集群kubectl config set-cluster root-k8s \  --certificate-authority=/root/certs/kubernetes/k8s-ca.pem \  --embed-certs=true \  --server=https://10.200.0.34:6443 \  --kubeconfig=/root/certs/kubeconfig/kube-proxy.kubeconfig#    4.设置一个用户项kubectl config set-credentials system:kube-proxy \  --client-certificate=/root/certs/kubernetes/kube-proxy.pem \  --client-key=/root/certs/kubernetes/kube-proxy-key.pem \  --embed-certs=true \  --kubeconfig=/root/certs/kubeconfig/kube-proxy.kubeconfig#    5.设置一个上下文环境kubectl config set-context kube-proxy@kubernetes \  --cluster=root-k8s \  --user=system:kube-proxy \  --kubeconfig=/root/certs/kubeconfig/kube-proxy.kubeconfig#    6.使用默认的上下文kubectl config use-context kube-proxy@kubernetes \  --kubeconfig=/root/certs/kubeconfig/kube-proxy.kubeconfig#    7.将kube-proxy的kube-proxy.kubeconfig文件发送到其他节点for NODE in k8s-master02 k8s-master03 k8s-worker01 k8s-worker02 k8s-worker03; do     echo $NODE     scp /root/certs/kubeconfig/kube-proxy.kubeconfig $NODE:/root/certs/kubeconfig/done</code></pre><h3 id="2-5-部署K8s高可用集群"><a href="#2-5-部署K8s高可用集群" class="headerlink" title="2.5. 部署K8s高可用集群"></a>2.5. 部署K8s高可用集群</h3><h4 id="1-高可用组件部署"><a href="#1-高可用组件部署" class="headerlink" title="1. 高可用组件部署"></a>1. 高可用组件部署</h4><ul><li><p>方案1：keepalived+nginx，具体方案省略。</p></li><li><p>方案2：keepalived+haproxy，具体方案省略。</p></li><li><p>方案3：云负载均衡（后端直接对接apiserver，本文采用）</p></li></ul><p><strong>云主机建议使用负载均称，物理机和虚拟机建议采用keepalived。</strong></p><h4 id="2-启动etcd集群"><a href="#2-启动etcd集群" class="headerlink" title="2. 启动etcd集群"></a>2. 启动etcd集群</h4><ol><li>创建etcd集群各节点配置文件</li></ol><pre><code class="bash"># 1.创建etc部署目录（三个节点）mkdir -pv /root/softwares/etcd# 2.master01cat &gt; /root/softwares/etcd/etcd.config.yml &lt;&lt;&#39;EOF&#39;name: &#39;k8s-master01&#39;data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 5000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: &#39;https://10.200.16.10:2380&#39;listen-client-urls: &#39;https://10.200.16.10:2379,http://127.0.0.1:2379&#39;max-snapshots: 3max-wals: 5cors:initial-advertise-peer-urls: &#39;https://10.200.16.10:2380&#39;advertise-client-urls: &#39;https://10.200.16.10:2379&#39;discovery:discovery-fallback: &#39;proxy&#39;discovery-proxy:discovery-srv:initial-cluster: &#39;k8s-master01=https://10.200.16.10:2380,k8s-master02=https://10.200.16.11:2380,k8s-master03=https://10.200.16.12:2380&#39;initial-cluster-token: &#39;etcd-k8s-cluster&#39;initial-cluster-state: &#39;new&#39;strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: &#39;off&#39;proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security:  cert-file: &#39;/root/certs/etcd/etcd-server.pem&#39;  key-file: &#39;/root/certs/etcd/etcd-server-key.pem&#39;  client-cert-auth: true  trusted-ca-file: &#39;/root/certs/etcd/etcd-ca.pem&#39;  auto-tls: truepeer-transport-security:  cert-file: &#39;/root/certs/etcd/etcd-server.pem&#39;  key-file: &#39;/root/certs/etcd/etcd-server-key.pem&#39;  peer-client-cert-auth: true  trusted-ca-file: &#39;/root/certs/etcd/etcd-ca.pem&#39;  auto-tls: truedebug: falselog-package-levels:log-outputs: [default]force-new-cluster: falseEOF# 这个配置文件是用于 etcd 集群的配置，其中包含了一些重要的参数和选项：# # - `name`：指定了当前节点的名称，用于集群中区分不同的节点。# - `data-dir`：指定了 etcd 数据的存储目录。# - `wal-dir`：指定了 etcd 数据写入磁盘的目录。# - `snapshot-count`：指定了触发快照的事务数量。# - `heartbeat-interval`：指定了 etcd 集群中节点之间的心跳间隔。# - `election-timeout`：指定了选举超时时间。# - `quota-backend-bytes`：指定了存储的限额，0 表示无限制。# - `listen-peer-urls`：指定了节点之间通信的 URL，使用 HTTPS 协议。# - `listen-client-urls`：指定了客户端访问 etcd 集群的 URL，同时提供了本地访问的 URL。# - `max-snapshots`：指定了快照保留的数量。# - `max-wals`：指定了日志保留的数量。# - `initial-advertise-peer-urls`：指定了节点之间通信的初始 URL。# - `advertise-client-urls`：指定了客户端访问 etcd 集群的初始 URL。# - `discovery`：定义了 etcd 集群发现相关的选项。# - `initial-cluster`：指定了 etcd 集群的初始成员。# - `initial-cluster-token`：指定了集群的 token。# - `initial-cluster-state`：指定了集群的初始状态。# - `strict-reconfig-check`：指定了严格的重新配置检查选项。# - `enable-v2`：启用了 v2 API。# - `enable-pprof`：启用了性能分析。# - `proxy`：设置了代理模式。# - `client-transport-security`：客户端的传输安全配置。# - `peer-transport-security`：节点之间的传输安全配置。# - `debug`：是否启用调试模式。# - `log-package-levels`：日志的输出级别。# - `log-outputs`：指定了日志的输出类型。# - `force-new-cluster`：是否强制创建一个新的集群。# # 这些参数和选项可以根据实际需求进行调整和配置。# 3.master02# 修改name，本节点urlcat &gt; /root/softwares/etcd/etcd.config.yml &lt;&lt; &#39;EOF&#39;name: &#39;k8s-master02&#39;data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 5000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: &#39;https://10.200.16.11:2380&#39;listen-client-urls: &#39;https://10.200.16.11:2379,http://127.0.0.1:2379&#39;max-snapshots: 3max-wals: 5cors:initial-advertise-peer-urls: &#39;https://10.200.16.11:2380&#39;advertise-client-urls: &#39;https://10.200.16.11:2379&#39;discovery:discovery-fallback: &#39;proxy&#39;discovery-proxy:discovery-srv:initial-cluster: &#39;k8s-master01=https://10.200.16.10:2380,k8s-master02=https://10.200.16.11:2380,k8s-master03=https://10.200.16.12:2380&#39;initial-cluster-token: &#39;etcd-k8s-cluster&#39;initial-cluster-state: &#39;new&#39;strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: &#39;off&#39;proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security:  cert-file: &#39;/root/certs/etcd/etcd-server.pem&#39;  key-file: &#39;/root/certs/etcd/etcd-server-key.pem&#39;  client-cert-auth: true  trusted-ca-file: &#39;/root/certs/etcd/etcd-ca.pem&#39;  auto-tls: truepeer-transport-security:  cert-file: &#39;/root/certs/etcd/etcd-server.pem&#39;  key-file: &#39;/root/certs/etcd/etcd-server-key.pem&#39;  peer-client-cert-auth: true  trusted-ca-file: &#39;/root/certs/etcd/etcd-ca.pem&#39;  auto-tls: truedebug: falselog-package-levels:log-outputs: [default]force-new-cluster: falseEOF# 4.master03cat &gt; /root/softwares/etcd/etcd.config.yml &lt;&lt; &#39;EOF&#39;name: &#39;k8s-master03&#39;data-dir: /var/lib/etcdwal-dir: /var/lib/etcd/walsnapshot-count: 5000heartbeat-interval: 100election-timeout: 1000quota-backend-bytes: 0listen-peer-urls: &#39;https://10.200.16.12:2380&#39;listen-client-urls: &#39;https://10.200.16.12:2379,http://127.0.0.1:2379&#39;max-snapshots: 3max-wals: 5cors:initial-advertise-peer-urls: &#39;https://10.200.16.12:2380&#39;advertise-client-urls: &#39;https://10.200.16.12:2379&#39;discovery:discovery-fallback: &#39;proxy&#39;discovery-proxy:discovery-srv:initial-cluster: &#39;k8s-master01=https://10.200.16.10:2380,k8s-master02=https://10.200.16.11:2380,k8s-master03=https://10.200.16.12:2380&#39;initial-cluster-token: &#39;etcd-k8s-cluster&#39;initial-cluster-state: &#39;new&#39;strict-reconfig-check: falseenable-v2: trueenable-pprof: trueproxy: &#39;off&#39;proxy-failure-wait: 5000proxy-refresh-interval: 30000proxy-dial-timeout: 1000proxy-write-timeout: 5000proxy-read-timeout: 0client-transport-security:  cert-file: &#39;/root/certs/etcd/etcd-server.pem&#39;  key-file: &#39;/root/certs/etcd/etcd-server-key.pem&#39;  client-cert-auth: true  trusted-ca-file: &#39;/root/certs/etcd/etcd-ca.pem&#39;  auto-tls: truepeer-transport-security:  cert-file: &#39;/root/certs/etcd/etcd-server.pem&#39;  key-file: &#39;/root/certs/etcd/etcd-server-key.pem&#39;  peer-client-cert-auth: true  trusted-ca-file: &#39;/root/certs/etcd/etcd-ca.pem&#39;  auto-tls: truedebug: falselog-package-levels:log-outputs: [default]force-new-cluster: falseEOF</code></pre><ol start="2"><li>编写etcd启动脚本</li></ol><pre><code class="bash">cat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt;&#39;EOF&#39;[Unit]Description=Etcd ServiceDocumentation=https://coreos.com/etcd/docs/latest/After=network.target[Service]Type=notifyExecStart=/usr/local/bin/etcd --config-file=/root/softwares/etcd/etcd.config.ymlRestart=on-failureRestartSec=10LimitNOFILE=65536[Install]WantedBy=multi-user.targetAlias=etcd3.serviceEOF</code></pre><ol start="3"><li>启动etcd集群</li></ol><pre><code class="bash"># 1.三点节点启动systemctl daemon-reload &amp;&amp; systemctl enable --now etcdsystemctl status etcd# 2.查看etcd集群状态-master01节点etcdctl --endpoints=&quot;10.200.16.10:2379,10.200.16.11:2379,10.200.16.12:2379&quot; --cacert=/root/certs/etcd/etcd-ca.pem --cert=/root/certs/etcd/etcd-server.pem --key=/root/certs/etcd/etcd-server-key.pem  endpoint status --write-out=table# 3.验证集群高可用，停止后观察集群状态systemctl stop etcd systemctl start etcd </code></pre><h4 id="3-部署ApiServer组件"><a href="#3-部署ApiServer组件" class="headerlink" title="3. 部署ApiServer组件"></a>3. 部署ApiServer组件</h4><ol><li>k8s-master01节点启动ApiServer</li></ol><pre><code class="bash">cat &gt; /usr/lib/systemd/system/kube-apiserver.service &lt;&lt; &#39;EOF&#39;[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \      --v=2  \      --bind-address=0.0.0.0  \      --secure-port=6443  \      --allow_privileged=true \      --advertise-address=10.200.16.10 \      --service-cluster-ip-range=10.100.0.0/16  \      --service-node-port-range=3000-50000  \      --etcd-servers=https://10.200.16.10:2379,https://10.200.16.11:2379,https://10.200.16.12:2379 \      --etcd-cafile=/root/certs/etcd/etcd-ca.pem  \      --etcd-certfile=/root/certs/etcd/etcd-server.pem  \      --etcd-keyfile=/root/certs/etcd/etcd-server-key.pem  \      --client-ca-file=/root/certs/kubernetes/k8s-ca.pem  \      --tls-cert-file=/root/certs/kubernetes/apiserver.pem  \      --tls-private-key-file=/root/certs/kubernetes/apiserver-key.pem  \      --kubelet-client-certificate=/root/certs/kubernetes/apiserver.pem  \      --kubelet-client-key=/root/certs/kubernetes/apiserver-key.pem  \      --service-account-key-file=/root/certs/kubernetes/sa.pub  \      --service-account-signing-key-file=/root/certs/kubernetes/sa.key \      --service-account-issuer=https://kubernetes.default.svc.root.com \      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \      --authorization-mode=Node,RBAC  \      --enable-bootstrap-token-auth=true  \      --requestheader-client-ca-file=/root/certs/kubernetes/front-proxy-ca.pem  \      --proxy-client-cert-file=/root/certs/kubernetes/front-proxy-client.pem  \      --proxy-client-key-file=/root/certs/kubernetes/front-proxy-client-key.pem  \      --requestheader-allowed-names=aggregator  \      --requestheader-group-headers=X-Remote-Group  \      --requestheader-extra-headers-prefix=X-Remote-Extra-  \      --requestheader-username-headers=X-Remote-UserRestart=on-failureRestartSec=10sLimitNOFILE=65535[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserversystemctl status kube-apiserver</code></pre><p>参数解释：</p><pre><code class="markdown">该配置文件是用于定义Kubernetes API Server的systemd服务的配置。systemd是一个用于启动和管理Linux系统服务的守护进程。[Unit]- Description: 服务的描述信息，用于显示在日志和系统管理工具中。- Documentation: 提供关于服务的文档链接。- After: 规定服务依赖于哪些其他服务或单元。在这个例子中，API Server服务在网络目标启动之后启动。[Service]- ExecStart: 定义服务的命令行参数和命令。这里指定了API Server的启动命令，包括各种参数选项。- Restart: 指定当服务退出时应该如何重新启动。在这个例子中，服务在失败时将被重新启动。- RestartSec: 指定两次重新启动之间的等待时间。- LimitNOFILE: 指定进程可以打开的文件描述符的最大数量。[Install]- WantedBy: 指定服务应该安装到哪个系统目标。在这个例子中，服务将被安装到multi-user.target目标，以便在多用户模式下启动。上述配置文件中定义的kube-apiserver服务将以指定的参数运行，这些参数包括：- `--v=2` 指定日志级别为2，打印详细的API Server日志。- `--allow-privileged=true` 允许特权容器运行。- `--bind-address=0.0.0.0` 绑定API Server监听的IP地址。- `--secure-port=6443` 指定API Server监听的安全端口。- `--advertise-address=10.200.16.10` 广告API Server的地址。- `--service-cluster-ip-range=10.96.0.0/12,fd00:1111::/112` 指定服务CIDR范围。- `--service-node-port-range=30000-32767` 指定NodePort的范围。- `--etcd-servers=https://10.200.16.10:2379,https://10.200.16.11:2379,https://10.200.16.12:2379` 指定etcd服务器的地址。- `--etcd-cafile` 指定etcd服务器的CA证书。- `--etcd-certfile` 指定etcd服务器的证书。- `--etcd-keyfile` 指定etcd服务器的私钥。- `--client-ca-file` 指定客户端CA证书。- `--tls-cert-file` 指定服务的证书。- `--tls-private-key-file` 指定服务的私钥。- `--kubelet-client-certificate` 和 `--kubelet-client-key` 指定与kubelet通信的客户端证书和私钥。- `--service-account-key-file` 指定服务账户公钥文件。- `--service-account-signing-key-file` 指定服务账户签名密钥文件。- `--service-account-issuer` 指定服务账户的发布者。- `--kubelet-preferred-address-types` 指定kubelet通信时的首选地址类型。- `--enable-admission-plugins` 启用一系列准入插件。- `--authorization-mode` 指定授权模式。- `--enable-bootstrap-token-auth` 启用引导令牌认证。- `--requestheader-client-ca-file` 指定请求头中的客户端CA证书。- `--proxy-client-cert-file` 和 `--proxy-client-key-file` 指定代理客户端的证书和私钥。- `--requestheader-allowed-names` 指定请求头中允许的名字。- `--requestheader-group-headers` 指定请求头中的组头。- `--requestheader-extra-headers-prefix` 指定请求头中的额外头前缀。- `--requestheader-username-headers` 指定请求头中的用户名头。- `--enable-aggregator-routing` 启用聚合路由。整个配置文件为Kubernetes API Server提供了必要的参数，以便正确地启动和运行。</code></pre><ol start="2"><li>k8s-master02节点启动ApiServer</li></ol><pre><code class="bash">cat &gt; /usr/lib/systemd/system/kube-apiserver.service &lt;&lt; &#39;EOF&#39;[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \      --v=2  \      --bind-address=0.0.0.0  \      --secure-port=6443  \      --allow_privileged=true \      --advertise-address=10.200.16.11 \      --service-cluster-ip-range=10.100.0.0/16  \      --service-node-port-range=3000-50000  \      --etcd-servers=https://10.200.16.10:2379,https://10.200.16.11:2379,https://10.200.16.12:2379 \      --etcd-cafile=/root/certs/etcd/etcd-ca.pem  \      --etcd-certfile=/root/certs/etcd/etcd-server.pem  \      --etcd-keyfile=/root/certs/etcd/etcd-server-key.pem  \      --client-ca-file=/root/certs/kubernetes/k8s-ca.pem  \      --tls-cert-file=/root/certs/kubernetes/apiserver.pem  \      --tls-private-key-file=/root/certs/kubernetes/apiserver-key.pem  \      --kubelet-client-certificate=/root/certs/kubernetes/apiserver.pem  \      --kubelet-client-key=/root/certs/kubernetes/apiserver-key.pem  \      --service-account-key-file=/root/certs/kubernetes/sa.pub  \      --service-account-signing-key-file=/root/certs/kubernetes/sa.key \      --service-account-issuer=https://kubernetes.default.svc.root.com \      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \      --authorization-mode=Node,RBAC  \      --enable-bootstrap-token-auth=true  \      --requestheader-client-ca-file=/root/certs/kubernetes/front-proxy-ca.pem  \      --proxy-client-cert-file=/root/certs/kubernetes/front-proxy-client.pem  \      --proxy-client-key-file=/root/certs/kubernetes/front-proxy-client-key.pem  \      --requestheader-allowed-names=aggregator  \      --requestheader-group-headers=X-Remote-Group  \      --requestheader-extra-headers-prefix=X-Remote-Extra-  \      --requestheader-username-headers=X-Remote-UserRestart=on-failureRestartSec=10sLimitNOFILE=65535[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserversystemctl status kube-apiserver</code></pre><ol start="3"><li>k8s-master03节点启动ApiServer</li></ol><pre><code class="bash">cat &gt; /usr/lib/systemd/system/kube-apiserver.service &lt;&lt; &#39;EOF&#39;[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \      --v=2  \      --bind-address=0.0.0.0  \      --secure-port=6443  \      --allow_privileged=true \      --advertise-address=10.200.16.12 \      --service-cluster-ip-range=10.100.0.0/16  \      --service-node-port-range=3000-50000  \      --etcd-servers=https://10.200.16.10:2379,https://10.200.16.11:2379,https://10.200.16.12:2379 \      --etcd-cafile=/root/certs/etcd/etcd-ca.pem  \      --etcd-certfile=/root/certs/etcd/etcd-server.pem  \      --etcd-keyfile=/root/certs/etcd/etcd-server-key.pem  \      --client-ca-file=/root/certs/kubernetes/k8s-ca.pem  \      --tls-cert-file=/root/certs/kubernetes/apiserver.pem  \      --tls-private-key-file=/root/certs/kubernetes/apiserver-key.pem  \      --kubelet-client-certificate=/root/certs/kubernetes/apiserver.pem  \      --kubelet-client-key=/root/certs/kubernetes/apiserver-key.pem  \      --service-account-key-file=/root/certs/kubernetes/sa.pub  \      --service-account-signing-key-file=/root/certs/kubernetes/sa.key \      --service-account-issuer=https://kubernetes.default.svc.root.com \      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \      --authorization-mode=Node,RBAC  \      --enable-bootstrap-token-auth=true  \      --requestheader-client-ca-file=/root/certs/kubernetes/front-proxy-ca.pem  \      --proxy-client-cert-file=/root/certs/kubernetes/front-proxy-client.pem  \      --proxy-client-key-file=/root/certs/kubernetes/front-proxy-client-key.pem  \      --requestheader-allowed-names=aggregator  \      --requestheader-group-headers=X-Remote-Group  \      --requestheader-extra-headers-prefix=X-Remote-Extra-  \      --requestheader-username-headers=X-Remote-UserRestart=on-failureRestartSec=10sLimitNOFILE=65535[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserversystemctl status kube-apiserver</code></pre><h4 id="4-部署ControlerManager组件"><a href="#4-部署ControlerManager组件" class="headerlink" title="4. 部署ControlerManager组件"></a>4. 部署ControlerManager组件</h4><pre><code class="bash"># 所有master节点的controller-manager组件配置文件相同# - &quot;--cluster-cidr&quot;是Pod的网段地址，不带service前缀，默认是pod的cidrcat &gt; /usr/lib/systemd/system/kube-controller-manager.service &lt;&lt; &#39;EOF&#39;[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-controller-manager \      --v=2 \      --root-ca-file=/root/certs/kubernetes/k8s-ca.pem \      --cluster-signing-cert-file=/root/certs/kubernetes/k8s-ca.pem \      --cluster-signing-key-file=/root/certs/kubernetes/k8s-ca-key.pem \      --service-account-private-key-file=/root/certs/kubernetes/sa.key \      --kubeconfig=/root/certs/kubeconfig/kube-controller-manager.kubeconfig \      --leader-elect=true \      --use-service-account-credentials=true \      --node-monitor-grace-period=40s \      --node-monitor-period=5s \      --controllers=*,bootstrapsigner,tokencleaner \      --allocate-node-cidrs=true \      --cluster-cidr=10.100.0.0/16 \      --requestheader-client-ca-file=/root/certs/kubernetes/front-proxy-ca.pem \      --node-cidr-mask-size=24Restart=alwaysRestartSec=10s[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable --now kube-controller-managersystemctl  status kube-controller-manager</code></pre><p>参数解释</p><pre><code class="bash">这是一个用于启动 Kubernetes 控制器管理器的 systemd 服务单元文件。下面是对每个部分的详细解释：[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。Description：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 控制器管理器。Documentation：可选项，提供了关于该服务单元的文档链接。After：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。[Service]：定义了服务的运行参数和行为。ExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-controller-manager，并通过后续的行继续传递了一系列的参数设置。Restart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。RestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。[Install]：定义了如何安装和启用服务单元。WantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。在 ExecStart 中传递的参数说明如下：--v=2：设置日志的详细级别为 2。--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。--root-ca-file：根证书文件的路径，用于验证其他组件的证书。--cluster-signing-cert-file：用于签名集群证书的证书文件路径。--cluster-signing-key-file：用于签名集群证书的私钥文件路径。--service-account-private-key-file：用于签名服务账户令牌的私钥文件路径。--kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。--leader-elect=true：启用 Leader 选举机制，确保只有一个控制器管理器作为 leader 在运行。--use-service-account-credentials=true：使用服务账户的凭据进行认证和授权。--node-monitor-grace-period=40s：节点监控的优雅退出时间，节点长时间不响应时会触发节点驱逐。--node-monitor-period=5s：节点监控的检测周期，用于检测节点是否正常运行。--controllers：指定要运行的控制器类型，在这里使用了通配符 *，表示运行所有的控制器，同时还包括了 bootstrapsigner 和 tokencleaner 控制器。--allocate-node-cidrs=true：为节点分配 CIDR 子网，用于分配 Pod 网络地址。--service-cluster-ip-range：定义 Service 的 IP 范围，这里设置为 10.96.0.0/12 和 fd00::/108。--cluster-cidr：定义集群的 CIDR 范围，这里设置为 172.16.0.0/12 和 fc00::/48。--node-cidr-mask-size-ipv4：分配给每个节点的 IPv4 子网掩码大小，默认是 24--node-cidr-mask-size-ipv6：分配给每个节点的 IPv6 子网掩码大小，默认是 120。--requestheader-client-ca-file：设置请求头中客户端 CA 的证书文件路径，用于认证请求头中的 CA 证书。这个服务单元文件描述了 Kubernetes 控制器管理器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 控制器管理器组件。</code></pre><h4 id="5-部署Scheduler组件"><a href="#5-部署Scheduler组件" class="headerlink" title="5. 部署Scheduler组件"></a>5. 部署Scheduler组件</h4><pre><code class="bash"># 所有master节点的kube-scheduler配置文件相同cat &gt; /usr/lib/systemd/system/kube-scheduler.service &lt;&lt;&#39;EOF&#39;[Unit]Description=Jason Yin&#39;s Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-scheduler \      --v=2 \      --leader-elect=true \      --kubeconfig=/root/certs/kubeconfig/kube-scheduler.kubeconfigRestart=alwaysRestartSec=10s[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable --now kube-schedulersystemctl  status kube-scheduler</code></pre><p>参数解释</p><pre><code class="markdown">你是谁这是一个用于启动 Kubernetes 调度器的 systemd 服务单元文件。下面是对每个部分的详细解释：[Unit]：单元的基本信息部分，用于描述和标识这个服务单元。Description：服务单元的描述信息，说明了该服务单元的作用，这里是 Kubernetes 调度器。Documentation：可选项，提供了关于该服务单元的文档链接。After：定义了该服务单元在哪些其他单元之后启动，这里是 network.target，即在网络服务启动之后启动。[Service]：定义了服务的运行参数和行为。ExecStart：指定服务启动时执行的命令，这里是 /usr/local/bin/kube-scheduler，并通过后续的行继续传递了一系列的参数设置。Restart：定义了服务在退出后的重新启动策略，这里设置为 always，表示总是重新启动服务。RestartSec：定义了重新启动服务的时间间隔，这里设置为 10 秒。[Install]：定义了如何安装和启用服务单元。WantedBy：指定了服务单元所属的 target，这里是 multi-user.target，表示启动多用户模式下的服务。在 ExecStart 中传递的参数说明如下：--v=2：设置日志的详细级别为 2。--bind-address=0.0.0.0：绑定的 IP 地址，用于监听 Kubernetes 控制平面的请求，这里设置为 0.0.0.0，表示监听所有网络接口上的请求。--leader-elect=true：启用 Leader 选举机制，确保只有一个调度器作为 leader 在运行。--kubeconfig=/etc/kubernetes/scheduler.kubeconfig：kubeconfig 文件的路径，包含了与 Kubernetes API 服务器通信所需的配置信息。这个服务单元文件描述了 Kubernetes 调度器的启动参数和行为，并且定义了服务的依赖关系和重新启动策略。通过 systemd 启动该服务单元，即可启动 Kubernetes 调度器组件。</code></pre><h4 id="6-创建Bootstrapping自动颁发kubelet证书配置"><a href="#6-创建Bootstrapping自动颁发kubelet证书配置" class="headerlink" title="6. 创建Bootstrapping自动颁发kubelet证书配置"></a>6. 创建Bootstrapping自动颁发kubelet证书配置</h4><ol><li>master01节点创建bootstrap-kubelet.kubeconfig文件</li></ol><pre><code class="bash">#    1.设置集群kubectl config set-cluster root-k8s \  --certificate-authority=/root/certs/kubernetes/k8s-ca.pem \  --embed-certs=true \  --server=https://10.200.0.34:6443 \  --kubeconfig=/root/certs/kubeconfig/bootstrap-kubelet.kubeconfig#    2.创建用户(id为6位,secret为16位，硬性规定)kubectl config set-credentials tls-bootstrap-token-user  \  --token=yindao.langxwroot16weii \  --kubeconfig=/root/certs/kubeconfig/bootstrap-kubelet.kubeconfig#    3.将集群和用户进行绑定kubectl config set-context tls-bootstrap-token-user@kubernetes \  --cluster=root-k8s \  --user=tls-bootstrap-token-user \  --kubeconfig=/root/certs/kubeconfig/bootstrap-kubelet.kubeconfig#    4.配置默认的上下文kubectl config use-context tls-bootstrap-token-user@kubernetes \  --kubeconfig=/root/certs/kubeconfig/bootstrap-kubelet.kubeconfig</code></pre><ol start="2"><li>所有master节点拷贝管理证书</li></ol><pre><code class="bash">mkdir -p /root/.kubecp /root/certs/kubeconfig/kube-admin.kubeconfig /root/.kube/configkubectl get cskubectl cluster-info </code></pre><ol start="3"><li>创建bootstrap-secret授权</li></ol><pre><code class="bash"># 1.创建yaml文件，注意token-id和token-secret 和上面保持一致cat &gt; bootstrap-secret.yaml &lt;&lt;EOFapiVersion: v1kind: Secretmetadata:  name: bootstrap-token-yindao  namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData:  description: &quot;The default bootstrap token generated by &#39;kubelet &#39;.&quot;  token-id: yindao  token-secret: langxwroot16weii  usage-bootstrap-authentication: &quot;true&quot;  usage-bootstrap-signing: &quot;true&quot;  auth-extra-groups:  system:bootstrappers:default-node-token,system:bootstrappers:worker,system:bootstrappers:ingress---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: kubelet-bootstraproleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io  kind: Group  name: system:bootstrappers:default-node-token---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: node-autoapprove-bootstraproleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:certificates.k8s.io:certificatesigningrequests:nodeclientsubjects:- apiGroup: rbac.authorization.k8s.io  kind: Group  name: system:bootstrappers:default-node-token---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: node-autoapprove-certificate-rotationroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientsubjects:- apiGroup: rbac.authorization.k8s.io  kind: Group  name: system:nodes---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:kube-apiserver-to-kubeletrules:  - apiGroups:      - &quot;&quot;    resources:      - nodes/proxy      - nodes/stats      - nodes/log      - nodes/spec      - nodes/metrics    verbs:      - &quot;*&quot;---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: system:kube-apiserver  namespace: &quot;&quot;roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:kube-apiserver-to-kubeletsubjects:  - apiGroup: rbac.authorization.k8s.io    kind: User    name: kube-apiserverEOF# 2 应用kubectl apply -f bootstrap-secret.yaml </code></pre><h4 id="9-部署worker节点"><a href="#9-部署worker节点" class="headerlink" title="9. 部署worker节点"></a>9. 部署worker节点</h4><ol><li>复制证书</li></ol><pre><code class="bash">for NODE in k8s-master02 k8s-master03 k8s-worker01 k8s-worker02 k8s-worker03; do     echo $NODE     ssh $NODE &quot;mkdir -p /root/certs/kube&#123;config,rnetes&#125;&quot;     for FILE in k8s-ca.pem k8s-ca-key.pem front-proxy-ca.pem; do       scp kubernetes/$FILE $NODE:/root/certs/kubernetes/$&#123;FILE&#125;     done     scp kubeconfig/bootstrap-kubelet.kubeconfig $NODE:/root/certs/kubeconfig/done</code></pre><ol start="2"><li>启动kubelet服务</li></ol><pre><code class="bash">#     - 在&quot;10-kubelet.con&quot;文件中使用&quot;--kubeconfig&quot;指定的&quot;kubelet.kubeconfig&quot;文件并不存在，这个证书文件后期会自动生成;#    - 对于&quot;clusterDNS&quot;是NDS地址，我们可以自定义，比如&quot;10.100.0.254&quot;;#    - &quot;clusterDomain&quot;对应的是域名信息，要和我们设计的集群保持一致，比如默认值&quot;cluster.local&quot;;#    - &quot;10-kubelet.conf&quot;文件中的&quot;ExecStart=&quot;需要写2次，否则可能无法启动kubelet;# 1.所有节点创建工作目录mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/# 2.所有节点创建kubelet的配置文件cat &gt; /etc/kubernetes/kubelet-conf.yml &lt;&lt;&#39;EOF&#39;apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0port: 10250readOnlyPort: 10255authentication:  anonymous:    enabled: false  webhook:    cacheTTL: 2m0s    enabled: true  x509:    clientCAFile: /root/certs/kubernetes/k8s-ca.pemauthorization:  mode: Webhook  webhook:    cacheAuthorizedTTL: 5m0s    cacheUnauthorizedTTL: 30scgroupDriver: systemdcgroupsPerQOS: trueclusterDNS:- 10.100.0.254clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5evictionHard:  imagefs.available: 15%  memory.available: 100Mi  nodefs.available: 10%  nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000maxPods: 110nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0sEOF# 3.所有节点配置kubelet servicecat &gt;  /usr/lib/systemd/system/kubelet.service &lt;&lt;&#39;EOF&#39;[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/kubernetes/kubernetesAfter=containerd.serviceRequires=containerd.service[Service]ExecStart=/usr/local/bin/kubeletRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.targetEOF# 4.所有节点配置kubelet service的配置文件cat &gt; /etc/systemd/system/kubelet.service.d/10-kubelet.conf &lt;&lt;&#39;EOF&#39;[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/root/certs/kubeconfig/bootstrap-kubelet.kubeconfig --kubeconfig=/root/certs/kubeconfig/kubelet.kubeconfig&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&quot;Environment=&quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot;Environment=&quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=&#39;&#39; &quot;ExecStart=ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGSEOF参数解释：# 这是一个Kubelet的配置文件，用于配置Kubelet的各项参数。# # - apiVersion: kubelet.config.k8s.io/v1beta1：指定了配置文件的API版本为kubelet.config.k8s.io/v1beta1。# - kind: KubeletConfiguration：指定了配置类别为KubeletConfiguration。# - address: 0.0.0.0：指定了Kubelet监听的地址为0.0.0.0。# - port: 10250：指定了Kubelet监听的端口为10250。# - readOnlyPort: 10255：指定了只读端口为10255，用于提供只读的状态信息。# - authentication：指定了认证相关的配置信息。#   - anonymous.enabled: false：禁用了匿名认证。#   - webhook.enabled: true：启用了Webhook认证。#   - x509.clientCAFile: /etc/kubernetes/pki/ca.pem：指定了X509证书的客户端CA文件路径。# - authorization：指定了授权相关的配置信息。#   - mode: Webhook：指定了授权模式为Webhook。#   - webhook.cacheAuthorizedTTL: 5m0s：指定了授权缓存时间段为5分钟。#   - webhook.cacheUnauthorizedTTL: 30s：指定了未授权缓存时间段为30秒。# - cgroupDriver: systemd：指定了Cgroup驱动为systemd。# - cgroupsPerQOS: true：启用了每个QoS类别一个Cgroup的设置。# - clusterDNS: 指定了集群的DNS服务器地址列表。#   - 10.96.0.10：指定了DNS服务器地址为10.96.0.10。# - clusterDomain: cluster.local：指定了集群的域名后缀为cluster.local。# - containerLogMaxFiles: 5：指定了容器日志文件保留的最大数量为5个。# - containerLogMaxSize: 10Mi：指定了容器日志文件的最大大小为10Mi。# - contentType: application/vnd.kubernetes.protobuf：指定了内容类型为protobuf。# - cpuCFSQuota: true：启用了CPU CFS Quota。# - cpuManagerPolicy: none：禁用了CPU Manager。# - cpuManagerReconcilePeriod: 10s：指定了CPU管理器的调整周期为10秒。# - enableControllerAttachDetach: true：启用了控制器的挂载和拆卸。# - enableDebuggingHandlers: true：启用了调试处理程序。# - enforceNodeAllocatable: 指定了强制节点可分配资源的列表。#   - pods：强制节点可分配pods资源。# - eventBurst: 10：指定了事件突发的最大数量为10。# - eventRecordQPS: 5：指定了事件记录的最大请求量为5。# - evictionHard: 指定了驱逐硬性限制参数的配置信息。#   - imagefs.available: 15%：指定了镜像文件系统可用空间的限制为15%。#   - memory.available: 100Mi：指定了可用内存的限制为100Mi。#   - nodefs.available: 10%：指定了节点文件系统可用空间的限制为10%。#   - nodefs.inodesFree: 5%：指定了节点文件系统可用inode的限制为5%。# - evictionPressureTransitionPeriod: 5m0s：指定了驱逐压力转换的时间段为5分钟。# - failSwapOn: true：指定了在发生OOM时禁用交换分区。# - fileCheckFrequency: 20s：指定了文件检查频率为20秒。# - hairpinMode: promiscuous-bridge：设置了Hairpin Mode为&quot;promiscuous-bridge&quot;。# - healthzBindAddress: 127.0.0.1：指定了健康检查的绑定地址为127.0.0.1。# - healthzPort: 10248：指定了健康检查的端口为10248。# - httpCheckFrequency: 20s：指定了HTTP检查的频率为20秒。# - imageGCHighThresholdPercent: 85：指定了镜像垃圾回收的上阈值为85%。# - imageGCLowThresholdPercent: 80：指定了镜像垃圾回收的下阈值为80%。# - imageMinimumGCAge: 2m0s：指定了镜像垃圾回收的最小时间为2分钟。# - iptablesDropBit: 15：指定了iptables的Drop Bit为15。# - iptablesMasqueradeBit: 14：指定了iptables的Masquerade Bit为14。# - kubeAPIBurst: 10：指定了KubeAPI的突发请求数量为10个。# - kubeAPIQPS: 5：指定了KubeAPI的每秒请求频率为5个。# - makeIPTablesUtilChains: true：指定了是否使用iptables工具链。# - maxOpenFiles: 1000000：指定了最大打开文件数为1000000。# - maxPods: 110：指定了最大的Pod数量为110。# - nodeStatusUpdateFrequency: 10s：指定了节点状态更新的频率为10秒。# - oomScoreAdj: -999：指定了OOM Score Adjustment为-999。# - podPidsLimit: -1：指定了Pod的PID限制为-1，表示无限制。# - registryBurst: 10：指定了Registry的突发请求数量为10个。# - registryPullQPS: 5：指定了Registry的每秒拉取请求数量为5个。# - resolvConf: /etc/resolv.conf：指定了resolv.conf的文件路径。# - rotateCertificates: true：指定了是否轮转证书。# - runtimeRequestTimeout: 2m0s：指定了运行时请求的超时时间为2分钟。# - serializeImagePulls: true：指定了是否序列化镜像拉取。# - staticPodPath: /etc/kubernetes/manifests：指定了静态Pod的路径。# - streamingConnectionIdleTimeout: 4h0m0s：指定了流式连接的空闲超时时间为4小时。# - syncFrequency: 1m0s：指定了同步频率为1分钟。# - volumeStatsAggPeriod: 1m0s：指定了卷统计聚合周期为1分钟。# 5.启动所有节点kubeletsystemctl daemon-reloadsystemctl enable --now kubeletsystemctl status kubelet# 6.在所有master节点上查看nodes信息。kubectl get nodes# 7.可以查看到有相应的csr用户客户端的证书请求kubectl get csr</code></pre><ol start="3"><li>启动kube-proxy服务</li></ol><pre><code class="bash"># 1.所有节点创建kube-proxy.conf配置文件，cat &gt; /etc/kubernetes/kube-proxy.yml &lt;&lt; EOFapiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationbindAddress: 0.0.0.0metricsBindAddress: 127.0.0.1:10249clientConnection:  acceptConnection: &quot;&quot;  burst: 10  contentType: application/vnd.kubernetes.protobuf  kubeconfig: /root/certs/kubeconfig/kube-proxy.kubeconfig  qps: 5clusterCIDR: 10.100.0.0/16configSyncPeriod: 15m0sconntrack:  max: null  maxPerCore: 32768  min: 131072  tcpCloseWaitTimeout: 1h0m0s  tcpEstablishedTimeout: 24h0m0senableProfiling: falsehealthzBindAddress: 0.0.0.0:10256hostnameOverride: &quot;&quot;iptables:  masqueradeAll: false  masqueradeBit: 14  minSyncPeriod: 0sipvs:  masqueradeAll: true  minSyncPeriod: 5s  scheduler: &quot;rr&quot;  syncPeriod: 30smode: &quot;ipvs&quot;nodeProtAddress: nulloomScoreAdj: -999portRange: &quot;&quot;udpIdelTimeout: 250msEOF# 2.所有节点使用systemd管理kube-proxycat &gt; /usr/lib/systemd/system/kube-proxy.service &lt;&lt; EOF[Unit]Description=Jason Yin&#39;s Kubernetes ProxyAfter=network.target[Service]ExecStart=/usr/local/bin/kube-proxy \  --config=/etc/kubernetes/kube-proxy.yml \  --v=2 Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 3.所有节点启动kube-proxysystemctl daemon-reload &amp;&amp; systemctl enable --now kube-proxysystemctl status kube-proxy</code></pre><h3 id="2-6-部署CNI网络插件"><a href="#2-6-部署CNI网络插件" class="headerlink" title="2.6. 部署CNI网络插件"></a>2.6. 部署CNI网络插件</h3><h4 id="1-安装Calico"><a href="#1-安装Calico" class="headerlink" title="1. 安装Calico"></a>1. 安装Calico</h4><p>使用calico.yaml方式安装</p><pre><code class="bash"># 1.更改calico网段wget https://mirrors.chenby.cn/https://github.com/projectcalico/calico/blob/master/manifests/calico-typha.yamlcp calico-typha.yaml calico.yamlvim calico.yaml# 打开注释CALICO_IPV4POOL_CIDR    - name: CALICO_IPV4POOL_CIDR      value: &quot;172.16.0.0/16&quot;# 若docker镜像拉不下来，可以使用国内的仓库sed -i &quot;s#docker.io/calico/#m.daocloud.io/docker.io/calico/#g&quot; calico.yaml kubectl apply -f calico.yaml# 验证podkubectl get pod -A</code></pre><h4 id="2-安装Prometheus监控"><a href="#2-安装Prometheus监控" class="headerlink" title="2. 安装Prometheus监控"></a>2. 安装Prometheus监控</h4><pre><code class="bash">wget https://raw.githubusercontent.com/cilium/cilium/1.15.1/examples/kubernetes/addons/prometheus/monitoring-example.yamlsed -i &quot;s#docker.io/#m.daocloud.io/docker.io/#g&quot; monitoring-example.yamlkubectl  apply -f monitoring-example.yaml# 安装成功后，可修改monitor的svc为NodePort类型，对方开放访问。</code></pre><h3 id="2-7-安装CoreDNS"><a href="#2-7-安装CoreDNS" class="headerlink" title="2.7. 安装CoreDNS"></a>2.7. 安装CoreDNS</h3><blockquote><p>corDNS需要使用helm 安装，如没有安装helm，请参考：9.2.1 安装helm</p></blockquote><pre><code class="bash"># 下载tgz包helm repo add coredns https://coredns.github.io/helmhelm pull coredns/corednstar xvf coredns-*.tgzcd coredns/# 修改IP地址vim values.yamlcat values.yaml | grep clusterIP:clusterIP: &quot;10.100.0.254&quot;# 修改为国内源 docker源可选sed -i &quot;s#coredns/#m.daocloud.io/docker.io/coredns/#g&quot; values.yamlsed -i &quot;s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g&quot; values.yaml# 默认参数安装helm install  coredns ./coredns/ -n kube-system</code></pre><h3 id="2-8-安装Metrics-Server"><a href="#2-8-安装Metrics-Server" class="headerlink" title="2.8. 安装Metrics Server"></a>2.8. 安装Metrics Server</h3><p>在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。</p><pre><code class="bash"># 单机版 wget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml# 高可用版本(我第一次安装使用的高可用版本)# wget https://mirrors.chenby.cn/https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability.yaml# 修改配置vim components.yaml# vim high-availability.yaml# 需要添加内容的3处地方---# 1- args:        - --cert-dir=/tmp        - --secure-port=4443        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname        - --kubelet-use-node-status-port        - --metric-resolution=15s        - --kubelet-insecure-tls        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem        - --requestheader-username-headers=X-Remote-User        - --requestheader-group-headers=X-Remote-Group        - --requestheader-extra-headers-prefix=X-Remote-Extra-# 2        volumeMounts:        - mountPath: /tmp          name: tmp-dir        - name: ca-ssl          mountPath: /etc/kubernetes/pki# 3      volumes:      - emptyDir: &#123;&#125;        name: tmp-dir      - name: ca-ssl        hostPath:          path:  /root/certs/kubernetes---# 修改为国内源 docker源可选sed -i &quot;s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g&quot; components.yaml# sed -i &quot;s#registry.k8s.io/#m.daocloud.io/registry.k8s.io/#g&quot; high-availability.yaml# 二选一kubectl apply -f components.yaml# kubectl apply -f high-availability.yamlkubectl  top node</code></pre><h3 id="2-9-安装Dashboard"><a href="#2-9-安装Dashboard" class="headerlink" title="2.9. 安装Dashboard"></a>2.9. 安装Dashboard</h3><pre><code class="bash"># 1wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml# 2.修改recommended.yaml中service的相关部分，可以临时使用nodeport的方式访问kind: ServiceapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  type: NodePort  # 添加这个NodePort  ports:    - port: 443      targetPort: 8443      nodePort: 32443  # 添加这个端口  selector:    k8s-app: kubernetes-dashboard# 3.镜像地址替换(此处阿里云仓库为公开，可以随意使用)sed -i &quot;s#kubernetesui/#registry.cn-hangzhou.aliyuncs.com/lzb-kubernetesui/#g&quot; recommended.yamlkubectl apply -f recommended.yaml# 4.查看dashboard是否启动成功kubectl  get pod -Akubectl get svc kubernetes-dashboard -n kubernetes-dashboard# 5.创建tokencat &gt; dashboard-user.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata:  name: admin-user  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: admin-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: admin-user  namespace: kube-systemEOFkubectl  apply -f dashboard-user.yaml# 创建tokenkubectl -n kube-system create token admin-user# 6.登录Dashboard</code></pre><h2 id="三、集群验证"><a href="#三、集群验证" class="headerlink" title="三、集群验证"></a>三、集群验证</h2><h3 id="1-部署pod资源"><a href="#1-部署pod资源" class="headerlink" title="1. 部署pod资源"></a>1. 部署pod资源</h3><pre><code class="bash">cat&lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata:  name: busybox  namespace: defaultspec:  containers:  - name: busybox    image: docker.io/library/busybox:1.28    command:      - sleep      - &quot;3600&quot;    imagePullPolicy: IfNotPresent  restartPolicy: AlwaysEOF# 查看kubectl  get pod</code></pre><h3 id="2-用pod解析默认命名空间中的kubernetes"><a href="#2-用pod解析默认命名空间中的kubernetes" class="headerlink" title="2. 用pod解析默认命名空间中的kubernetes"></a>2. 用pod解析默认命名空间中的kubernetes</h3><pre><code class="bash">kubectl get svc# 进行解析kubectl exec  busybox -n default -- nslookup kubernetesServer:   10.100.0.254Address 1: 10.100.0.10 coredns-coredns.kube-system.svc.cluster.localName:      kubernetesAddress 1: 10.100.0.1 kubernetes.default.svc.cluster.local</code></pre><h3 id="3-测试跨命名空间是否可以解析"><a href="#3-测试跨命名空间是否可以解析" class="headerlink" title="3. 测试跨命名空间是否可以解析"></a>3. 测试跨命名空间是否可以解析</h3><pre><code class="bash"># 查看有那些namekubectl  get svc -A# 进行解析kubectl exec  busybox -n default -- nslookup coredns-coredns.kube-systemServer:   10.100.0.254Address 1: 10.100.0.10 coredns-coredns.kube-system.svc.cluster.localName:      coredns-coredns.kube-systemAddress 1: 10.100.0.10 coredns-coredns.kube-system.svc.cluster.local</code></pre><h3 id="4-每个节点都必须要能访问Kubernetes的kubernetes-svc-443和kube-dns的service-53"><a href="#4-每个节点都必须要能访问Kubernetes的kubernetes-svc-443和kube-dns的service-53" class="headerlink" title="4. 每个节点都必须要能访问Kubernetes的kubernetes svc 443和kube-dns的service 53"></a>4. 每个节点都必须要能访问Kubernetes的kubernetes svc 443和kube-dns的service 53</h3><pre><code class="bash"># 所有节点执行telnet 10.100.0.1 443Trying 10.100.0.1...Connected to 10.100.0.1.Escape character is &#39;^]&#39;.telnet 10.100.0.10 53Trying 10.100.0.10...Connected to 10.100.0.10.Escape character is &#39;^]&#39;.curl 10.100.0.10:53curl: (52) Empty reply from server</code></pre><h3 id="5-Pod和Pod之前要能通"><a href="#5-Pod和Pod之前要能通" class="headerlink" title="5. Pod和Pod之前要能通"></a>5. Pod和Pod之前要能通</h3><pre><code class="bash">kubectl get po -owidekubectl get po -n kube-system -owide# 进入busybox ping其他节点上的podkubectl exec -ti busybox -- sh/ # ping 192.168.0.34PING 192.168.0.34 (192.168.0.34): 56 data bytes64 bytes from 192.168.0.34: seq=0 ttl=63 time=42.624 ms64 bytes from 192.168.0.34: seq=1 ttl=63 time=0.530 ms64 bytes from 192.168.0.34: seq=2 ttl=63 time=0.568 ms64 bytes from 192.168.0.34: seq=3 ttl=63 time=1.066 ms# 可以连通证明这个pod是可以跨命名空间和跨主机通信的</code></pre><h3 id="6-创建三个副本，可以看到3个副本分布在不同的节点上"><a href="#6-创建三个副本，可以看到3个副本分布在不同的节点上" class="headerlink" title="6. 创建三个副本，可以看到3个副本分布在不同的节点上"></a>6. 创建三个副本，可以看到3个副本分布在不同的节点上</h3><pre><code class="bash">at &gt; deployments.yaml &lt;&lt; EOFapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80EOFkubectl  apply -f deployments.yaml # 查看nginx分布在3个节点上kubectl  get pod -o wide# 删除nginxkubectl delete -f deployments.yaml</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、高可用架构&quot;&gt;&lt;a href=&quot;#一、高可用架构&quot; class=&quot;headerlink&quot; title=&quot;一、高可用架构&quot;&gt;&lt;/a&gt;一、高可用架构&lt;/h2&gt;&lt;h3 id=&quot;1、架构图&quot;&gt;&lt;a href=&quot;#1、架构图&quot; class=&quot;headerlink&quot; ti</summary>
      
    
    
    
    <category term="虚拟化" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    <category term="Kubernetes" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>K8s学习笔记</title>
    <link href="https://www.langxw.com/2024/02/06/K8s%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.langxw.com/2024/02/06/K8s%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2024-02-06T10:25:15.000Z</published>
    <updated>2024-02-06T10:35:24.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、存储"><a href="#一、存储" class="headerlink" title="一、存储"></a>一、存储</h2><h3 id="1、StorageClass和volumeClaimTemplates"><a href="#1、StorageClass和volumeClaimTemplates" class="headerlink" title="1、StorageClass和volumeClaimTemplates"></a>1、StorageClass和volumeClaimTemplates</h3><ul><li><p>storageClass是PV的自动化实现，动态PV</p></li><li><p>volumeClaimTemplates是PVC的自动化实现，用在StatefulSet里</p></li></ul><h3 id="2、PV和PVC"><a href="#2、PV和PVC" class="headerlink" title="2、PV和PVC"></a>2、PV和PVC</h3><ul><li><p>PV和PVC是成对存在的。</p></li><li><p>PV默认会先和PVC进行绑定，再进行Pod的调度和创建。</p></li><li><p>PV可以通过StorageClass设置延时绑定，即Pod完成调度，再去和PVC绑定。</p></li></ul><h2 id="二、可观测性"><a href="#二、可观测性" class="headerlink" title="二、可观测性"></a>二、可观测性</h2><h3 id="1、-三种探针"><a href="#1、-三种探针" class="headerlink" title="1、 三种探针"></a>1、 三种探针</h3><ul><li><p>readinessProbe（可以没有），就绪探针，失败就由kube-proxy拆除流量</p></li><li><p>livenessProbe（一定要有），存活探针，失败就由kubelet杀死重启</p></li><li><p>startupProbe(不一定要有，第一优先级)，解决启动慢的问题。</p></li><li><p>livenessProbe和readinessProbe可以一致，即服务假死就重启Pod。</p></li><li><p>startup探针优先级最高，readness和liveness在启动的时候可能是并行的。</p></li></ul><h3 id="2、-重启策略"><a href="#2、-重启策略" class="headerlink" title="2、 重启策略"></a>2、 重启策略</h3><ul><li><p>默认Always，总是重启（包括正常退出0和异常退出）</p></li><li><p>Onfailure 异常失败才重启（非0状态，OOM等异常）</p></li><li><p>Never不重启</p></li></ul><h3 id="3、-远程调试service"><a href="#3、-远程调试service" class="headerlink" title="3、 远程调试service"></a>3、 远程调试service</h3><ul><li><p>port-forward 将集群中服务代理到本地，本地可以访问集群中服务（正向流量）</p></li><li><p>Telepresence 将本地服务代理到集群中，集群可以访问本地服务（反向流量）–swap-deployment （将本地的开发程序插入到kubernetes集群内部，包括DNS）</p></li></ul><h3 id="4、-调试node和pod"><a href="#4、-调试node和pod" class="headerlink" title="4、 调试node和pod"></a>4、 调试node和pod</h3><ul><li>使用kubectl debug 附加一个Ephemeral Containers到pod中，可共享网络、磁盘，进程等。在pod不断重启、pod中不存在调试工具的时候，推荐使用。</li></ul><h2 id="二、Pod"><a href="#二、Pod" class="headerlink" title="二、Pod"></a>二、Pod</h2><h3 id="1、静态Pod"><a href="#1、静态Pod" class="headerlink" title="1、静态Pod"></a>1、静态Pod</h3><h4 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h4><p>静态Pod直接由特定节点上的kubelet进程来管理，不通过master节点上的apiserver。无法与我们常用的控制器<code>Deployment</code>或者<code>DaemonSet</code>进行关联，它由<code>kubelet</code>进程自己来监控，当<code>pod</code>崩溃时重启该<code>pod</code>，<code>kubelet</code>也无法对他们进行健康检查。静态 pod 始终绑定在某一个<code>kubelet</code>，并且始终运行在同一个节点上。 <code>kubelet</code>会自动为每一个静态 pod 在 Kubernetes 的 apiserver 上创建一个镜像 Pod（Mirror Pod），因此我们可以在 apiserver 中查询到该 pod，但是不能通过 apiserver 进行控制（例如不能删除）</p><h4 id="1-2-创建方式"><a href="#1-2-创建方式" class="headerlink" title="1.2 创建方式"></a>1.2 创建方式</h4><ul><li><p><strong>配置文件</strong>：一般默认在<code>/etc/kubernetes/manifest/</code>目录下存在yaml文件</p></li><li><p><strong>HTTP</strong>：kubelet 周期地从<code>–manifest-url=</code>参数指定的地址下载文件，并且把它翻译成 JSON/YAML 格式的 pod。</p></li></ul><h4 id="1-3-其他"><a href="#1-3-其他" class="headerlink" title="1.3 其他"></a>1.3 其他</h4><ul><li><p>通过命令<code>systemctl status kubelet</code>查看kubelet的配置文件路径为 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，其中<code>Environment=&quot;KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests</code> 参数代表了静态pod的目录。</p></li><li><p>在master节点上可以看到<code>etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml</code> master 节点上面的几个重要组件都是用静态 Pod 的方式运行的。这些Pod不会受到apiserver控制，同时防止误删除apiserver自己删除自己。</p></li></ul><h3 id="2、Pod更新策略"><a href="#2、Pod更新策略" class="headerlink" title="2、Pod更新策略"></a>2、Pod更新策略</h3><ul><li><p>Deployment的更新策略：RollingUpdate和Recreate</p></li><li><p>StatefulSet的更新策略：RollingUpdate和OnDelete（禁止自动删除，必须手动删除才能出发更新重建。）</p></li></ul><h2 id="三、容器运行时"><a href="#三、容器运行时" class="headerlink" title="三、容器运行时"></a>三、容器运行时</h2><h3 id="1、k8s的cgroup驱动"><a href="#1、k8s的cgroup驱动" class="headerlink" title="1、k8s的cgroup驱动"></a>1、k8s的cgroup驱动</h3><ul><li><p>cgroupfs和systemd是linux系统cgroup的两种不同实现方式，systemd操作简单，且在一些发行版上默认支持使用。</p></li><li><p>K8s的cgroup驱动官方推荐选择systemd，和宿主机操作系统的cgroup、容器运行时的cgroup保持一致。</p></li><li><p>如果K8s和docker的cgroup驱动选择cgroupfs，而宿主机节点是systemd，在负载压力大的情况下，会导致k8s系统不稳定。</p></li><li><p>k8s和容器运行时要使用相同的cgroup驱动，最好和操作系统也一致，即systemd</p></li></ul><h2 id="四、调度"><a href="#四、调度" class="headerlink" title="四、调度"></a>四、调度</h2><h3 id="1、Taints和Toleration"><a href="#1、Taints和Toleration" class="headerlink" title="1、Taints和Toleration"></a>1、Taints和Toleration</h3><ul><li><p>节点亲和性是pod和node的亲和，pod被调度到特定的node上。</p></li><li><p>污点taints则相反，是一种洁癖，他排斥一类特定的Pod（不能容忍这个污点的Pod）。</p></li><li><p>容忍度Toleration是应用于Pod上的，如果Pod不容忍node的污点，则不能被调度到这个Node上</p></li></ul><p>比如： <code>kubectl taint nodes node1 key1=value1:NoSchedule</code> 在说，我有洁癖，key1=value1且不能调度；如果一个pod 有<code>tolerations:key: &quot;key1&quot; operator: &quot;Equal&quot; value: “value1” effect: &quot;NoSchedule&quot;</code></p><p>的容忍度标签，代表我接受key1=value1且效果为不能调度的洁癖，他就可以调度到node1上。当然也可以调度到其他没这个污点的node上。</p><ul><li><p>污点的作用：专用节点（别人别和我抢）、我有特殊硬件（GPU）、基于污点的驱逐。</p></li><li><p>Pod的容忍度和node的污点要在KV和调度策略两方面都匹配才行。</p></li></ul><h2 id="五、安全"><a href="#五、安全" class="headerlink" title="五、安全"></a>五、安全</h2><h3 id="1、K8s-API请求"><a href="#1、K8s-API请求" class="headerlink" title="1、K8s API请求"></a>1、K8s API请求</h3><ul><li><p>用户通过kubectl请求api</p></li><li><p>pod等通过service account请求api</p></li></ul><h3 id="2、K8s支持的请求认证方式"><a href="#2、K8s支持的请求认证方式" class="headerlink" title="2、K8s支持的请求认证方式"></a>2、K8s支持的请求认证方式</h3><ul><li><p>Basic认证</p></li><li><p>X509证书认证–集群间组件通讯</p></li><li><p>JWT（Json Web Token）</p><ul><li><p>Service Account</p></li><li><p>OpenID Connet</p></li><li><p>Webhooks</p></li></ul></li><li><p>Kubernetes中的用户通常是通过请求凭证设置</p><ul><li>User=dahu</li><li>Groups=[“tester”,“developer”]</li></ul></li><li><p>service account 对用的token 会被装载到secret中</p></li></ul><h3 id="3、RBAC"><a href="#3、RBAC" class="headerlink" title="3、RBAC"></a>3、RBAC</h3><ul><li><p><strong>Role</strong>：作用于指定的命名空间，定义在某个命名空间资源上可以进行的操作。</p></li><li><p><strong>ClusterRole</strong>：作用于集群所有命名空间，定义针对所有命名空间的资源进行的操作。</p></li><li><p>RoleBinding和ClusterRoleBinding差别仅在于ns的有无。</p></li><li><p>默认ClusterRolebinding</p><ul><li><p>system:basic-user，未认证用户组(group system:unauthenticated)的默认角色，不具备任何操作权限</p></li><li><p>cluster-admin，是system:master组默认的集群角色绑定，具体集群所有资源操作权限。</p></li><li><p>集群系统组件都有默认clusterrolebing，包括controller-manage、scheduler、kube-proxy等。</p></li></ul></li><li><p>安全上下文</p><ul><li><p>权限最小化原则</p></li><li><p>在pod或者container维度设置Security Context</p></li><li><p>开启PSP</p></li></ul></li></ul><h3 id="4、多租安全加固"><a href="#4、多租安全加固" class="headerlink" title="4、多租安全加固"></a>4、多租安全加固</h3><ul><li><p>RBAC和基于namespace的软隔离是基本且必要的安全措施</p></li><li><p>使用PSP(Pod Security Policies)对Pod的安全参数进行校验，同时加固Pod运行时刻安全</p></li><li><p>使用Rerouce Quota &amp; Limit Range限制租户的资源使用配额</p></li><li><p>敏感信息保护（secret encryption at REST）</p></li><li><p>在应用运行时刻遵循权限的最小化原则，尽可能缩小podn内容器的系统权限</p></li><li><p>使用NetworkPolicy进行业务应用间东西向网络流量的访问控制</p></li><li><p>Log everything（包括终端日志）</p></li><li><p>对接监控系统，实现容器应用维度的监控</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、存储&quot;&gt;&lt;a href=&quot;#一、存储&quot; class=&quot;headerlink&quot; title=&quot;一、存储&quot;&gt;&lt;/a&gt;一、存储&lt;/h2&gt;&lt;h3 id=&quot;1、StorageClass和volumeClaimTemplates&quot;&gt;&lt;a href=&quot;#1、StorageC</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.langxw.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Kubernetes" scheme="https://www.langxw.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>容器内抓包</title>
    <link href="https://www.langxw.com/2023/12/18/%E5%AE%B9%E5%99%A8%E5%86%85%E6%8A%93%E5%8C%85/"/>
    <id>https://www.langxw.com/2023/12/18/%E5%AE%B9%E5%99%A8%E5%86%85%E6%8A%93%E5%8C%85/</id>
    <published>2023-12-18T02:03:29.000Z</published>
    <updated>2023-12-18T06:38:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、使用脚本一键进入Pod-netns-抓包"><a href="#一、使用脚本一键进入Pod-netns-抓包" class="headerlink" title="一、使用脚本一键进入Pod netns 抓包"></a>一、使用脚本一键进入Pod netns 抓包</h2><ol><li>执行以下命令，获取该 Pod 副本所在的节点和 Pod 名称。</li></ol><pre><code class="Plaintext">kubectl get pod -o wide</code></pre><ol start="2"><li>登录到Pod所在的节点</li></ol><pre><code class="shell">#!/usr/bin/env bashfunction debug_net() &#123;  set -eu  echo $pod_name  containerID=$(crictl ps -q --label io.kubernetes.pod.name=$pod_name |sed -n 1p)  echo $containerID  pid=$(crictl inspect $&#123;containerID&#125; |grep -i pid --m 1 |awk &#39;&#123;print $2 &#125;&#39; |awk -F, &#39;&#123;print $1&#125;&#39;)  echo $pid  cmd=&quot;nsenter -n -t $&#123;pid&#125;&quot;  echo -e &quot;\033[32m Execute the command: $&#123;cmd&#125; \033[0m&quot;  $&#123;cmd&#125;    &#125;# 运行函数pod_name=$1debug_net</code></pre><p>在节点上执行命令进入Pod的容器的网络命名空间</p><pre><code class="Bash">./debug.sh &#123;PodName&#125;ip a ## 验证是否已经进入 查看ip是否是容器ip</code></pre><p>使用示例:</p><p><a href="https://imgse.com/i/pi5zNVI"><img src="https://s11.ax1x.com/2023/12/18/pi5zNVI.png" alt="pi5zNVI.png"></a></p><p>在Pod的网络命名空间内抓包命令:</p><pre><code class="shell">nohup tcpdump -i any tcp and host 域名 -C 100M -W 10 -w /tmp/debug-&#123;podName&#125;.pcap &amp;</code></pre><p>在节点上抓包不需要进入容器的网络命名空间</p><pre><code class="shell">nohup tcpdump -i any tcp and host 域名 -C 100M -W 10 -w /tmp/debug-&#123;NodeName&#125;.pcap &amp;</code></pre><hr><h2 id="二、定制脚本对单个节点多个容器进行抓包，一键执行抓包"><a href="#二、定制脚本对单个节点多个容器进行抓包，一键执行抓包" class="headerlink" title="二、定制脚本对单个节点多个容器进行抓包，一键执行抓包"></a>二、定制脚本对单个节点多个容器进行抓包，一键执行抓包</h2><pre><code class="Bash">#!/usr/bin/env bashfunction debug_net() &#123;  set -eu  echo $pod_name  podID=$(crictl pods |grep $&#123;pod_name&#125;|awk &#39;&#123;print $1&#125;&#39;)  echo &quot;--&gt; $&#123;podID&#125;&quot;  pid=`crictl pods |grep $pod_name |awk &#39;&#123;print $1&#125;&#39; |xargs -n 1 -I &#123;&#125; sh -c &#39;crictl inspect $(crictl ps --pod &#123;&#125; -q) |grep -i pid --m 1&#39; |awk &#39;&#123;print $2&#125;&#39;|awk -F, &#39;&#123;print $1&#125;&#39;`  for p in $&#123;pid&#125;  do          echo -e &quot;\033[32m currunt PID: $p \033[0m&quot;        nohup  nsenter -n -t $&#123;p&#125; sh -c &quot;tcpdump -i any port 80 and host img-faceplay-dy-1300308946.cos.ap-guangzhou.myqcloud.com or host sh-segment-dp.oss-cn-shanghai.aliyuncs.com -C 100M -W 50 -w /tmp/debug-$p.pcap &quot; &amp;          echo -e &quot;\033[32m tcpdump started \033[0m&quot;  done&#125;# 运行函数pod_name=$1debug_net</code></pre><p>保存名称为容器pid.pcapxx</p><p>使用示例:</p><pre><code class="Bash">./debug.sh &#123;pod关键字&#125;</code></pre><p><a href="https://imgse.com/i/pi5z7L9"><img src="https://s11.ax1x.com/2023/12/18/pi5z7L9.png" alt="pi5z7L9.png"></a></p><h2 id="三、其他方式"><a href="#三、其他方式" class="headerlink" title="三、其他方式"></a>三、其他方式</h2><p><strong>kubelet debug</strong> </p><pre><code class="bash"> kubectl debug -it -n zhw-test zhw-test-shqclient-deploy-578df65c55-5dvmj \--image=busybox --share-processes \--copy-to=zhw-test-shqclient-deploy-578df65c55-5dvmj-debug \--container=shqclient-container-debug</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、使用脚本一键进入Pod-netns-抓包&quot;&gt;&lt;a href=&quot;#一、使用脚本一键进入Pod-netns-抓包&quot; class=&quot;headerlink&quot; title=&quot;一、使用脚本一键进入Pod netns 抓包&quot;&gt;&lt;/a&gt;一、使用脚本一键进入Pod netns </summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="Shell" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/Shell/"/>
    
    
    <category term="shell" scheme="https://www.langxw.com/tags/shell/"/>
    
    <category term="docker" scheme="https://www.langxw.com/tags/docker/"/>
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Nginx不喜欢下划线_</title>
    <link href="https://www.langxw.com/2023/12/14/Nginx%E4%B8%8D%E5%96%9C%E6%AC%A2%E4%B8%8B%E5%88%92%E7%BA%BF/"/>
    <id>https://www.langxw.com/2023/12/14/Nginx%E4%B8%8D%E5%96%9C%E6%AC%A2%E4%B8%8B%E5%88%92%E7%BA%BF/</id>
    <published>2023-12-14T06:00:56.000Z</published>
    <updated>2023-12-14T09:29:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>最近有发生过一个诡异想象：</p><ol><li><p>生产环境异常，客户端添加的自定义Header：app_versiion，经过nginx和网关后，传到后端服务会丢失。</p></li><li><p>预发布环境正常，预发布和生产的差别在没经过nginx代理。初步判断是Nginx的问题。</p></li><li><p>在生产环境访问日志中打印Header信息$http_app_version，值为空。确认是Nginx丢弃了。</p></li><li><p>联想到之前upstream下划线的问题，怀疑是下划线的问题。更换Header中下划线为中划线即app-version后，再次通过curl模拟请求，Nginx日志正常打印了app-version的Header信息。</p></li></ol><h2 id="二、自定义Header不喜欢下划线"><a href="#二、自定义Header不喜欢下划线" class="headerlink" title="二、自定义Header不喜欢下划线"></a>二、自定义Header不喜欢下划线</h2><p><strong>Nginx会把header中变量的大写字母转化为小写字母，把中划线转换为下划线，再加上前缀http：比如X-LIST 转换为http_x_list。</strong></p><h3 id="1、原因"><a href="#1、原因" class="headerlink" title="1、原因"></a>1、原因</h3><p>为什么会使nginx和apache服务器对名称中带下划线header不做转发呢，查阅相关资料后发现是CGI（公共网关接口Common Gateway Interface，CGI是Web 服务器运行时外部程序的规范）的历史遗留问题如下，大概意思就是 下划线和中划线都为会被映射为 CGI 系统变量中名中的下划线，这样容易引起混淆。官方解释如下：</p><blockquote><p>If you do not explicitly set underscores_in_headers on;, NGINX will silently drop HTTP headers with underscores (which are perfectly valid according to the HTTP standard). This is done in order to prevent ambiguities when mapping headers to CGI variables as both dashes and underscores are mapped to underscores during that process.</p></blockquote><h3 id="2、Nginx关于头部的两个配置"><a href="#2、Nginx关于头部的两个配置" class="headerlink" title="2、Nginx关于头部的两个配置"></a>2、Nginx关于头部的两个配置</h3><ul><li>ignore_invalid_headers 默认on</li></ul><blockquote><p>Controls whether header fields with invalid names should be ignored. Valid names are composed of English letters, digits, hyphens, and possibly underscores (as controlled by the underscores_in_headers directive).</p></blockquote><ul><li>underscores_in_headers  默认off</li></ul><blockquote><p>Enables or disables the use of underscores in client request header fields. When the use of underscores is disabled, request header fields whose names contain underscores are marked as invalid and become subject to the ignore_invalid_headers directive.</p></blockquote><p>上述说明：</p><ol><li><p>underscores_in_headers 默认为off，带有下划线的header默认是invalid。</p></li><li><p>Nginx对valid_headers的规定就是英文字母，数字，连字号和下划线(可以通过underscores_in_headers控制)，默认情况下其他invalid_headers会忽略并丢弃。</p></li></ol><h3 id="3、解决办法"><a href="#3、解决办法" class="headerlink" title="3、解决办法"></a>3、解决办法</h3><ol><li><p><strong>在自定义header中要使用中划线，禁止使用下划线（推荐）。</strong></p></li><li><p>在http或者server中，将underscores_in_headers参数设置为on。</p></li></ol><h3 id="4、nginx的header还禁止dot"><a href="#4、nginx的header还禁止dot" class="headerlink" title="4、nginx的header还禁止dot(.)"></a>4、nginx的header还禁止dot(.)</h3><h2 id="三、Upstream-name不喜欢下划线"><a href="#三、Upstream-name不喜欢下划线" class="headerlink" title="三、Upstream name不喜欢下划线"></a>三、Upstream name不喜欢下划线</h2><p><strong>后端是Java服务</strong>的情况下，在配置upstream的name的时候，如果带有下划线，比如big_data，前端会反回400错误。</p><h3 id="1、原因-1"><a href="#1、原因-1" class="headerlink" title="1、原因"></a>1、原因</h3><ul><li><p>nginx在转发http请求的时候会加上实际的Host请求头，nginx在没有配置<code>proxy_set_header HOST $host</code> 的时候，在转发http请求的时候会默认把upstream的名称作为Host头部的内容。</p></li><li><p>比如<code>proxy_pass http://big_data;</code>中http header中Host字段值为big_data。</p></li><li><p><a href="https://github.com/spring-projects/spring-boot/issues/13236">[tomcat] Spring boot web always return 400 when use a domain name · Issue #13236 · spring-projects/spring-boot · GitHub</a> 从这里可以看出问题的原因是带有下划线的Host的http请求，tomcat认为是有问题。</p></li></ul><h3 id="2、解决办法"><a href="#2、解决办法" class="headerlink" title="2、解决办法"></a>2、解决办法</h3><ol><li><p>upsrteam的名字不是用下划线，使用中划线或者驼峰。</p></li><li><p>在location处设置 <code>proxy_set_header HOST $host</code></p><blockquote><p>这个配置的主要是在nginx在转发http请求的时候会加上实际的Host请求头。如http请求是 <a href="http://abc.com/hello%EF%BC%8C">http://abc.com/hello，</a> 那么nginx在转发http请求的时候会原封不动的把host请求头(Host:abc.com)转发给后台服务。对于nginx而言，如果没有配置proxy_set_header HOST $host的时候会默认修改Host为upstream的名称。</p></blockquote></li></ol><h2 id="四、排查手段"><a href="#四、排查手段" class="headerlink" title="四、排查手段"></a>四、排查手段</h2><h3 id="1、Nginx-错误日志级别"><a href="#1、Nginx-错误日志级别" class="headerlink" title="1、Nginx 错误日志级别"></a>1、Nginx 错误日志级别</h3><p>nginx error日志默认级别是error，只打印重要的错误日志。在调试和定位nginx问题的时候可以提高日志级别为debug 或者 info。<code>error_log logs/app_err.log debug;</code></p><h3 id="2、Curl-模拟请求"><a href="#2、Curl-模拟请求" class="headerlink" title="2、Curl 模拟请求"></a>2、Curl 模拟请求</h3><pre><code class="bash">curl -H &quot;app-version:1.2.3&quot; http://www.abc.com:/health-check</code></pre><h3 id="3、Tcpdump抓包"><a href="#3、Tcpdump抓包" class="headerlink" title="3、Tcpdump抓包"></a>3、Tcpdump抓包</h3><h2 id="五、启示"><a href="#五、启示" class="headerlink" title="五、启示"></a>五、启示</h2><ul><li><p>排查问题，第一是看日志，debug级别的日志。</p></li><li><p>排查问题，第二是看官方文档的相关说明。</p></li><li><p>运维必须系统地学习常用工具组件的官档和教程。</p></li><li><p>最好能看懂源代码，源代码能解决一切疑问。</p></li><li><p>抓包，当实在无法确认问题出在哪一个环节的时候，就抓包。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;最近有发生过一个诡异想象：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;生产环境异常，客户端添加的自定义Header：app_versi</summary>
      
    
    
    
    <category term="中间件" scheme="https://www.langxw.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="Nginx" scheme="https://www.langxw.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Nginx/"/>
    
    
    <category term="nginx" scheme="https://www.langxw.com/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>定期检查某个SSL证书的有效期并告警</title>
    <link href="https://www.langxw.com/2023/12/08/%E5%AE%9A%E6%9C%9F%E6%A3%80%E6%9F%A5%E6%9F%90%E4%B8%AASSL%E8%AF%81%E4%B9%A6%E7%9A%84%E6%9C%89%E6%95%88%E6%9C%9F%E5%B9%B6%E5%91%8A%E8%AD%A6/"/>
    <id>https://www.langxw.com/2023/12/08/%E5%AE%9A%E6%9C%9F%E6%A3%80%E6%9F%A5%E6%9F%90%E4%B8%AASSL%E8%AF%81%E4%B9%A6%E7%9A%84%E6%9C%89%E6%95%88%E6%9C%9F%E5%B9%B6%E5%91%8A%E8%AD%A6/</id>
    <published>2023-12-08T03:48:03.000Z</published>
    <updated>2023-12-08T03:55:28.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>由于历史原因，我司SSL证书分散在不同的云服务器上，更换证书的时候可能会有遗漏。故将分散的SSL证书记录在文件里，定期用脚本检查剩余有效期天数，并告警。可以防止更换遗漏的事情发生。</p><h2 id="二、脚本"><a href="#二、脚本" class="headerlink" title="二、脚本"></a>二、脚本</h2><ul><li>记录ssl证书所在服务器的位置</li></ul><pre><code>/root/ops_scripts/ip_ssl_path.txt10.10.10.1#/usr/local/nginx/conf/cert/didikaihei.com.pem10.10.10.2#/usr/local/nginx/conf/vhost/cert/zuhaowan.com.cn.pem</code></pre><ul><li>设置Crontab定时任务执行脚本，并发送微信告警</li></ul><pre><code class="shell">#!/bin/bashexport LC_ALL=Cerror_ips=&quot;&quot;notify()&#123;   curl &#39;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=5b37f5cc-d1ed-48d4-a4b4-7d4f8a9da478&#39; \   -H &#39;Content-Type: application/json&#39; \   -d &#39;   &#123;        &quot;msgtype&quot;: &quot;text&quot;,        &quot;text&quot;: &#123;            &quot;content&quot;: &quot;SSL证书有效期小于30天的IP列表(非Proxy机器):\n&#39;$1&#39;&quot;,            &quot;mentioned_mobile_list&quot;:[&quot;@all&quot;,]        &#125;   &#125;&#39;&#125;for line in `cat /root/ops_scripts/ip_ssl_path.txt`do        echo &quot;================================&quot;        ip=$(echo $line|awk -F&quot;#&quot; &#39;&#123;print $1&#125;&#39;)        ssl_path=$(echo $line|awk -F&quot;#&quot; &#39;&#123;print $2&#125;&#39;)        echo $ip        end_time=$(ssh $ip &quot;openssl x509 -in $ssl_path -noout -enddate |awk -F &#39;=&#39; &#39;&#123;print \$2&#125;&#39;&quot;)        end_times=`date -d &quot;$end_time&quot; +%s `        current_times=`date -d &quot;$(date -u &#39;+%b %d %T %Y GMT&#39;) &quot; +%s `        let left_time=$end_times-$current_times        days=`expr $left_time / 86400`        echo &quot;剩余天数: &quot; $days        [ $days -lt 31 ] &amp;&amp; echo &quot;https 证书有效期少于30天，存在风险&quot; &amp;&amp; error_ips=&quot;$error_ips#$ip&quot; doneecho -e &quot;准备过期的域名为： \n  $error_ips&quot;if [ &quot;$error_ips&quot; = &quot;&quot; ]then        echo &quot;不包含准备过期的域名&quot;else        notify $error_ips        echo &quot;包含准备过期的域名&quot; fi</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;由于历史原因，我司SSL证书分散在不同的云服务器上，更换证书的时候可能会有遗漏。故将分散的SSL证书记录在文件里，定期用脚本</summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="Shell" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/Shell/"/>
    
    
    <category term="shell" scheme="https://www.langxw.com/tags/shell/"/>
    
    <category term="linux" scheme="https://www.langxw.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>批量检查域名SSL证书的有效期</title>
    <link href="https://www.langxw.com/2023/12/08/%E6%89%B9%E9%87%8F%E6%A3%80%E6%9F%A5%E5%9F%9F%E5%90%8DSSL%E8%AF%81%E4%B9%A6%E7%9A%84%E6%9C%89%E6%95%88%E6%9C%9F/"/>
    <id>https://www.langxw.com/2023/12/08/%E6%89%B9%E9%87%8F%E6%A3%80%E6%9F%A5%E5%9F%9F%E5%90%8DSSL%E8%AF%81%E4%B9%A6%E7%9A%84%E6%9C%89%E6%95%88%E6%9C%9F/</id>
    <published>2023-12-08T03:35:10.000Z</published>
    <updated>2023-12-08T03:45:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>我司的某个主域名有很多子域名，SSL证书到期提示更换，由于没有记录哪个域名使用了https，手动一个个去验证很麻烦，就想到靠一个脚本解决。</p><h2 id="二、脚本"><a href="#二、脚本" class="headerlink" title="二、脚本"></a>二、脚本</h2><p>先将子域名一行一个地写入https_list.txt文件里，然后执行脚本即可。</p><pre><code class="bash">#!/bin/bash# 检测https证书有效echo &#39;开始检查 https证书有效期 &#39;# 先手动写域名到文件https_list.txt中，再读取文件检查证书是否过期了# 例子：#echo &#39;www.baidu.com&#39; &gt;&gt; https_list.txtsource /etc/profile# 定义错误的域名errorDominStr=&quot;&quot;while read line; do    echo &quot;=====================================================================================&quot;    echo &quot;当前检测的域名：&quot; $line    end_time=$(echo | timeout 1 openssl s_client -servername $line -connect $line:443 2&gt;/dev/null | openssl x509 -noout -enddate 2&gt;/dev/null | awk -F &#39;=&#39; &#39;&#123;print $2&#125;&#39;)    ([ $? -ne 0 ] || [[ $end_time == &#39;&#39; ]]) &amp;&amp; echo &#39;该域名链接不上,跳到下一个域名&#39; &amp;&amp; continue    end_times=`date -d &quot;$end_time&quot; +%s `    current_times=`date -d &quot;$(date -u &#39;+%b %d %T %Y GMT&#39;) &quot; +%s `    let left_time=$end_times-$current_times    days=`expr $left_time / 86400`    echo &quot;剩余天数: &quot; $days    [ $days -lt 60 ] &amp;&amp; echo &quot;https 证书有效期少于60天，存在风险&quot;  &amp;&amp; errorDominStr=&quot;$errorDominStr \n $line&quot;done &lt; ~/https_list.txtecho -e &quot;准备过期的域名为： \n  $errorDominStr&quot;if [ &quot;$errorDominStr&quot; = &quot;&quot; ]then  echo &quot;不包含准备过期的域名&quot;else  echo &quot;包含准备过期的域名&quot; &amp;&amp; exit 10fiecho &quot;Good bye!&quot;exit 0</code></pre><p><strong>注意：脚本输出的准备过期的域名提示仅供参考，具体还需要看脚本的详细日志逐个甄别</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;我司的某个主域名有很多子域名，SSL证书到期提示更换，由于没有记录哪个域名使用了https，手动一个个去验证很麻烦，就想到靠</summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="Shell" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/Shell/"/>
    
    
    <category term="shell" scheme="https://www.langxw.com/tags/shell/"/>
    
    <category term="linux" scheme="https://www.langxw.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>命令行sed提取字符串中的字段</title>
    <link href="https://www.langxw.com/2023/12/08/%E5%91%BD%E4%BB%A4%E8%A1%8Csed%E6%8F%90%E5%8F%96%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E7%9A%84%E5%AD%97%E6%AE%B5/"/>
    <id>https://www.langxw.com/2023/12/08/%E5%91%BD%E4%BB%A4%E8%A1%8Csed%E6%8F%90%E5%8F%96%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E7%9A%84%E5%AD%97%E6%AE%B5/</id>
    <published>2023-12-08T03:23:50.000Z</published>
    <updated>2023-12-08T03:26:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>一般切割提取字符串的字段，如果有规律可寻，采用awk或者cut就可以搞定。如果无规律的分隔符，就必须要用正则表达式来提取需要的数据了。</p><h2 id="二、解决办法"><a href="#二、解决办法" class="headerlink" title="二、解决办法"></a>二、解决办法</h2><p>使用sed时，我通常通过搜索除分隔符之外的任何东西来实现非贪婪搜索，直到分隔符为止：</p><pre><code class="bash">echo &quot;http://www.suon.co.uk/product/1/7/3/&quot; | sed -n &#39;s;\(http://[^/]*\)/.*;\1;p&#39;</code></pre><p>输出</p><pre><code class="bash">http://www.suon.co.uk</code></pre><p>这是：</p><ul><li>不输出不打印 <code>-n</code></li><li>s搜索，匹配模式，替换并打印 <code>s/&lt;pattern&gt;/&lt;replace&gt;/p</code></li><li>使用<code>;</code>搜索命令分隔符而不是<code>/</code>使其更容易键入，以便<code>s;&lt;pattern&gt;;&lt;replace&gt;;p</code></li><li>记住括号之间的匹配<code>\(</code>… <code>\)</code>，以后可通过<code>\1</code>，<code>\2</code>… 访问</li><li>固定匹配 <code>http://</code></li><li>后面在括号任何东西<code>[]</code>，<code>[ab/]</code>就意味着无论是<code>a</code>或<code>b</code>或<code>/</code></li><li>首先<code>^</code>是<code>[]</code>中的<code>not</code>，所以<code>[^/]</code>是除了<code>/</code>的其他字符</li><li><code>*</code>是重复前一组，<code>[^/]*</code>表示重复除<code>/</code>以外的字符，也意味着重复到<code>/</code>就结束了。</li><li>到目前为止，<code>sed -n &#39;s;\(http://[^/]*\)</code>表示搜索并记住（提取），<code>http://</code>后面紧跟任何字符，除了<code>/</code>，记住（提取）您找到的内容</li><li>我们要搜索直到域的末尾，所以在下一个<code>/</code>停止，因此<code>/</code>在末尾添加。另一个：<code>sed -n &#39;s;\(http://[^/]*\)/&#39;</code>。（可以不需要）</li><li>我们要匹配域后的其余行，因此添加<code>.*</code></li><li>在组1（<code>\1</code>）中记住的匹配项是域，因此将匹配的行替换为保存在组中<code>\1</code>并把内容打印<code>p</code>出来：<code>sed -n &#39;s;\(http://[^/]*\)/.*;\1;p&#39;</code></li></ul><h2 id="三、实际例子"><a href="#三、实际例子" class="headerlink" title="三、实际例子"></a>三、实际例子</h2><p>提取SQL语句中的某个字段（goods_sku）</p><p>SQL语句如下：</p><pre><code>string=&quot;UPDATE \`db\`.\`purchase_mould\` SET \`update_time\`=1, \`__dropped_col_16__\`=\`2023-11-14 23:27:55\`, \`goods_id\`=\`123123123\`, \`mould_id\`=123123, \`specs_crc\`=123123, \`pid\`=123123, \`state\`=8.00, \`pusername\`=\`aaaa\`, \`protect_price\`=2, \`shop_id\`=123123, \`create_time\`=\`2023-12-02 19:37:56\`, \`game_name\`=\`火影\`, \`goods_sku\`=316123123, \`duration\`=1, \`game_id\`=560, \`mould_name\`=\`鲛肌\`, \`id\`=54916 WHERE \`update_time\`=2 AND \`__dropped_col_16__\`=\`2023-11-14 23:27:55\` AND \`goods_id\`=\`5123431\` AND \`mould_id\`=12222 AND \`specs_crc\`=123123444 AND \`pid\`=1122222 AND \`state\`=1.00 AND \`pusername\`=\`zaaaaa\` AND \`protect_price\`=2 AND \`shop_id\`=741231 AND \`create_time\`=\`2023-12-02 19:37:27\` AND \`game_name\`=\`火影\` AND \`goods_sku\`=316123 AND \`duration\`=1 AND \`game_id\`=560 AND \`mould_name\`=\`鲛肌` AND \`id\`=5123 LIMIT 1; #start 349365354 end 349365957 time 2023-12-02 19:37:56&quot;</code></pre><p>sed正则表达式：</p><pre><code class="bash"> echo $string |sed -n &quot;s;.*\(\`goods_sku\`=[^ ]*\).*;\1;p</code></pre><p>如果只具体的值，不要字段，只需要把字段移到分组括号外就行。</p><pre><code class="bash">echo $string1 |sed -n &quot;s;.*\`goods_sku\`=\([^ ]*\).*;\1;p&quot;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;一般切割提取字符串的字段，如果有规律可寻，采用awk或者cut就可以搞定。如果无规律的分隔符，就必须要用正则表达式来提取需要</summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="命令行" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/%E5%91%BD%E4%BB%A4%E8%A1%8C/"/>
    
    
    <category term="shell" scheme="https://www.langxw.com/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>浅析OCI、CRI、Docker、Containerd、runc、Dockershim、Containerd-shim分别是什么</title>
    <link href="https://www.langxw.com/2023/08/18/%E6%B5%85%E6%9E%90OCI%E3%80%81CRI%E3%80%81Docker%E3%80%81Containerd%E3%80%81runc%E3%80%81Dockershim%E3%80%81Containerd-shim%E5%88%86%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/"/>
    <id>https://www.langxw.com/2023/08/18/%E6%B5%85%E6%9E%90OCI%E3%80%81CRI%E3%80%81Docker%E3%80%81Containerd%E3%80%81runc%E3%80%81Dockershim%E3%80%81Containerd-shim%E5%88%86%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/</id>
    <published>2023-08-18T07:25:51.000Z</published>
    <updated>2023-08-18T08:50:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在学习K8S的过程中，会经常看到runc，cri，containerd容器运行时这些名词，必须弄清楚了，才能对K8S的架构又更清晰深入的了解。</p><p><strong>如今容器已不再与Docker紧密耦合，我们可以使用Docker或者其他非Docker工具运行容器。Docker不能代表容器，日常中紧紧指代Docker工具。</strong></p><h2 id="二、名词解释"><a href="#二、名词解释" class="headerlink" title="二、名词解释"></a>二、名词解释</h2><h3 id="1、容器生态系统"><a href="#1、容器生态系统" class="headerlink" title="1、容器生态系统"></a>1、容器生态系统</h3><p>容器生态系统是由许多令人兴奋的技术、大量行话组成，是由许多大公司共同维护。</p><p><strong>容器相关主流标准：</strong></p><ol><li><p>Open Container Initiative（OCI）发布了容器运行时标准和容器镜像标准</p></li><li><p>Container Runtime Interface（CRI）定义了 Kubernetes 和下层容器运行时之间的 API</p></li></ol><p>下图准确展示了 Kubernetes、Docker、CRI、OCI、containerd 和 runc 在这个生态系统中是如何组织到一起的。</p><p><img src="https://s2.loli.net/2023/08/18/ztofpl4ga2b1WIc.png" alt="container-ecosystem.drawio.png"></p><h3 id="2、Docker"><a href="#2、Docker" class="headerlink" title="2、Docker"></a>2、Docker</h3><p>Docker是最流行的容器工具，Docker被设计成安装在个人电脑或者服务器上的一组工具，开发者能够轻松的用它投建或者运行容器。</p><h4 id="2-1-docker项目组成"><a href="#2-1-docker项目组成" class="headerlink" title="2.1 docker项目组成"></a>2.1 docker项目组成</h4><ul><li><p>docker-cli：docker daemon的命令行交互工具，我们说的docker默认指它</p></li><li><p>containerd：运行和管理容器的守护进程，推送和拉取镜像、管理存储和网络并监视容器的运行状态。<strong>高级别容器运行时。</strong></p></li><li><p>containerd-shim：属于containerd，是一个代理，负责containerd和底层runc进行交互。</p></li><li><p>runc：底层的容器运行时，<strong>低级别容器运行时</strong>，真正创建并运行容器的东西，包括 libcontainer。创建完容器后会退出，由containerd-shim接管。</p></li></ul><p>Docker简版调用图：</p><p><img src="https://s2.loli.net/2023/08/18/2takTCcVYzMUegs.png" alt="container-ecosystem-docker.drawio.png"></p><p>更进一步逻辑：</p><p><img src="https://s2.loli.net/2023/08/18/LWwxvCmlsFKGejU.png" alt="微信截图_20230818141342.png"></p><h4 id="2-2-Docker模块详解"><a href="#2-2-Docker模块详解" class="headerlink" title="2.2 Docker模块详解"></a>2.2 Docker模块详解</h4><p>从 Docker 1.11 之后，Docker Daemon 被分成了多个模块以适应 OCI 标准。拆分之后，结构分成了以下几个部分。</p><p><img src="https://s2.loli.net/2023/08/18/Ud2kCmyLapzOgMP.png" alt="6b576aff173724d6a8055bde5d06a1e7.png"></p><p>其中，containerd 独立负责容器运行时和生命周期(如创建、启动、停止、中止、信号处理、删除等)，其他一些如镜像构建、卷管理、日志等由 Docker Daemon 的其他模块处理。</p><ul><li><p>Docker 的模块块拥抱了开放标准，希望通过 OCI 的标准化，容器技术能够有很快的发展。</p></li><li><p>现在创建一个docker容器的时候，Docker Daemon 并不能直接帮我们创建了，而是请求 containerd 来创建一个容器。当containerd 收到请求后，也不会直接去操作容器，而是创建一个叫做 containerd-shim 的进程。让这个进程去操作容器，我们指定容器进程是需要一个父进程来做状态收集、维持 stdin 等 fd 打开等工作的，假如这个父进程就是 containerd，那如果 containerd 挂掉的话，整个宿主机上所有的容器都得退出了，而引入 containerd-shim 这个垫片就可以来规避这个问题了，就是提供的live-restore的功能。这里需要注意systemd的 MountFlags=slave。</p></li><li><p>然后创建容器需要做一些 namespaces 和 cgroups 的配置，以及挂载 root 文件系统等操作。runc 就可以按照这个 OCI 文档来创建一个符合规范的容器。</p></li><li><p>真正启动容器是通过 containerd-shim 去调用 runc 来启动容器的，runc 启动完容器后本身会直接退出，containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程。containerd，containerd-shim和容器进程(即容器主进程)三个进程，是有依赖关系的。</p></li></ul><h3 id="3、Dockershim"><a href="#3、Dockershim" class="headerlink" title="3、Dockershim"></a>3、Dockershim</h3><p>Kubernetes 曾经包含了一个叫做 dockershim 的组件（v1.24 版本中彻底移除），使它能够支持 Docker。</p><p>Kubernetes 偏向于通过支持 Container Runtime Interface（CRI）接口的任何容易运行时运行容器，但 Docker 比 Kubernetes 出现得更早，Docker 并不支持 CRI（容器运行时接口）这一 Kubernetes 运行时 API，而 Kubernetes 用户一直以来所使用的其实是名为“dockershim”的桥接服务。Dockershim 能够转换 Docker API 与 CRI。</p><blockquote><p>在软件系统中，shim 垫片扮演不同 API 之间桥的角色，或作为兼容层。当你想使用一个第三方的组件，但需要少许胶水代码来使其奏效，有时候就会添加一个 shim。</p></blockquote><p><img src="https://s2.loli.net/2023/08/18/3XOPYnV2QN9koJb.png" alt="cri-performance.png"></p><p>移除 dockershim 并不意味着 Kubernetes 无法运行 Docker 格式的容器。不论 containerd 还是 CRI-O 都能运行 Docker 格式（实际上是 OCI 格式）的镜像，只是无需通过 <code>docker</code> 命令或 Docker daemon 来进行。</p><h3 id="4、Docker镜像"><a href="#4、Docker镜像" class="headerlink" title="4、Docker镜像"></a>4、Docker镜像</h3><p><strong>大家所说的 Docker 镜像，实际上是以 Open Container Initiative（OCI）格式打包的。</strong></p><p>不论从 Docker Hub 还是其他 registry 拉取的镜像，都能够用 <code>docker</code> 命令使用它，亦或者是 Kubernetes 集群中，或者通过 podman 工具，还是其他支持 OCI 镜像格式规范的工具。</p><p>这就是有一个开放的标准的好处——任何人都可以编写支持标准的软件。</p><h3 id="5、CRI"><a href="#5、CRI" class="headerlink" title="5、CRI"></a>5、CRI</h3><p><strong>CRI-Container Runtime Interface） 是 Kubernetes 用来控制不同运行时创建和管理容器的协议。</strong></p><p>CRI 抽象了容器运行时，Kubernetes 无需关心到底是哪一种。Kubernetes 不应该自身支持<br>每一种容器运行时，那样代码库会更庞大而且难以管理，CRI API 描述了 Kubernetes 如何与运行时交互。这样，实际上管理容器的是下面的容器运行时。</p><p><img src="https://s2.loli.net/2023/08/18/Mybwf92K1IpRYtC.png" alt="微信截图_20230818144646.png"></p><p>不论 containerd 还是 CRI-O 都可以使用，因为这两种运行时都实现了 CRI 规范。作为用户的我们无需关心，每种 CRI 实现略有不同但旨在可插拔和无缝修改。</p><p>红帽的 OpenShift 使用 CRI-O 并为其提供支持而 Docker 支持它们自己的 containerd。</p><h3 id="6、Containerd"><a href="#6、Containerd" class="headerlink" title="6、Containerd"></a>6、Containerd</h3><p><strong>containerd 是来自于 Docker 的高级别容器运行时，并实现了 CRI 规范。真正创建和运行容器进程的是它所控制的底层运行时。</strong></p><ul><li>containerd 从 Docker 项目中拆分出来使其更模块化。</li><li>Docker 自己内部使用 containerd，安装 Docker 同时也会安装 containerd。</li><li>containerd 通过它的 cri 插件实现了 Kubernetes CRI。</li></ul><p><img src="https://s2.loli.net/2023/08/18/qgZteIOWBzj68xK.png" alt="1d2badf2b2f92fcfc4386dca8854e995.png"></p><p>containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性，containerd 可以负责干下面这些事情：</p><ul><li>管理容器的生命周期(从创建容器到销毁容器)</li><li>拉取/推送容器镜像</li><li>存储管理(管理镜像及容器数据的存储)</li><li>调用 runc 运行容器(与 runc 等容器运行时交互)</li><li>管理容器网络接口及网络</li></ul><p><img src="https://s2.loli.net/2023/08/18/4o7sQJbXmpB9tqG.png" alt="v2-dd73d91848ccbdb6db2d920da3b6db95_720w.png"></p><p>上图是 Containerd 整体的架构。由下往上，Containerd支持的操作系统和架构有 Linux、<br>Windows 以及像 ARM 的一些平台。在这些底层的操作系统之上运行的就是底层容器运行<br>时，其中有上文提到的runc、gVisor 等。在底层容器运行时之上的是Containerd 相关的组件，比如 Containerd 的 runtime、core、API、backend、store 还有metadata 等等。构筑在 Containerd 组件之上以及跟这些组件做交互的都是 Containerd 的 client，Kubernetes 跟 Containerd 通过 CRI 做交互时，本身也作为 Containerd 的一个 client。Containerd 本身有提供了一个 CRI，叫 ctr，不过这个命令行工具并不是很好用。</p><p>在这些组件之上就是真正的平台，Google Cloud、Docker、IBM、阿里云、微软云还有RANCHER等等都是，这些平台目前都已经支持 containerd， 并且有些已经作为自己的默认容器运行时了。</p><p>从 k8s 的角度看，选择 containerd作为运行时的组件，它调用链更短，组件更少，更稳定，占用节点资源更少。</p><p><img src="https://s2.loli.net/2023/08/18/oJbW5Lf7BVK9ZlT.png" alt="33c809cc2219b915c1168da271097d02.png"></p><h3 id="7、CRI-O"><a href="#7、CRI-O" class="headerlink" title="7、CRI-O"></a>7、CRI-O</h3><p><strong>CRI-O 是另一种高级别的容器运行时，也实现了 CRI。它作为 containerd 的替代选项，也通过底层容器运行时运行容器进程。</strong></p><ul><li>CRI-O 诞生于 Red Hat、IBM、Intel、SUSE 等大公司。</li><li>它就是专门作为 Kubernetes 容器运行时被打造的。</li></ul><h3 id="8、OCI"><a href="#8、OCI" class="headerlink" title="8、OCI"></a>8、OCI</h3><p><strong>OCI 是一组科技公司维护的规范，定义了容器镜像格式还有容器应该如何运行。</strong></p><p>OCI（Open Container Initiative）即开放的容器运行时<code>规范</code>，目的在于定义一个容器运行时及镜像的相关标准和规范，其中包括</p><ul><li>runtime-spec：容器的生命周期管理，具体参考<a href="https://github.com/opencontainers/runtime-spec/blob/master/runtime.md">runtime-spec</a>。</li><li>image-spec：镜像的生命周期管理，具体参考<a href="https://github.com/opencontainers/image-spec/blob/main/spec.md">image-spec</a>。</li></ul><p>实现OCI标准的容器运行时有<code>runc</code>，<code>kata</code>等。</p><p><img src="https://s2.loli.net/2023/08/18/WNiTGpHqQ7sxdbt.png" alt="oci-1.png"></p><p>OCI 对容器 runtime 的标准主要是指定容器的运行状态，和 runtime 需要提供的命令。下图可以是容器状态转换图：</p><p><img src="https://s2.loli.net/2023/08/18/QKAWrTw1Etf45vO.png" alt="oci-2.png"></p><ul><li>init 状态：这个是我自己添加的状态，并不在标准中，表示没有容器存在的初始状态</li><li>creating：使用 create 命令创建容器，这个过程称为创建中</li><li>created：容器创建出来，但是还没有运行，表示镜像和配置没有错误，容器能够运行在当前平台</li><li>running：容器的运行状态，里面的进程处于 up 状态，正在执行用户设定的任务</li><li>stopped：容器运行完成，或者运行出错，或者 stop 命令之后，容器处于暂停状态。这个状态，容器还有很多信息保存在平台中，并没有完全被删除</li></ul><h3 id="9、Containerd-shim"><a href="#9、Containerd-shim" class="headerlink" title="9、Containerd-shim"></a>9、Containerd-shim</h3><p><strong>containerd 并不直接调用 runc 去创建和运行容器，而是通过 containerd-shim 来进行。</strong></p><p>runc 创建完容器会直接退出，于是 containerd-shim 就成为容器的父进程，伴随容器的整个生命周期监控其运行状态。这样避免了 containerd 进程挂掉导致主机上的所有容器都退出。</p><h3 id="10、Runc"><a href="#10、Runc" class="headerlink" title="10、Runc"></a>10、Runc</h3><p><strong>runc 是一种 OCI 兼容的底层容器运行时。它实现了 OCI 规范并运行容器进程。</strong></p><p>runc 是 OCI 的<em>参考实现</em>。</p><blockquote><p>参考实现通常是第一个根据规范开发的软件。</p></blockquote><p>runc 为容器提供了所有底层功能：利用底层的 Linux 功能，例如命名空间和控制组。</p><p>runc 的几个替代选项：</p><ul><li><a href="https://github.com/containers/crun">crun</a>：C 编写的容器运行时（runc 是 Go 编写的）</li><li>KataContainer 项目的 <a href="https://github.com/kata-containers/kata-containers/tree/main/src/runtime">kata-runtime</a>：将 OCI 规范实现为轻量级的虚机</li><li>Google 的 <a href="https://gvisor.dev/">gVisor</a>：创建<em>有自己内核的容器</em>。它在自己的运行时 runsc 中实现 OCI。</li></ul><blockquote><p>runc 是在 Linux 上运行容器的工具；在 Windows 操作系统中则是微软的 Host Compute Service(HCS)，包括了一个叫 runhcs 的工具。</p></blockquote><p>在命令行中使用 runc，我们可以根据需要启动任意数量的容器。但是，如果我们想自动化这个过程，我们需要一个容器管理器。为什么这样?想象一下，我们需要启动数十个容器来跟踪它们的状态。其中一些需要在失败时重新启动，需要在终止时释放资源，必须从注册表中提取图像，需要配置容器间网络等等。就需要有Low-Level和High-Level容器运行时，runc就是Low-Level实现的实现。</p><h3 id="11、Low-Level和High-Level容器运行时"><a href="#11、Low-Level和High-Level容器运行时" class="headerlink" title="11、Low-Level和High-Level容器运行时"></a>11、Low-Level和High-Level容器运行时</h3><p>当人们想到容器运行时，可能会想到一系列示例;runc、lxc、lmctfy、Docker(容器)、rkt、cri-o。<br>这些中的每一个都是为不同的情况而构建的，并实现了不同的功能。有些，如 containerd 和 cri-o，实际上使用 runc 来运行容器，在High-Level实现镜像管理和 API。<br>与 runc 的Low-Level实现相比，可以将这些功能(包括镜像传输、镜像管理、镜像解包和 API)视为High-Level功能。考虑到这一点，您可以看到容器运行时空间相当复杂。每个运行时都涵盖了这个Low-Level到High-Level频谱的不同部分。<br>这是一个非常主观的图表：</p><p><img src="https://s2.loli.net/2023/08/18/xvOFHyViBe1I28l.png" alt="Low-Level和High-Level容器运行时.png"></p><p>因此，从实际出发，通常只专注于正在运行的容器的runtime通常称为“Low-Level容器运行时”，支持更多高级功能(如镜像管理和gRPC / Web API)的运行时通常称为“High-Level容器运行时”，“High-Level容器运行时”或通常仅称为“容器运行时”，我将它们称为“High-Level容器运行时”。<br>值得注意的是，Low-Level容器运行时和High-Level容器运行时是解决不同问题的、从根本上不同的事物。</p><h4 id="11-1、Low-Level容器运行时"><a href="#11-1、Low-Level容器运行时" class="headerlink" title="11.1、Low-Level容器运行时"></a>11.1、Low-Level容器运行时</h4><p>容器是通过Linux nanespace和Cgroups实现的，Namespace能让你为每个容器提供虚拟化系统资源，像是文件系统和网络，Cgroups提供了限制每个容器所能使用的资源的如内存和CPU使用量的方法。在最低级别的运行时中，容器运行时负责为容器建立namespaces和cgroups,然后在其中运行命令，Low-Level容器运行时支持在容器中使用这些操作系统特性。</p><p>目前来看低级容器运行时有：</p><ul><li><p>runc ：我们最熟悉也是被广泛使用的容器运行时，代表实现Docker。</p></li><li><p>runv：runV 是一个基于虚拟机管理程序(OCI)的运行时。它通过虚拟化 guest kernel，将容器和主机隔离开来，使得其边界更加清晰，这种方式很容易就能帮助加强主机和容器的安全性。代表实现是kata和Firecracker。</p></li><li><p>runsc：runsc = runc + safety ，典型实现就是谷歌的gvisor，通过拦截应用程序的所有系统调用，提供安全隔离的轻量级容器运行时沙箱。截止目前，貌似并不没有生产环境使用案例。</p></li><li><p>wasm : Wasm的沙箱机制带来的隔离性和安全性，都比Docker做的更好。但是wasm 容器处于草案阶段，距离生产环境尚有很长的一段路。</p></li></ul><h4 id="11-2、High-Level容器运行时"><a href="#11-2、High-Level容器运行时" class="headerlink" title="11.2、High-Level容器运行时"></a>11.2、High-Level容器运行时</h4><p>通常情况下，开发人员想要运行一个容器不仅仅需要Low-Level容器运行时提供的这些特性同时也需要与镜像格式、镜像管理和共享镜像相关的API接口和特性，而这些特性一般由High-Level容器运行时提供。<br>就日常使用来说，Low-Level容器运行时提供的这些特性可能满足不了日常所需，因为这个缘故，唯一会使用Low-Level容器运行时的人是那些实现High-Level容器运行时以及容器工具的开发人员。<br>那些实现Low-Level容器运行时的开发者会说High-Level容器运行时比如containerd和cri-o不像真正的容器运行时，因为从他们的角度来看，他们将容器运行的实现外包给了runc。但是从用户的角度来看，它们只是提供容器功能的单个组件，可以被另一个的实现替换，因此从这个角度将其称为runtime仍然是有意义的。即使containerd和cri-o都使用runc，但是它们是截然不同的项目，支持的特性也是非常不同的。</p><p>dockershim, containerd 和cri-o都是遵循CRI的容器运行时，我们称他们为高层级运行时(High-level Runtime)。</p><p>Kubernetes 只需支持 containerd 等high-level container runtime即可。由containerd 按照OCI 规范去对接不同的low-level container runtime，比如通用的runc，安全增强的gvisor，隔离性更好的runv。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、背景&quot;&gt;&lt;a href=&quot;#一、背景&quot; class=&quot;headerlink&quot; title=&quot;一、背景&quot;&gt;&lt;/a&gt;一、背景&lt;/h2&gt;&lt;p&gt;在学习K8S的过程中，会经常看到runc，cri，containerd容器运行时这些名词，必须弄清楚了，才能对K8S的架构又</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.langxw.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Kubernetes" scheme="https://www.langxw.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>ES集群搭建-K8s环境</title>
    <link href="https://www.langxw.com/2023/08/18/ES%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-K8s%E7%8E%AF%E5%A2%83/"/>
    <id>https://www.langxw.com/2023/08/18/ES%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-K8s%E7%8E%AF%E5%A2%83/</id>
    <published>2023-08-18T01:42:56.000Z</published>
    <updated>2023-08-18T01:49:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p><strong>ES集群上K8s，主要解决横向扩展复杂的问题，次要可以节约一定的成本。</strong></p><p>通过更改StatefulSet的副本数，可以有序横向扩展ES集群，并通过集群自动平衡数据的方法，将数据迁移到新加入的节点。</p><h2 id="二、预备环境"><a href="#二、预备环境" class="headerlink" title="二、预备环境"></a>二、预备环境</h2><ol><li><p>阿里云ASK集群：版本1.22.15-aliyun.1</p></li><li><p>ES集群：版本 elasticsearch:6.3.2</p></li><li><p>存储：阿里云SSD云盘</p></li></ol><h2 id="三、架构"><a href="#三、架构" class="headerlink" title="三、架构"></a>三、架构</h2><p><strong>部署在阿里云ASK集群的statefulset，不存在资源竞争</strong></p><ul><li><p>方案1（正常方案）：master节点3个，data节点N个。</p></li><li><p>方案2（简单粗暴）：初始3个节点，所有节点都是master节点和node节点；后续扩容不用在意discovery，保持master是初期节点即可。（<strong>本例采用</strong>）</p></li></ul><h2 id="四、部署步骤"><a href="#四、部署步骤" class="headerlink" title="四、部署步骤"></a>四、部署步骤</h2><h3 id="1、全部所需yaml文件预览"><a href="#1、全部所需yaml文件预览" class="headerlink" title="1、全部所需yaml文件预览"></a>1、全部所需yaml文件预览</h3><pre><code class="bash">create-ns.yaml             # 命名空间mhw-es-headless-svc.yaml   # 通过headless-svc配置es集群域名mhw-storage-class.yaml     # 动态存储卷mhw-es-statefulset.yaml    # 有状态应用ESmhw-es-svc.yaml            # LoadBalance类型的svc，作为集群入口mhw-kibana.yaml            # kibana</code></pre><h3 id="2、创建elasticsearch命名空间"><a href="#2、创建elasticsearch命名空间" class="headerlink" title="2、创建elasticsearch命名空间"></a>2、创建elasticsearch命名空间</h3><p>新建<strong>create-ns.yaml</strong></p><pre><code class="yaml">apiVersion: v1kind: Namespacemetadata:  name: elasticsearch  labels:    name: elasticsearch</code></pre><p>说明：创建专属的namespace和业务应用网络隔离，资源隔离。</p><h3 id="3、创建headless类型的svc"><a href="#3、创建headless类型的svc" class="headerlink" title="3、创建headless类型的svc"></a>3、创建headless类型的svc</h3><p>新建<strong>mhw-es-headless-svc.yaml</strong></p><pre><code class="yaml">kind: ServiceapiVersion: v1metadata:  name: mhw-es-svc  namespace: elasticsearchspec:  selector:    app: mhw-es   clusterIP: None  ports:  - name: rest # 节点与外部通信端口    port: 9200  - name: inter-node # 节点内部通信端口    port: 9300</code></pre><p>说明：有状态应用通过headless-svc所确定的域名来相互通信。</p><h3 id="4、创建动态存储卷storageclass"><a href="#4、创建动态存储卷storageclass" class="headerlink" title="4、创建动态存储卷storageclass"></a>4、创建动态存储卷storageclass</h3><p>新建<strong>mhw-storage-class.yaml</strong></p><pre><code class="yaml">apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: alicloud-disk-ssd-esprovisioner: diskplugin.csi.alibabacloud.comparameters:  type: cloud_ssd  fstype: ext4  resourceGroupId: &quot;rg-aek2q6kagvjz2zq&quot;volumeBindingMode: Immediate # 立即绑定，非延迟绑定reclaimPolicy: Delete # 保留策略，不保留，删除pvc的时候删除云盘allowVolumeExpansion: false # 是否动态扩展</code></pre><p>说明：数据存储采用云盘，云盘可以选择类型和IOPS，就是按量付费可能略贵。具体参数见文档：[使用阿里云云盘动态存储卷](<a href="https://help.aliyun.com/document_detail/134859.html?spm=a2c4g.134767.0.0.fe0e2b5aEvth5d">使用云盘动态存储卷 (aliyun.com)</a>)</p><h3 id="5、创建有状态应用ES"><a href="#5、创建有状态应用ES" class="headerlink" title="5、创建有状态应用ES"></a>5、创建有状态应用ES</h3><p>新建<strong>mhw-es-statefulset.yaml</strong></p><pre><code class="yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: mhw-es-cluster # statefuleSet的名称  namespace: elasticsearch # 所属命名空间spec:  serviceName: mhw-es-svc # headless-svc的名称  selector:    matchLabels:      app: mhw-es  replicas: 3 # 3个节点  template:    metadata:      labels:        app: mhw-es # es容器标签      annotations:        k8s.aliyun.com/eci-use-specs : &quot;2-8Gi&quot; # 实例配置    spec:      containers:      - name: mhw-es # es容器名称        image: registry-vpc.cn-beijing.aliyuncs.com/zuhaowan-tools/elasticsearch:6.3.2        resources: # 分配的资源          requests:            cpu: 2            memory: 8Gi          limits:             cpu: 2            memory: 8Gi        livenessProbe:           tcpSocket:            port: 9200          initialDelaySeconds: 40          timeoutSeconds: 5          periodSeconds: 5        ports:        - containerPort: 9200          name: rest          protocol: TCP        - name: inter-node          containerPort: 9300          protocol: TCP        volumeMounts: # 数据卷挂载        - name: data          mountPath: /usr/share/elasticsearch/data        env:        - name: TZ # 时区设置          value: Asia/Shanghai        - name: cluster.name # ES集群名称          value: mhw-cluster        - name: node.name # ES节点名称，来自有状态应用自身名字          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: discovery.zen.ping.unicast.hosts # 自动发现种子节点列表，用于后期选主节点          value: &quot;mhw-es-cluster-0.mhw-es-svc,mhw-es-cluster-1.mhw-es-svc,mhw-es-cluster-2.mhw-es-svc&quot;        - name: discovery.zen.minimum_master_nodes # 防止脑裂的配置          value: &quot;2&quot;        - name: ES_JAVA_OPTS # Java堆内存          value: &quot;-Xms4g -Xmx4g&quot;        - name: network.host           value: &quot;0.0.0.0&quot;        - name: http.cors.allow-origin          value: &quot;*&quot;        - name: http.cors.enabled          value: &quot;true&quot;        - name: http.max_initial_line_length          value: &quot;8k&quot;        - name: http.max_header_size          value: &quot;16k&quot;        - name: bootstrap.memory_lock # 内存锁定          value: &quot;false&quot;      initContainers: # init容器解决es数据目录权限问题      - name: fix-permissions        image: busybox        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;chown -R 1000:1000 /usr/share/elasticsearch/data&quot;]        volumeMounts: # es数据目录挂载路径          - name:  data            mountPath:  /usr/share/elasticsearch/data  volumeClaimTemplates:  # PVC模版，使用storageclass动态创建pv  - metadata:      name: data    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ] # 单节点读写      storageClassName: alicloud-disk-ssd-es      resources:        requests:          storage: 50Gi # 创建云盘的大小</code></pre><p>说明：</p><ul><li><p>ES6集群不需要配置集群初始化时引导选择主节点的配置，所以这个配置cluster.initial_master_nodes 可以不写。</p></li><li><p>这个集群三个节点默认都是master和data节点。</p></li><li><p>discovery.zen.ping.unicast.hosts 只在最开始配置三个节点即可，后续节点不用再添加进来，永远使用这三个选一个当主节点的意思。</p></li><li><p>注意指定时区</p></li></ul><h3 id="6、创建ES集群入口SLB"><a href="#6、创建ES集群入口SLB" class="headerlink" title="6、创建ES集群入口SLB"></a>6、创建ES集群入口SLB</h3><p>创建<strong>mhw-es-svc.yaml</strong></p><pre><code class="yaml">apiVersion: v1kind: Servicemetadata:  annotations: # 创建按量付费的SLB    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-connect-port: &#39;9200&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-flag: &#39;on&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-interval: &#39;3&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-type: tcp    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-healthy-threshold: &#39;4&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-scheduler: wlc    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-instance-charge-type: &quot;PayByCLCU&quot;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-unhealthy-threshold: &#39;4&#39;    service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-name: &quot;mhw-es&quot;  name: mhw-es-svc-slb  namespace: elasticsearch spec:  ports:    - name: http      port: 9200       protocol: TCP      targetPort: 9200   selector:    app: mhw-es  type: LoadBalancer</code></pre><p>说明：因为本实例中ES集群和业务应用分属于不同namespace，网络隔离了，通过headless-svc名称无法访问通。所以需要一个集群外的访问入口，这里使用私网SLB。</p><h3 id="7、创建Kibana"><a href="#7、创建Kibana" class="headerlink" title="7、创建Kibana"></a>7、创建Kibana</h3><p>创建<strong>mhw-kibana.yaml</strong></p><pre><code class="yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: mhw-kibana-deployment  namespace: elasticsearch  labels:    app: mhw-kibanaspec:  minReadySeconds: 15  replicas: 1  selector:    matchLabels:      app: mhw-kibana  template:    metadata:      labels:        app: mhw-kibana      annotations:        k8s.aliyun.com/eci-use-specs : &quot;1-2Gi&quot;    spec:      terminationGracePeriodSeconds: 40      containers:      - name: mhw-kibana        image: registry-vpc.cn-beijing.aliyuncs.com/zuhaowan-tools/kibana:6.3.2        ports:        - containerPort: 5601        resources:          requests:            cpu: 1            memory: 2Gi          limits:             cpu: 1            memory: 2Gi        livenessProbe:          tcpSocket:            port: 5601          initialDelaySeconds: 40          timeoutSeconds: 5          periodSeconds: 5        env:        - name: TZ # 时区          value: &quot;Asia/Shanghai&quot;        - name: ELASTICSEARCH_URL # ES集群的地址          value: &quot;http://mhw-es-svc:9200&quot;        - name: XPACK.MONITORING.UI.CONTAINER.ELASTICSEARCH.ENABLED # 关闭x-pack          value: &quot;false&quot;        - name: I18N.DEFAULTLOCALE # 本地化          value: &quot;zh-CN&quot;      imagePullSecrets:                - name: zhw-user-secret</code></pre><p>说明：</p><ul><li><p>注意连接ES集群地址的变量配置即可，ES6和ES7可能不一样。</p></li><li><p>可以直接使用容器IP或者创建nodeport和loadblance类型的svc来访问kibana。</p></li><li><p>本例是部署在ASK集群，集群内网是打通的，故直接使用容器IP访问Kibana。</p></li></ul><h2 id="五、遇到的问题"><a href="#五、遇到的问题" class="headerlink" title="五、遇到的问题"></a>五、遇到的问题</h2><h3 id="1、交换分区memory-lock问题"><a href="#1、交换分区memory-lock问题" class="headerlink" title="1、交换分区memory_lock问题"></a>1、交换分区memory_lock问题</h3><p>在ES的配置文件中，官方建议不使用交换分区（使用交换分区会使ES性能急剧下降），将bootstrap.memory_lock设置为true，或者最本质方法彻底禁用交换内存。</p><p>在本例中，ASK集群的底层是ECI主机，默认交换分区是禁用的（阿里云ECS交换分区默认也是禁用的）。故可以在配置文件中将bootstrap.memory_lock设置为false。</p><h3 id="2、缩容PVC释放问题"><a href="#2、缩容PVC释放问题" class="headerlink" title="2、缩容PVC释放问题"></a>2、缩容PVC释放问题</h3><ul><li><p>缩容的时候PVC默认是保留的，同样对应的PV和云盘都是保留的。</p></li><li><p>上面storageclass设置的PVC删除的时候PV和云盘是同时删除的。</p></li><li><p>基于以上，如果确认不再使用对应的PVC（存储），需要手动删除，云盘会立刻释放。</p></li></ul><h3 id="3、版本问题"><a href="#3、版本问题" class="headerlink" title="3、版本问题"></a>3、版本问题</h3><ul><li><p>以上是基于ES6部署的配置文件，如果是ES7或者ES8集群，可能会有所差别。</p></li><li><p>以上是基于阿里云ASK集群，如果是阿里云ACK集群（普通K8s集群），yaml文件可能有差别。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;ES集群上K8s，主要解决横向扩展复杂的问题，次要可以节约一定的成本。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过</summary>
      
    
    
    
    <category term="数据库" scheme="https://www.langxw.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    <category term="ES" scheme="https://www.langxw.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ES/"/>
    
    
    <category term="es" scheme="https://www.langxw.com/tags/es/"/>
    
  </entry>
  
  <entry>
    <title>Nacos集群搭建-K8s环境</title>
    <link href="https://www.langxw.com/2022/07/04/Nacos%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-K8s%E7%8E%AF%E5%A2%83/"/>
    <id>https://www.langxw.com/2022/07/04/Nacos%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-K8s%E7%8E%AF%E5%A2%83/</id>
    <published>2022-07-04T07:44:51.000Z</published>
    <updated>2022-07-04T07:45:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、预备环境"><a href="#一、预备环境" class="headerlink" title="一、预备环境"></a>一、预备环境</h2><ol><li>阿里云ASK集群 k8s版本：1.20.4-aliyun.1</li><li> nacos镜像：nacos/nacos-server:v2.0.3</li></ol><h2 id="二、流程图"><a href="#二、流程图" class="headerlink" title="二、流程图"></a>二、流程图</h2><pre class="mermaid">graph LRA[集群外部SLB] --> B[阿里云 Ingress] --> C[headless-Svc] --> D[statefulSet-Pod] --> E[MySQL]F[集群内部FQDN] --> CD --> G[PVC数据持久化]</pre><p>说明：</p><ul><li>集群入口：外部入口是SLB，主要是后台管理；内部入口是headless-svc的FQDN域名，内部服务间通信。</li><li>集群数据持久化：可以采用NAS，CEPH。如果不太关注落地数据，可以不需要，此篇文档就没有配置PVC。</li><li>如果不太想用Ingress，可以用LoadBlance类型的Service替代Ingress，只管理界面用途可以这么做。</li></ul><h2 id="三、搭建步骤"><a href="#三、搭建步骤" class="headerlink" title="三、搭建步骤"></a>三、搭建步骤</h2><h3 id="1、全部所需yaml文件概览"><a href="#1、全部所需yaml文件概览" class="headerlink" title="1、全部所需yaml文件概览"></a>1、全部所需yaml文件概览</h3><pre><code class="bash">nacos-k8s-cm.yamlnacos-k8s-headless-svc.yamlnacos-k8s-stateful.yamlnacos-k8s-svc.yaml                #可不用nginx_ingress_svc.yamlnginx_ingress.yaml</code></pre><h3 id="2、创建Config-Map数据库信息"><a href="#2、创建Config-Map数据库信息" class="headerlink" title="2、创建Config-Map数据库信息"></a>2、创建Config-Map数据库信息</h3><p>新建nacos-k8s-cm.yaml文件，并写入以下内容：</p><pre><code class="yaml">apiVersion: v1kind: ConfigMapmetadata:  name: nacos-cm  namespace: zhwdata:  mysql.host: &quot;xxxxxxx.mysql.rds.aliyuncs.com&quot;  mysql.db.name: &quot;nacos_cluster&quot;  mysql.port: &quot;3306&quot;  mysql.user: &quot;nacos&quot;  mysql.password: &quot;XN5lqwtnl8jUbWsu&quot;</code></pre><h3 id="3、创建Headless-Service"><a href="#3、创建Headless-Service" class="headerlink" title="3、创建Headless Service"></a>3、创建Headless Service</h3><p>新建nacos-k8s-headless-svc.yaml文件，并写入以下内容</p><pre><code class="yaml">apiVersion: v1kind: Servicemetadata:  name: nacos-headless  namespace: zhw  labels:    app: nacos-headlessspec:  type: ClusterIP  clusterIP: None  ports:    - port: 8848       name: server      targetPort: 8848    - port: 9848      name: client-rpc      targetPort: 9848    - port: 9849      name: raft-rpc      targetPort: 9849      ## 兼容1.4.x版本的选举端口    - port: 7848      name: old-raft-rpc      targetPort: 7848  selector:    app: nacos</code></pre><h3 id="4、创建StatefulSet有状态Pod"><a href="#4、创建StatefulSet有状态Pod" class="headerlink" title="4、创建StatefulSet有状态Pod"></a>4、创建StatefulSet有状态Pod</h3><p>新建nacos-k8s-stateful.yaml文件，并写入以下内容：</p><pre><code class="yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: nacos  namespace: zhw  labels:    app: nacosspec:  # 上一步中headless-svc的名字  serviceName: nacos-headless  # nacos集群必须是三个节点  replicas: 3  selector:    matchLabels:      app: nacos  template:    metadata:      labels:        app: nacos      annotations:        # 阿里云ASK申请ECI的配置，可以忽略        k8s.aliyun.com/eci-use-specs : &quot;2-4Gi&quot;    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            - labelSelector:                matchExpressions:                  - key: &quot;app&quot;                    operator: In                    values:                      - nacos              topologyKey: &quot;kubernetes.io/hostname&quot;      containers:        - name: nacos          imagePullPolicy: Always          image: nacos/nacos-server:v2.0.3           resources:            requests:              memory: &quot;4Gi&quot;              cpu: &quot;2&quot;          ports:            - containerPort: 8848              name: client            - containerPort: 9848              name: client-rpc            - containerPort: 9849              name: raft-rpc            - containerPort: 7848              name: old-raft-rpc          livenessProbe:            tcpSocket:              port: 8848            periodSeconds: 10            initialDelaySeconds: 30          readinessProbe:            tcpSocket:              port: 8848            initialDelaySeconds: 30            timeoutSeconds: 5            periodSeconds: 5          env:            - name: MYSQL_SERVICE_DB_PARAM              value: &quot;characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useSSL=false&quot;            - name: NACOS_REPLICAS              value: &quot;3&quot;            - name: MYSQL_SERVICE_HOST              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.host            - name: MYSQL_SERVICE_DB_NAME              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.db.name            - name: MYSQL_SERVICE_PORT              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.port            - name: MYSQL_SERVICE_USER              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.user            - name: MYSQL_SERVICE_PASSWORD              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.password            - name: MODE              value: &quot;cluster&quot;            - name: NACOS_SERVER_PORT              value: &quot;8848&quot;            - name: PREFER_HOST_MODE              value: &quot;hostname&quot;            - name: NACOS_SERVERS              # 集群的主机配置，这个value必须写对，格式为FQDN形式，pod-编号.headless-svc的名字.命名空间.svc.cluster.local:8848              value: &quot;nacos-0.nacos-headless.zhw.svc.cluster.local:8848 nacos-1.nacos-headless.zhw.svc.cluster.local:8848 nacos-2.nacos-headless.zhw.svc.cluster.local:8848&quot;</code></pre><p><strong>以上三步完成后，整个nacos集群基本已经就绪，应用可以使用headless-svc的域名<code>nacos-headless.zhw.svc.cluster.local</code>和nacos集群通信。如果想访问管理界面，就需要创建Ingress SLB或者LB类型的Service。</strong></p><h3 id="5、创建Ingress"><a href="#5、创建Ingress" class="headerlink" title="5、创建Ingress"></a>5、创建Ingress</h3><p>安装完ingress组件后，即可创建ingress规则文件nginx-ingress.yaml</p><pre><code class="yaml">apiVersion: extensions/v1beta1 kind: Ingressmetadata:  name: nacos-ingress  namespace: zhwspec:  rules:  # 配置七层域名。  - host: test.nacos.com    http:      paths:      # 配置Context Path。      - path: /        backend:          serviceName: nacos-headless          servicePort: 8848 </code></pre><p><strong>根据具体情况，可继续添加9848、9849等端口。</strong></p><h3 id="6、创建-ingress-lb-svc"><a href="#6、创建-ingress-lb-svc" class="headerlink" title="6、创建 ingress-lb-svc"></a>6、创建 ingress-lb-svc</h3><p>先手动创建好阿里云内网SLB，然后创建nginx-ingress-svc.yaml文件</p><pre><code class="yaml">#nginx ingress slb serviceapiVersion: v1kind: Servicemetadata:  name: nginx-ingress-lb  namespace: kube-system   labels:    app: nginx-ingress-lb  annotations:    # 指明SLB实例地址类型为私网类型。    service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet    # 修改为您的私网SLB实例ID,事先创建好的slb。    service.beta.kubernetes.io/alicloud-loadbalancer-id: lb-2zev3l6hna765zkfi94ru     # 是否自动创建SLB端口监听（会覆写已有端口监听），也可手动创建端口监听。    service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: &#39;true&#39;spec:  type: LoadBalancer  # route traffic to other nodes  externalTrafficPolicy: &quot;Cluster&quot;  ports:  - port: 80    name: http    targetPort: 80  - port: 443    name: https    targetPort: 443  selector:    # select app=ingress-nginx pods    app: ingress-nginx</code></pre><p><strong>注意：这里的SLB既作为管理界面的入口，也可以作为Nacos集群的一个入口配置到各个服务的配置文件中，需要加上9848、9849等端口。</strong></p><h3 id="7、创建LB类型的Service"><a href="#7、创建LB类型的Service" class="headerlink" title="7、创建LB类型的Service"></a>7、创建LB类型的Service</h3><p>创建nacos-k8s-svc.yaml 文件，并写入以下内容：</p><pre><code class="yaml">apiVersion: v1kind: Servicemetadata:  annotations:    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-connect-port: &#39;8848&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-flag: &#39;on&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-interval: &#39;3&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-health-check-type: tcp    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-healthy-threshold: &#39;4&#39;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-scheduler: wlc    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-spec: slb.s1.small    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-unhealthy-threshold: &#39;4&#39;    service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet  name: nacos-svc  namespace: zhwspec:  ports:    - name: http      port: 80      protocol: TCP      targetPort: 8848  selector:    app: nacos  type: LoadBalancer</code></pre><p>在不想使用Ingress的情况下，可以用LB的类型SVC来创建一个管理入口。但是这个只建议做管理入口，不建议做集群入口。</p><h2 id="四、参考信息"><a href="#四、参考信息" class="headerlink" title="四、参考信息"></a>四、参考信息</h2><ul><li><a href="https://github.com/nacos-group/nacos-k8s">官方文档</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、预备环境&quot;&gt;&lt;a href=&quot;#一、预备环境&quot; class=&quot;headerlink&quot; title=&quot;一、预备环境&quot;&gt;&lt;/a&gt;一、预备环境&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;阿里云ASK集群 k8s版本：1.20.4-aliyun.1&lt;/li&gt;
&lt;li&gt; nacos镜像</summary>
      
    
    
    
    <category term="中间件" scheme="https://www.langxw.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="Nacos" scheme="https://www.langxw.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Nacos/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
    <category term="nacos" scheme="https://www.langxw.com/tags/nacos/"/>
    
  </entry>
  
  <entry>
    <title>Kube-prometheus添加自定义监控项</title>
    <link href="https://www.langxw.com/2022/04/19/Kube-prometheus%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%91%E6%8E%A7%E9%A1%B9/"/>
    <id>https://www.langxw.com/2022/04/19/Kube-prometheus%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%91%E6%8E%A7%E9%A1%B9/</id>
    <published>2022-04-19T05:57:02.000Z</published>
    <updated>2022-04-19T06:22:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>本文档题目也可叫做，<strong>kube-prometheus添加target</strong>。</p><p>公司有个canal项目想接入prometheus进行监控告警，从<a href="https://github.com/alibaba/canal/wiki/Prometheus-QuickStart">canal官档</a>介绍看到只需配置prometheus.yaml添加canal的job后重启服务，即可将canal接入prometheus监控。</p><p>官档介绍的是非容器部署的prometheus，鉴于我司有k8s环境，我就想将canal接入k8s集群的prometheus监控。</p><h2 id="二、部署步骤（自建K8S的Prometheus）"><a href="#二、部署步骤（自建K8S的Prometheus）" class="headerlink" title="二、部署步骤（自建K8S的Prometheus）"></a>二、部署步骤（自建K8S的Prometheus）</h2><h3 id="1、Exporter介绍"><a href="#1、Exporter介绍" class="headerlink" title="1、Exporter介绍"></a>1、Exporter介绍</h3><p>一般来讲向Prometheus提供监控样本数据的程序都可以被称为一个Exporter，Exporter的一个实例称为Target。Prometheus通过轮询的方式定时从这些Target中获取监控数据样本，并且存储在数据库当中。</p><p><a href="https://imgtu.com/i/LdOyEF"><img src="https://s1.ax1x.com/2022/04/18/LdOyEF.png" alt="LdOyEF.png"></a></p><h4 id="1-1-Exporter按来源分类"><a href="#1-1-Exporter按来源分类" class="headerlink" title="1.1 Exporter按来源分类"></a>1.1 Exporter按来源分类</h4><ul><li><p><strong>社区提供的</strong>：比如Node Exporter、MySQL Exporter、Nginx Exporter等</p><table><thead><tr><th align="center">范围</th><th align="center">常用Exporter</th></tr></thead><tbody><tr><td align="center">数据库</td><td align="center">MySQL Exporter, Redis Exporter,  MongoDB Exporter, MSSQL Exporter等</td></tr><tr><td align="center">硬件</td><td align="center">Apcupsd Exporter，IoT Edison  Exporter， IPMI Exporter, Node Exporter等</td></tr><tr><td align="center">消息队列</td><td align="center">Beanstalkd Exporter, Kafka  Exporter, NSQ Exporter, RabbitMQ Exporter等</td></tr><tr><td align="center">存储</td><td align="center">Ceph Exporter, Gluster Exporter,  HDFS Exporter, ScaleIO Exporter等</td></tr><tr><td align="center">HTTP服务</td><td align="center">Apache Exporter, HAProxy  Exporter, Nginx Exporter等</td></tr><tr><td align="center">API服务</td><td align="center">AWS ECS Exporter， Docker Cloud  Exporter, Docker Hub Exporter, GitHub Exporter等</td></tr><tr><td align="center">日志</td><td align="center">Fluentd Exporter, Grok Exporter等</td></tr><tr><td align="center">监控系统</td><td align="center">Collectd Exporter, Graphite  Exporter, InfluxDB Exporter, Nagios Exporter, SNMP Exporter等</td></tr><tr><td align="center">其它</td><td align="center">Blockbox Exporter, JIRA  Exporter, Jenkins Exporter， Confluence Exporter等</td></tr></tbody></table></li><li><p><strong>用户自定义的</strong>：用户还可以基于Prometheus提供的Client Library创建自己的Exporter程序。目前Promthues社区官方提供了对以下编程语言的支持：Go、Java/Scala、Python、Ruby。同时还有第三方实现的如：Bash、C++、Common Lisp、Erlang,、Haskeel、Lua、Node.js、PHP、Rust等。</p></li></ul><h4 id="1-2-Exporter按运行方式分类"><a href="#1-2-Exporter按运行方式分类" class="headerlink" title="1.2 Exporter按运行方式分类"></a>1.2 Exporter按运行方式分类</h4><ul><li><p><strong>独立使用的</strong></p><p>以我们已经使用过的Node Exporter为例，由于操作系统本身并不直接支持Prometheus，同时用户也无法通过直接从操作系统层面上提供对Prometheus的支持。因此，用户只能通过独立运行一个程序的方式，通过操作系统提供的相关接口，将系统的运行状态数据转换为可供Prometheus读取的监控数据。 除了Node Exporter以外，比如MySQL Exporter、Redis Exporter等都是通过这种方式实现的。 这些Exporter程序扮演了一个中间代理人的角色。</p></li><li><p><strong>集成到应用中的</strong></p><p>为了能够更好的监控系统的内部运行状态，有些开源项目如Kubernetes，ETCD等直接在代码中使用了Prometheus的Client Library，提供了对Prometheus的直接支持。这种方式打破的监控的界限，让应用程序可以直接将内部的运行状态暴露给Prometheus，适合于一些需要更多自定义监控指标需求的项目。</p><pre><code> **我要纳入监控的cannal项目，就属于集成到应用中的，直接访问http://ip:port即可拿到监控数据，不要部署代理Exporter程序。**</code></pre></li></ul><h3 id="2、部署Prometheus"><a href="#2、部署Prometheus" class="headerlink" title="2、部署Prometheus"></a>2、部署Prometheus</h3><h4 id="2-1-下载"><a href="#2-1-下载" class="headerlink" title="2.1 下载"></a>2.1 下载</h4><pre><code class="bash">git clone https://github.com/prometheus-operator/kube-prometheus.gitcd kube-prometheus</code></pre><h4 id="2-2-部署"><a href="#2-2-部署" class="headerlink" title="2.2 部署"></a>2.2 部署</h4><pre><code class="bash"># Create the namespace and CRDs, and then wait for them to be available before creating the remaining resourceskubectl apply --server-side -f manifests/setupuntil kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo &quot;&quot;; donekubectl apply -f manifests/</code></pre><p>部署成功后，结果如下（如果部署失败，可手动想办法更换镜像地址）：</p><pre><code class="bash">root@k8s-master001:~# kubectl get pods -n monitoringNAME                                  READY   STATUS    RESTARTS   AGEalertmanager-main-0                   2/2     Running   0          3d1halertmanager-main-1                   2/2     Running   0          3d1halertmanager-main-2                   2/2     Running   0          3d1hblackbox-exporter-5cb5d7479d-cqxrd    3/3     Running   0          3d1hgrafana-789bc4b4b8-wjk4j              1/1     Running   0          3d1hkube-state-metrics-79f478884f-w6mqw   3/3     Running   0          3d1hnode-exporter-cl42r                   2/2     Running   0          3d1hnode-exporter-f8n6n                   2/2     Running   0          3d1hnode-exporter-m4xqr                   2/2     Running   0          3d1hprometheus-adapter-7bf7ff5b67-gph5t   1/1     Running   0          3d1hprometheus-adapter-7bf7ff5b67-pppq2   1/1     Running   0          3d1hprometheus-k8s-0                      2/2     Running   0          3d1hprometheus-k8s-1                      2/2     Running   0          3d1hprometheus-operator-b998f8597-k75bm   2/2     Running   0          3d1h</code></pre><p>将Prometheus映射出去，可以在局域网（公网）访问：</p><pre><code class="bash">#!/bin/bashnohup kubectl --namespace monitoring port-forward --address 0.0.0.0 svc/prometheus-k8s 9090:9090 &amp;</code></pre><p>之后可以通过 http://内网IP:9090 访问Prometheus的dashboard。</p><h4 id="2-3-删除（根据需求）"><a href="#2-3-删除（根据需求）" class="headerlink" title="2.3 删除（根据需求）"></a>2.3 删除（根据需求）</h4><p><code>kubectl delete --ignore-not-found=true -f manifests/ -f manifests/setup</code></p><h3 id="3、添加target并应用到集群"><a href="#3、添加target并应用到集群" class="headerlink" title="3、添加target并应用到集群"></a>3、添加target并应用到集群</h3><h4 id="3-1-增加target"><a href="#3-1-增加target" class="headerlink" title="3.1 增加target"></a>3.1 增加target</h4><p>编写需要增加的target添加到prometheus-additional.yaml 文件里面，不存在就新建。</p><pre><code class="bash">cat &gt;&gt; ./kube-prometheus/manifests/prometheus-additional.yaml  &lt;&lt; EOF- job_name: &#39;canal&#39;  scrape_interval: 15s  scrape_timeout: 15s  static_configs:  - targets: [&#39;10.31.4.225:11112&#39;]EOF</code></pre><p>注释：根据实际情况，修改job_name和targets变量即可。</p><h4 id="3-2-创建新的secret并应用到prometheus"><a href="#3-2-创建新的secret并应用到prometheus" class="headerlink" title="3.2 创建新的secret并应用到prometheus"></a>3.2 创建新的secret并应用到prometheus</h4><pre><code class="bash">cd ./kube-prometheus/manifests/# 创建secretkubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus-additional.yaml --dry-run=client -o yaml &gt; additional-scrape-configs.yaml# 应用到prometheuskubectl apply -f additional-scrape-configs.yaml -n monitoring</code></pre><h4 id="3-3-将target应用到集群"><a href="#3-3-将target应用到集群" class="headerlink" title="3.3 将target应用到集群"></a>3.3 将target应用到集群</h4><p>这一步类似于修改正常部署模式下的prometheus.yaml配置文件。</p><pre><code class="bash">vim kube-prometheus/manifests/prometheus-prometheus.yaml  additionalScrapeConfigs:    name: additional-configs    key: prometheus-additional.yaml</code></pre><p>具体添加位置如下：</p><p><a href="https://imgtu.com/i/L0NIDe"><img src="https://s1.ax1x.com/2022/04/19/L0NIDe.png" alt="L0NIDe.png"></a></p><p>应用变更到K8S生效：</p><pre><code class="bash">kubectl apply -f kube-prometheus/manifests/prometheus-prometheus.yaml -n monitoring</code></pre><h4 id="4、验证"><a href="#4、验证" class="headerlink" title="4、验证"></a>4、验证</h4><p>稍等片刻，查看prometheus的target列表即可，或者prometheus–&gt; Status–&gt;Configuration 中可以搜到job_name为canal的配置信息。</p><p><a href="https://imgtu.com/i/L0Urxf"><img src="https://s1.ax1x.com/2022/04/19/L0Urxf.png" alt="L0Urxf.png"></a></p><p>同时，我们上一步添加的Secret可以通过K8s集群查看到。</p><p><strong>以上的修改和添加，在Prometheus Operator项目的example/<a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs">additional-scrape-configs</a>中有一样的例子可供参考。</strong></p><h2 id="三、部署步骤（阿里云Prometheus）"><a href="#三、部署步骤（阿里云Prometheus）" class="headerlink" title="三、部署步骤（阿里云Prometheus）"></a>三、部署步骤（阿里云Prometheus）</h2><h3 id="1、部署"><a href="#1、部署" class="headerlink" title="1、部署"></a>1、部署</h3><p>需要监控部署在K8s集群之外的业务数据，如Redis连接数。操作步骤如下：</p><ol><li><p>在阿里云Prometheus监控服务控制台页面，选择相应地区的Prometheus实例。</p></li><li><p>在设置页面，单击<strong>Prometheus设置</strong>页签。</p></li><li><p>在Prometheus.yaml中输入以下内容，然后单击保存。</p><pre><code class="yaml">global:  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.scrape_configs:  - job_name: &#39;canal&#39;    static_configs:    - targets: [&#39;10.31.4.225:11112&#39;]</code></pre></li></ol><h3 id="2、验证"><a href="#2、验证" class="headerlink" title="2、验证"></a>2、验证</h3><p>在阿里云Prometheus监控服务页面，选择某个地区对应的实例后，选择客户端接入后，即可看到我们自定义接入的target。</p><p><a href="https://imgtu.com/i/L0wFPA"><img src="https://s1.ax1x.com/2022/04/19/L0wFPA.png" alt="L0wFPA.png"></a></p><h2 id="四、持久化数据存储"><a href="#四、持久化数据存储" class="headerlink" title="四、持久化数据存储"></a>四、持久化数据存储</h2><h3 id="4-1-根据需要调整持久化存储策略"><a href="#4-1-根据需要调整持久化存储策略" class="headerlink" title="4.1 根据需要调整持久化存储策略"></a>4.1 根据需要调整持久化存储策略</h3><ul><li><p>grafana和prometheus的数据都需要做持久存储。</p></li><li><p>grafana持久持久存储是因为可能安装些第三方的插件。</p></li><li><p>prometheus的持久存储，自不必多说，那是肯定需要的。</p></li></ul><p>我们部署在云上，持久化存储这块，可以很方便的使用他们提供的服务（NFS、OSS、aliyun-disk都可以）</p><h2 id="五、参考"><a href="#五、参考" class="headerlink" title="五、参考"></a>五、参考</h2><ul><li><p><a href="https://github.com/prometheus-operator/prometheus-operator">prometheus-operator项目</a></p></li><li><p><a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md">添加自定义监控配置官方文档 Additional Scrape Configuration</a></p></li><li><p><a href="https://github.com/alibaba/canal/wiki/Prometheus-QuickStart">canal项目官方文档</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;本文档题目也可叫做，&lt;strong&gt;kube-prometheus添加target&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;公司有</summary>
      
    
    
    
    <category term="虚拟化" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    <category term="k8s" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/k8s/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
    <category term="Prometheus" scheme="https://www.langxw.com/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>自建k8s使用阿里云存储插件csi</title>
    <link href="https://www.langxw.com/2022/03/22/%E8%87%AA%E5%BB%BAk8s%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%AD%98%E5%82%A8%E6%8F%92%E4%BB%B6csi/"/>
    <id>https://www.langxw.com/2022/03/22/%E8%87%AA%E5%BB%BAk8s%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%AD%98%E5%82%A8%E6%8F%92%E4%BB%B6csi/</id>
    <published>2022-03-22T05:01:45.000Z</published>
    <updated>2022-04-19T06:17:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>k8s在阿里云上使用，推荐还是使用阿里云的ack。无论是从稳定性，以及后期的维护来说，ack 都是最优的方案，自建k8s 阿里云不提供任何帮助，一旦集群出现问题（主要是网络问题）很难进行排查。</p><p>如果出于特殊原因，希望在阿里云上进行自建k8s当然也可以。<strong>自建k8s需要集成阿里云两个重要的插件cloud-controller-manager（负载均衡插件），csi(存储插件)。</strong>下面只介绍csi 存储插安装。</p><h2 id="二、部署"><a href="#二、部署" class="headerlink" title="二、部署"></a>二、部署</h2><h3 id="1、准备操作："><a href="#1、准备操作：" class="headerlink" title="1、准备操作："></a>1、准备操作：</h3><h4 id="1-1-自建k8s集群添加节点标签"><a href="#1-1-自建k8s集群添加节点标签" class="headerlink" title="1.1 自建k8s集群添加节点标签"></a>1.1 自建k8s集群添加节点标签</h4><ul><li>手动添加</li></ul><pre><code class="shell"># 获取providerIDMETA_EP=http://100.100.100.200/latest/meta-data &amp;&amp;echo `curl -s $META_EP/region-id`.`curl -s $META_EP/instance-id`# 打标签，注意修改node节点名称和providerIDkubectl patch node master1 -p &#39;&#123;&quot;spec&quot;:&#123;&quot;providerID&quot;: &quot;cn-zhangjiakou.i-8vbhy24ntae8zwo8zudn&quot;&#125;&#125;&#39;kubectl patch node master2 -p &#39;&#123;&quot;spec&quot;:&#123;&quot;providerID&quot;: &quot;cn-zhangjiakou.i-8vbhy24ntae8zwo8zudo&quot;&#125;&#125;&#39;kubectl patch node master3 -p &#39;&#123;&quot;spec&quot;:&#123;&quot;providerID&quot;: &quot;cn-zhangjiakou.i-8vbhy24ntae8zwo8zudr&quot;&#125;&#125;&#39;</code></pre><h4 id="1-2-配置CSI组建的RAM权限"><a href="#1-2-配置CSI组建的RAM权限" class="headerlink" title="1.2 配置CSI组建的RAM权限"></a>1.2 配置CSI组建的RAM权限</h4><ul><li><p>创建RAM用户 </p></li><li><p>创建并给予RAM用户自定义权限</p><pre><code class="json">&#123;    &quot;Version&quot;: &quot;1&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Action&quot;: [                &quot;ecs:AttachDisk&quot;,                &quot;ecs:DetachDisk&quot;,                &quot;ecs:DescribeDisks&quot;,                &quot;ecs:CreateDisk&quot;,                &quot;ecs:ResizeDisk&quot;,                &quot;ecs:CreateSnapshot&quot;,                &quot;ecs:DeleteSnapshot&quot;,                &quot;ecs:CreateAutoSnapshotPolicy&quot;,                &quot;ecs:ApplyAutoSnapshotPolicy&quot;,                &quot;ecs:CancelAutoSnapshotPolicy&quot;,                &quot;ecs:DeleteAutoSnapshotPolicy&quot;,                &quot;ecs:DescribeAutoSnapshotPolicyEX&quot;,                &quot;ecs:ModifyAutoSnapshotPolicyEx&quot;,                &quot;ecs:AddTags&quot;,                &quot;ecs:DescribeTags&quot;,                &quot;ecs:DescribeSnapshots&quot;,                &quot;ecs:ListTagResources&quot;,                &quot;ecs:TagResources&quot;,                &quot;ecs:UntagResources&quot;,                &quot;ecs:ModifyDiskSpec&quot;,                &quot;ecs:CreateSnapshot&quot;,                &quot;ecs:DeleteDisk&quot;,                &quot;ecs:DescribeInstanceAttribute&quot;,                &quot;ecs:DescribeInstances&quot;            ],            &quot;Resource&quot;: [                &quot;*&quot;            ],            &quot;Effect&quot;: &quot;Allow&quot;        &#125;,        &#123;            &quot;Action&quot;: [                &quot;nas:DescribeFileSystems&quot;,                &quot;nas:DescribeMountTargets&quot;,                &quot;nas:AddTags&quot;,                &quot;nas:DescribeTags&quot;,                &quot;nas:RemoveTags&quot;,                &quot;nas:CreateFileSystem&quot;,                &quot;nas:DeleteFileSystem&quot;,                &quot;nas:ModifyFileSystem&quot;,                &quot;nas:CreateMountTarget&quot;,                &quot;nas:DeleteMountTarget&quot;,                &quot;nas:ModifyMountTarget&quot;,                &quot;nas:TagResources&quot;,                &quot;nas:SetDirQuota&quot;,                &quot;nas:EnableRecycleBin&quot;,                &quot;nas:GetRecycleBinAttribute&quot;            ],            &quot;Resource&quot;: [                &quot;*&quot;            ],            &quot;Effect&quot;: &quot;Allow&quot;        &#125;,        &#123;            &quot;Action&quot;: [                &quot;oss:PutBucket&quot;,                &quot;oss:GetObjectTagging&quot;,                &quot;oss:ListBuckets&quot;,                &quot;oss:PutBucketTags&quot;,                &quot;oss:GetBucketTags&quot;,                &quot;oss:PutBucketEncryption&quot;,                &quot;oss:GetBucketInfo&quot;            ],            &quot;Resource&quot;: [                &quot;*&quot;            ],            &quot;Effect&quot;: &quot;Allow&quot;        &#125;    ]&#125;</code></pre></li><li><p>为RAM用户创建AccessKsy。</p></li></ul><h3 id="2、安装CSI组件"><a href="#2、安装CSI组件" class="headerlink" title="2、安装CSI组件"></a>2、安装CSI组件</h3><p>  <strong>下载ACK相关组件，支持块存储、NAS和OSS</strong></p><h4 id="2-1-配置AK"><a href="#2-1-配置AK" class="headerlink" title="2.1 配置AK"></a>2.1 配置AK</h4><pre><code class="shell">kubectl -n kube-system create secret generic alibaba-addon-secret --from-literal=&#39;access-key-id=xxxxx&#39; --from-literal=&#39;access-key-secret=xxxxx&#39;</code></pre><h4 id="2-2-CSI插件下载"><a href="#2-2-CSI插件下载" class="headerlink" title="2.2 CSI插件下载"></a>2.2 CSI插件下载</h4><ul><li><p>阿里云csi插件<a href="https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver">地址</a>： <code>git clone https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver.git</code></p></li><li><p>rbac.yaml，csi-plugin.yaml，csi-provisioner.yaml 三个文件都需要。</p><pre><code class="shell">wget https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/master/deploy/rbac.yamlwget https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/master/deploy/ack/csi-plugin.yamlhttps://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/master/deploy/ack/csi-provisioner.yaml</code></pre><h4 id="2-3-编辑配置"><a href="#2-3-编辑配置" class="headerlink" title="2.3 编辑配置"></a>2.3 编辑配置</h4><p>在csi-plugin.yaml、csi-provisioner.yaml中添加如下env参数</p><pre><code class="yaml">          - name: ACCESS_KEY_ID            valueFrom:              secretKeyRef:                key: access-key-id                name: alibaba-addon-secret          - name: ACCESS_KEY_SECRET            valueFrom:              secretKeyRef:                key: access-key-secret                name: alibaba-addon-secret</code></pre><h4 id="2-4-部署rbac-yaml"><a href="#2-4-部署rbac-yaml" class="headerlink" title="2.4 部署rbac.yaml"></a>2.4 部署rbac.yaml</h4></li><li><p>部署 <code>kubectl  apply -f rbac.yaml</code></p></li><li><p>查看生成的 secrets 的csi token 名字 <code>kubectl get secrets -A |grep csi</code></p></li><li><p>替换两个csi*.yaml文件中secretName为上一步sc的名字，比如csi-admin-token-ssflh</p><h4 id="2-5-部署CSI"><a href="#2-5-部署CSI" class="headerlink" title="2.5 部署CSI"></a>2.5 部署CSI</h4><pre><code class="shell">kubectl apply -f csi-plugin.yamlkubectl apply -f csi-provisioner.yaml</code></pre><h4 id="2-6-查看部署情况"><a href="#2-6-查看部署情况" class="headerlink" title="2.6 查看部署情况"></a>2.6 查看部署情况</h4><p><code>kubectl get pods -A |grep csi</code></p></li></ul><h2 id="三、使用"><a href="#三、使用" class="headerlink" title="三、使用"></a>三、使用</h2><h3 id="1、创建StorageClass"><a href="#1、创建StorageClass" class="headerlink" title="1、创建StorageClass"></a>1、创建StorageClass</h3><p>SC已在csi-provisioner.yaml中创建成功。</p><h3 id="2、创建PVC"><a href="#2、创建PVC" class="headerlink" title="2、创建PVC"></a>2、创建PVC</h3><pre><code class="yaml">apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: aliyun-csi-pvcspec:  accessModes:    - ReadWriteOnce  storageClassName: alicloud-disk-essd   resources:    requests:      storage: 20Gi</code></pre><p>然后执行<code>kubectl apply -f pvc.yaml</code></p><h3 id="3、创建Pod"><a href="#3、创建Pod" class="headerlink" title="3、创建Pod"></a>3、创建Pod</h3><pre><code class="yaml">apiVersion: v1 kind: Podmetadata:  name: nginx-aliyun-csispec:  containers:    - name: webserver      image: nginx      volumeMounts:        - name: mypvc          mountPath: /usr/share/nginx/html  volumes:    - name: mypvc      persistentVolumeClaim:        claimName: aliyun-csi-pvc        readOnly: false</code></pre><p>然后执行<code>kubectl apply -f nginx-pod.yaml</code></p><h3 id="4、查看创建情况"><a href="#4、查看创建情况" class="headerlink" title="4、查看创建情况"></a>4、查看创建情况</h3><pre><code class="she"># 查看pvc创建和绑定情况kubectl get pvc # 查看pod创建情况kubectl get pod</code></pre><h3 id="5、验证"><a href="#5、验证" class="headerlink" title="5、验证"></a>5、验证</h3><ul><li>登录阿里云控制台查看云盘创建和挂载情况</li><li>登录nginx-pod用<code>df -h &amp;&amp; fdisk -l &amp;&amp; mount</code> 等命令查看阿里云盘挂载情况</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;k8s在阿里云上使用，推荐还是使用阿里云的ack。无论是从稳定性，以及后期的维护来说，ack 都是最优的方案，自建k8s 阿</summary>
      
    
    
    
    <category term="虚拟化" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    <category term="k8s" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/k8s/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
    <category term="csi-plugin" scheme="https://www.langxw.com/tags/csi-plugin/"/>
    
  </entry>
  
  <entry>
    <title>kubelet命令自动补全功能</title>
    <link href="https://www.langxw.com/2022/03/02/kubelet%E5%91%BD%E4%BB%A4%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8%E5%8A%9F%E8%83%BD/"/>
    <id>https://www.langxw.com/2022/03/02/kubelet%E5%91%BD%E4%BB%A4%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8%E5%8A%9F%E8%83%BD/</id>
    <published>2022-03-02T09:22:57.000Z</published>
    <updated>2022-03-02T09:28:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>我们在管理k8s集群的时候，避免不了使用kubectl命令工具，但是该命令还是挺复杂的，使用中也记不住那么多的api选项，故这里介绍一下kubectl命令补全工具的安装。</p><h2 id="二、实现"><a href="#二、实现" class="headerlink" title="二、实现"></a>二、实现</h2><h3 id="1、安装bash-completion"><a href="#1、安装bash-completion" class="headerlink" title="1、安装bash-completion"></a>1、安装bash-completion</h3><pre><code class="bash">yum install -y bash-completion source /usr/share/bash-completion/bash_completion</code></pre><h3 id="2、应用kubectl的completion到系统环境"><a href="#2、应用kubectl的completion到系统环境" class="headerlink" title="2、应用kubectl的completion到系统环境"></a>2、应用kubectl的completion到系统环境</h3><pre><code class="bash">source &lt;(kubectl completion bash)echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;我们在管理k8s集群的时候，避免不了使用kubectl命令工具，但是该命令还是挺复杂的，使用中也记不住那么多的api选项，故</summary>
      
    
    
    
    <category term="虚拟化" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    <category term="k8s" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/k8s/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
    <category term="kubelet" scheme="https://www.langxw.com/tags/kubelet/"/>
    
  </entry>
  
  <entry>
    <title>K8S启用非安全内核参数</title>
    <link href="https://www.langxw.com/2022/02/21/K8S%E5%90%AF%E7%94%A8%E9%9D%9E%E5%AE%89%E5%85%A8%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0/"/>
    <id>https://www.langxw.com/2022/02/21/K8S%E5%90%AF%E7%94%A8%E9%9D%9E%E5%AE%89%E5%85%A8%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0/</id>
    <published>2022-02-21T10:03:19.000Z</published>
    <updated>2022-04-19T06:17:42.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p>我司从云主机向k8s迁移的过程中，由于在云主机上对内核参数进行过优化，所以尽量想K8s也和云主机的内存参数保持一致。</p><h2 id="二、解决办法："><a href="#二、解决办法：" class="headerlink" title="二、解决办法："></a>二、解决办法：</h2><p><strong>k8s并不是支持所有的linux的内核参数，具体支持情况看k8s集群环境和参考<a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/sysctl-cluster/">官方文档</a>。</strong></p><p><strong>实际设定还是结合产品自身用以下方式进行操作了，原则就是pod镜像系统里面进去看没有的就不能开启了，具体进容器里面看一下/proc/sys/的文件信息。</strong></p><h3 id="1、设置集群的PSP"><a href="#1、设置集群的PSP" class="headerlink" title="1、设置集群的PSP"></a>1、设置集群的PSP</h3><h4 id="1-1-解释"><a href="#1-1-解释" class="headerlink" title="1.1 解释"></a>1.1 解释</h4><p>可以通过在 PodSecurityPolicy 的 <code>forbiddenSysctls</code> 或 <code>allowedUnsafeSysctls</code> 字段中，指定sysctl 或填写 sysctl 匹配模式来进一步为 Pod 设置 sysctl 参数。 sysctl 参数匹配模式以 <code>*</code> 字符结尾，如 <code>kernel.*</code>。 单独的 <code>*</code> 字符匹配所有 sysctl 参数。</p><p>以下示例设置启用了以 <code>kernel.msg</code> 为前缀的非安全的 sysctl 参数，同时禁用了 sysctl 参数 <code>kernel.shm_rmid_forced</code>。</p><pre><code class="yaml">apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata:  name: sysctl-pspspec:  allowedUnsafeSysctls:  - kernel.msg*  forbiddenSysctls:  - kernel.shm_rmid_forced ...</code></pre><h4 id="1-2-具体操作"><a href="#1-2-具体操作" class="headerlink" title="1.2 具体操作"></a>1.2 具体操作</h4><ol><li><p>通过<code>kubectl describe PodSecurityPolicy  </code>查询psp内容，看是否有内核参数的相关内容</p></li><li><p>通过<code>kubectl edit PodSecurityPolicy  </code> 修改psp内容，加入allowed的内核参数的配置</p></li><li><p>再次查询psp的内容，确认修改是否生效。</p></li></ol><h3 id="2、启用非安全的sysctl参数"><a href="#2、启用非安全的sysctl参数" class="headerlink" title="2、启用非安全的sysctl参数"></a>2、启用非安全的sysctl参数</h3><h4 id="2-1-解释"><a href="#2-1-解释" class="headerlink" title="2.1 解释"></a>2.1 解释</h4><p><strong>所有的安全sysctl参数都默认启用。</strong></p><p>集群管理员只有在一些非常特殊的情况下（如：高可用或实时应用调整）， 才可以启用特定的 <em>非安全的</em> sysctl 参数。 如需启用 <em>非安全的</em> sysctl 参数，请你在每个节点上分别设置 kubelet 命令行参数。例如：</p><pre><code class="shell">kubelet --allowed-unsafe-sysctls \  &#39;kernel.msg*,net.core.somaxconn&#39; ...</code></pre><h4 id="2-2-具体操作"><a href="#2-2-具体操作" class="headerlink" title="2.2 具体操作"></a>2.2 具体操作</h4><ol><li><p>在所有的node节点上找到kubelet启动的配置文件，比如<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></p></li><li><p>在ExecStart=后面添加启动非安全内存参数的配置<code>--allowed-unsafe-sysctls=net.core.somaxconn</code></p><pre><code>[Service]Environment=&quot;KUBELET_EXTRA_ARGS=--node-labels=alibabacloud.com/nodepool-&quot;.............................................ExecStart=ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS $KUBELET_CUSTOMIZED_ARGS  --allowed-unsafe-sysctls=net.core.somaxconn,kernel.msgmax,net.ipv4.*,net.core.netdev_max_backlog,net.nf_conntrack_max,net.netfilter.nf_conntrack_tcp_timeout_established</code></pre></li><li><p>重启kubelet，<code>systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code>。若启动不成功，请查看日志</p></li><li><p>ps -ef|grep kubelet 确认参数已生效</p><pre><code>/usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods 256 --pod-max-pids 16384 --pod-manifest-path=/etc/kubernetes/manifests --feature-gates=IPv6DualStack=true --network-plugin=cni .................................--system-reserved=cpu=100m,memory=1280Mi --kube-reserved=cpu=100m,memory=1280Mi --kube-reserved=pid=1000 --system-reserved=pid=1000 --allowed-unsafe-sysctls=net.core.somaxconn,kernel.msgmax,net.ipv4.*,net.core.netdev_max_backlog,net.nf_conntrack_max,net.netfilter.nf_conntrack_tcp_timeout_established</code></pre></li></ol><h3 id="3、设置Pod的Sysctl参数"><a href="#3、设置Pod的Sysctl参数" class="headerlink" title="3、设置Pod的Sysctl参数"></a>3、设置Pod的Sysctl参数</h3><pre><code class="yaml">apiVersion: v1kind: Podmetadata:  name: sysctl-examplespec:  securityContext:    sysctls:    - name: kernel.shm_rmid_forced      value: &quot;0&quot;    - name: net.core.somaxconn      value: &quot;1024&quot;    - name: kernel.msgmax      value: &quot;65536&quot;  ...</code></pre><p><strong>注意：deployment的sysctl参数设置在spec.template.spec.securityContext.sysctls</strong></p><h2 id="三、其他解决方案"><a href="#三、其他解决方案" class="headerlink" title="三、其他解决方案"></a>三、其他解决方案</h2><h3 id="1、使用-initContainers"><a href="#1、使用-initContainers" class="headerlink" title="1、使用 initContainers"></a>1、使用 initContainers</h3><p>如果希望设置内核参数更简单通用，可以在 initContainer 中设置，不过这个要求给 initContainer 打开 <code>privileged</code> 权限。</p><pre><code class="yaml">apiVersion: v1kind: Podmetadata:  name: sysctl-example-initspec:  initContainers:  - image: busybox    command:    - sh    - -c    - |      sysctl -w net.core.somaxconn=65535      sysctl -w net.ipv4.ip_local_port_range=&quot;1024 65535&quot;      sysctl -w net.ipv4.tcp_tw_reuse=1      sysctl -w fs.file-max=1048576    imagePullPolicy: Always    name: setsysctl    securityContext:      privileged: true  containers:  ......</code></pre><h3 id="2、使用-tuning-CNI-插件统一设置-sysctl"><a href="#2、使用-tuning-CNI-插件统一设置-sysctl" class="headerlink" title="2、使用 tuning CNI 插件统一设置 sysctl"></a>2、使用 tuning CNI 插件统一设置 sysctl</h3><p>如果想要为所有 Pod 统一配置某些内核参数，可以使用 <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning">tuning</a> 这个 CNI 插件来做:</p><pre><code class="json">&#123;  &quot;name&quot;: &quot;mytuning&quot;,  &quot;type&quot;: &quot;tuning&quot;,  &quot;sysctl&quot;: &#123;          &quot;net.core.somaxconn&quot;: &quot;500&quot;,          &quot;net.ipv4.tcp_tw_reuse&quot;: &quot;1&quot;  &#125;&#125;</code></pre><h2 id="四、参考文档"><a href="#四、参考文档" class="headerlink" title="四、参考文档"></a>四、参考文档</h2><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">Using sysctls in a Kubernetes Cluster</a></li><li><a href="https://www.cni.dev/plugins/current/meta/tuning/">tuning 插件文档</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;我司从云主机向k8s迁移的过程中，由于在云主机上对内核参数进行过优化，所以尽量想K8s也和云主机的内存参数保持一致。&lt;/p&gt;</summary>
      
    
    
    
    <category term="虚拟化" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    <category term="k8s" scheme="https://www.langxw.com/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/k8s/"/>
    
    
    <category term="k8s" scheme="https://www.langxw.com/tags/k8s/"/>
    
    <category term="sysctl" scheme="https://www.langxw.com/tags/sysctl/"/>
    
  </entry>
  
  <entry>
    <title>统计15天内阿里云SLB后端异常和并发指标</title>
    <link href="https://www.langxw.com/2021/12/01/%E7%BB%9F%E8%AE%A115%E5%A4%A9%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91SLB%E5%90%8E%E7%AB%AF%E5%BC%82%E5%B8%B8%E5%92%8C%E5%B9%B6%E5%8F%91%E6%8C%87%E6%A0%87/"/>
    <id>https://www.langxw.com/2021/12/01/%E7%BB%9F%E8%AE%A115%E5%A4%A9%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91SLB%E5%90%8E%E7%AB%AF%E5%BC%82%E5%B8%B8%E5%92%8C%E5%B9%B6%E5%8F%91%E6%8C%87%E6%A0%87/</id>
    <published>2021-12-01T03:00:46.000Z</published>
    <updated>2021-12-01T03:04:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p><strong>本脚本是阿里云资源利用率定期统计方案中的其中一个脚本。</strong></p><p>本脚本可实现，从每天95个平均值数据中取平均值，15天15个中位数取平均值，得到最终的15天内CPU和内存使用率数值。</p><p>进而根据阈值进行判断，资源是否处于低利用率状态。</p><h2 id="二、环境"><a href="#二、环境" class="headerlink" title="二、环境"></a>二、环境</h2><p><strong>Python3.7 + 阿里云云监控SDK + 阿里云SLB的SDK + pandas + numpy</strong></p><h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><pre><code class="bash"># -*- coding: utf-8 -*-import timeimport numpyimport jsonimport pandas as pdfrom typing import Listfrom alibabacloud_slb20140515.client import Client as Slb20140515Clientfrom alibabacloud_tea_openapi import models as open_api_modelsfrom alibabacloud_slb20140515 import models as slb_20140515_modelsfrom alibabacloud_cms20190101.client import Client as Cms20190101Clientfrom alibabacloud_cms20190101 import models as cms_20190101_models# cn-zhangjiakou 张家口区域的SLB在其他区域的config查不到，只能用张家口configREGION_ID = [&#39;cn-beijing&#39;, &#39;cn-hangzhou&#39;, &#39;cn-hongkong&#39;]PAGE_SIZE = 100class GetMonitorData:    def __init__(self):        pass    @staticmethod    def create_client(        access_key_id: str,        access_key_secret: str,    ) -&gt; Cms20190101Client:        config = open_api_models.Config(            access_key_id=&#39;xxxxxxxx&#39;,            access_key_secret=&#39;xxxxxxxxxxxx&#39;        )        config.endpoint = &#39;metrics.cn-hangzhou.aliyuncs.com&#39;        return Cms20190101Client(config)    @staticmethod    def main(        args: List[str],    ) -&gt; None:        client = GetMonitorData.create_client(&#39;acessKeyId&#39;, &#39;accessKeySecret&#39;)        describe_metric_list_request = cms_20190101_models.DescribeMetricListRequest(            metric_name=args[1],            namespace=args[0],            period=&#39;900&#39;,            start_time=args[2],            end_time=args[3],            length=&#39;100&#39;,            dimensions=&#39;&#123;&#123;"instanceId":&#123;&#125;&#125;&#125;&#39;.format(args[4]),            next_token=args[5]        )        res = client.describe_metric_list(describe_metric_list_request)        return res.bodyclass GetInstanceIdName:    def __init__(self):        pass    @staticmethod    def create_client(        access_key_id: str,        access_key_secret: str,    ) -&gt; Slb20140515Client:        config = open_api_models.Config(            access_key_id=&#39;xxxxxxxxxxx&#39;,            access_key_secret=&#39;xxxxxxxxxxxxx&#39;        )        # 访问的域名        config.endpoint = &#39;slb.aliyuncs.com&#39;        return Slb20140515Client(config)    @staticmethod    def main(        args: List[str],    ) -&gt; None:        client = GetInstanceIdName.create_client(&#39;accessKeyId&#39;, &#39;accessKeySecret&#39;)        describe_load_balancers_request = slb_20140515_models.DescribeLoadBalancersRequest(            region_id=args[0],            page_size=PAGE_SIZE,            page_number=args[1]        )        res = client.describe_load_balancers(describe_load_balancers_request)        return res.bodydef get_id_name_dict():    instance_dict = &#123;&#125;    for i in range(0, len(REGION_ID)):        result = GetInstanceIdName.main([REGION_ID[i], 1])        total_page = result.total_count // PAGE_SIZE + 1        for j in range(0, total_page):            result2 = GetInstanceIdName.main([REGION_ID[i], j+1])            slb_info = result2.load_balancers.load_balancer            for k in range(0, len(slb_info)):                instance_dict[slb_info[k].load_balancer_id] = slb_info[k].load_balancer_name    print(&#39;实例ID和名字的字典:&#39;, instance_dict)    return instance_dictdef get_average_24h(instance_dict, pre_days, metric_name):    average_dict = &#123;&#125;    today = time.strftime(&#39;%Y-%m-%d&#39;, time.localtime(time.time()))    today_time = time.mktime(time.strptime(today, &#39;%Y-%m-%d&#39;))    # 从昨天开始，往前推15天，15次循环,取1-15。取00:00:00-23:59:59的时间戳    start_time = str(round((today_time - 86400*pre_days)*1000))    end_time = str(round((today_time - 86400*(pre_days-1) - 1)*1000))    namespace = &#39;acs_slb_dashboard&#39;    for i in instance_dict.keys():        token = &#39;init_data&#39;        average_list = []        while token:            result = GetMonitorData.main([namespace, metric_name, start_time, end_time, i, token])            token = result.next_token            res_list = json.loads(result.datapoints)            # 停机和未安装监控agent的主机拿不到监控数据，res_list是个空列表，计算平均是会报错            if len(res_list) != 0:                for j in range(0, len(res_list)):                    # print(res_list[i])                    average_list.append(round(res_list[j][&#39;Average&#39;], 2))            else:                average_list = [0.00, ]        # 取平均值        average_dict[i] = numpy.mean(average_list)        time.sleep(0.2)    return average_dictdef get_average_15days(instance_dict, metric):    temp_dict = &#123;&#125;    median_dict1 = get_average_24h(instance_dict, 1, metric)    for k, v in median_dict1.items():        temp_dict[k] = []    for k1, v1 in median_dict1.items():        temp_dict[k1].append(v1)    for i in range(2, 16):        median_dict = get_average_24h(instance_dict, i, metric)        for k2, v2 in median_dict.items():            temp_dict[k2].append(v2)    for k3, v3 in temp_dict.items():        temp_dict[k3] = round(numpy.mean(v3), 3)    print(temp_dict)    return temp_dictdef write_to_execl(data):    df = pd.DataFrame.from_dict(data, orient=&#39;index&#39;, columns=[&#39;后端异常ECS实例个数&#39;, &#39;实例每秒最大并发连接数&#39;])    df.to_excel(&#39;slb_statistics.xlsx&#39;)if __name__ == &#39;__main__&#39;:    # 获取InstanceId和InstanceName的对应字典    str_time = time.time()    instance_dict = get_id_name_dict()    # 数据合并处理    id_list_dict = &#123;&#125;    for k, v in instance_dict.items():        id_list_dict[k] = []    ecs_metric = [&#39;UnhealthyServerCount&#39;, &#39;InstanceMaxConnection&#39;]    for metric in ecs_metric:        metric_data = get_average_15days(instance_dict, metric)        for k2, v2 in metric_data.items():            id_list_dict[k2].append(v2)    for k_id, k_name in instance_dict.items():        if k_id in id_list_dict:            id_list_dict[k_name] = id_list_dict.pop(k_id)    # 数据写入excel表格    write_to_execl(id_list_dict)    print(time.time() - str_time)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;本脚本是阿里云资源利用率定期统计方案中的其中一个脚本。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本脚本可实现，从每天</summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="Python" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/Python/"/>
    
    
    <category term="Python" scheme="https://www.langxw.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>统计15天内阿里云ECS内存使用率和CPU使用率</title>
    <link href="https://www.langxw.com/2021/12/01/%E7%BB%9F%E8%AE%A115%E5%A4%A9%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91ECS%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E7%8E%87%E5%92%8CCPU%E4%BD%BF%E7%94%A8%E7%8E%87/"/>
    <id>https://www.langxw.com/2021/12/01/%E7%BB%9F%E8%AE%A115%E5%A4%A9%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91ECS%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E7%8E%87%E5%92%8CCPU%E4%BD%BF%E7%94%A8%E7%8E%87/</id>
    <published>2021-12-01T02:42:57.000Z</published>
    <updated>2021-12-01T02:59:02.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p><strong>本脚本是阿里云资源利用率定期统计方案中的其中一个脚本。</strong></p><p>本脚本可实现，从每天95个平均值数据中取中位数，15天15个中位数取平均值，得到最终的15天内CPU和内存使用率数值。</p><p>进而根据阈值进行判断，资源是否处于低利用率状态。</p><h2 id="二、环境"><a href="#二、环境" class="headerlink" title="二、环境"></a>二、环境</h2><p><strong>Python3.7 + 阿里云云监控SDK + 阿里云ECS的SDK + pandas + numpy</strong></p><h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><pre><code class="bash"># -*- coding: utf-7 -*-import timeimport numpyimport jsonimport pandas as pdfrom typing import Listfrom alibabacloud_cms20190101.client import Client as Cms20190101Clientfrom alibabacloud_tea_openapi import models as open_api_modelsfrom alibabacloud_cms20190101 import models as cms_20190101_modelsfrom alibabacloud_ecs20140526.client import Client as Ecs20140527Clientfrom alibabacloud_ecs20140526 import models as ecs_20140526_modelsREGION_ID = [&#39;cn-beijing&#39;, &#39;cn-hangzhou&#39;, &#39;cn-zhangjiakou&#39;, &#39;cn-shenzhen&#39;, &#39;cn-shanghai&#39;, &#39;cn-hongkong&#39;,             &#39;ap-southeast-1&#39;, &#39;ap-northeast-1&#39;]class GetMonitorData:    def __init__(self):        pass    @staticmethod    def create_client(        access_key_id: str,        access_key_secret: str,    ) -&gt; Cms20190101Client:        config = open_api_models.Config(            access_key_id=&#39;xxxxxxxxx&#39;,            access_key_secret=&#39;xxxxxxxxxxxxx&#39;        )        config.endpoint = &#39;metrics.cn-hangzhou.aliyuncs.com&#39;        return Cms20190101Client(config)    @staticmethod    def main(        args: List[str],    ) -&gt; None:        client = GetMonitorData.create_client(&#39;acessKeyId&#39;, &#39;accessKeySecret&#39;)        describe_metric_list_request = cms_20190101_models.DescribeMetricListRequest(            metric_name=args[1],            namespace=args[0],            period=&#39;900&#39;,            start_time=args[2],            end_time=args[3],            length=&#39;100&#39;,            dimensions=&#39;&#123;&#123;"instanceId":&#123;&#125;&#125;&#125;&#39;.format(args[4])        )        res = client.describe_metric_list(describe_metric_list_request)        return res.bodyclass GetInstanceIdName:    def __init__(self):        pass    @staticmethod    def create_client(        access_key_id: str,        access_key_secret: str,    ) -&gt; Ecs20140526Client:        config = open_api_models.Config(            access_key_id=&#39;xxxxxxxxxxxxxx&#39;,            access_key_secret=&#39;xxxxxxxxxxxxxxxxxxxx&#39;        )        config.endpoint = &#39;ecs-cn-hangzhou.aliyuncs.com&#39;        return Ecs20140526Client(config)    @staticmethod    def main(        args: List[str],    ) -&gt; None:        client = GetInstanceIdName.create_client(&#39;accessKeyId&#39;, &#39;accessKeySecret&#39;)        describe_instances_request = ecs_20140526_models.DescribeInstancesRequest(            region_id=args[1],            next_token=args[0],            max_results=50        )        res = client.describe_instances(describe_instances_request)        return res.bodydef get_id_name_dict():    instance_dict = &#123;&#125;    for i in range(0, len(REGION_ID)):        token = &#39;init_data&#39;        while token:            result = GetInstanceIdName.main([token, REGION_ID[i]])            token = result.next_token            info_list = result.instances.instance            for j in range(0, len(info_list)):                instance_dict[info_list[j].instance_id] = info_list[j].instance_name    print(&#39;实例ID和名字的字典:&#39;, instance_dict)    return instance_dictdef get_median_24h(instance_dict, pre_days, metric_name):    median_dict = &#123;&#125;    today = time.strftime(&#39;%Y-%m-%d&#39;, time.localtime(time.time()))    today_time = time.mktime(time.strptime(today, &#39;%Y-%m-%d&#39;))    # 从昨天开始，往前推15天，15次循环,取1-15。取00:00:00-23:59:59的时间戳    start_time = str(round((today_time - 86400*pre_days)*1000))    end_time = str(round((today_time - 86400*(pre_days-1) - 1)*1000))    namespace = &#39;acs_ecs_dashboard&#39;    for i in instance_dict.keys():        result = GetMonitorData.main([namespace, metric_name, start_time, end_time, i])        average_list = []        res_list = json.loads(result.datapoints)        # 停机和未安装监控agent的主机拿不到监控数据，res_list是个空列表，计算平均是会报错        if len(res_list) != 0:            for j in range(0, len(res_list)):                # print(res_list[i])                average_list.append(round(res_list[j][&#39;Average&#39;], 2))        else:            average_list = [0.00, ]        # print(len(average_list), average_list)        # 取中位数        median_dict[i] = numpy.median(average_list)        time.sleep(0.2)    return median_dictdef get_average_15days(instance_dict, metric):    temp_dict = &#123;&#125;    median_dict1 = get_median_24h(instance_dict, 1, metric)    for k, v in median_dict1.items():        temp_dict[k] = []    for k1, v1 in median_dict1.items():        temp_dict[k1].append(v1)    for i in range(2, 16):        median_dict = get_median_24h(instance_dict, i, metric)        for k2, v2 in median_dict.items():            temp_dict[k2].append(v2)    for k3, v3 in temp_dict.items():        temp_dict[k3] = round(numpy.mean(v3), 3)    print(temp_dict)    return temp_dictdef write_to_execl(data):    df = pd.DataFrame.from_dict(data, orient=&#39;index&#39;, columns=[&#39;CPU使用率&#39;, &#39;内存使用率&#39;])    df.to_excel(&#39;cpu_mem_15days.xlsx&#39;)if __name__ == &#39;__main__&#39;:    # 获取InstanceId和InstanceName的对应字典    # str_time = time.time()    instance_dict = get_id_name_dict()    # 数据合并处理，比较复杂    id_list_dict = &#123;&#125;    for k, v in instance_dict.items():        id_list_dict[k] = []    ecs_metric = [&#39;CPUUtilization&#39;, &#39;memory_usedutilization&#39;]    for metric in ecs_metric:        metric_data = get_average_15days(instance_dict, metric)        for k2, v2 in metric_data.items():            id_list_dict[k2].append(v2)    for k_id, k_name in instance_dict.items():        if k_id in id_list_dict:            id_list_dict[k_name] = id_list_dict.pop(k_id)    # 数据写入excel表格    write_to_execl(id_list_dict)    # print(time.time() - str_time)</code></pre><p><strong>注意：以上代码中24小时内的数据，是一个实例一个循环取的数据，调用接口次数多，耗时较长。经实践400个ECS，执行完本脚本要5个小时左右，可耐心等待。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;本脚本是阿里云资源利用率定期统计方案中的其中一个脚本。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本脚本可实现，从每天</summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="Python" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/Python/"/>
    
    
    <category term="python" scheme="https://www.langxw.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>统计阿里云ECS停机状态的主机</title>
    <link href="https://www.langxw.com/2021/12/01/%E7%BB%9F%E8%AE%A1%E9%98%BF%E9%87%8C%E4%BA%91ECS%E5%81%9C%E6%9C%BA%E7%8A%B6%E6%80%81%E7%9A%84%E4%B8%BB%E6%9C%BA/"/>
    <id>https://www.langxw.com/2021/12/01/%E7%BB%9F%E8%AE%A1%E9%98%BF%E9%87%8C%E4%BA%91ECS%E5%81%9C%E6%9C%BA%E7%8A%B6%E6%80%81%E7%9A%84%E4%B8%BB%E6%9C%BA/</id>
    <published>2021-12-01T02:32:24.000Z</published>
    <updated>2021-12-01T03:05:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、缘由"><a href="#一、缘由" class="headerlink" title="一、缘由"></a>一、缘由</h2><p><strong>本脚本是阿里云资源利用率定期统计方案中的其中一个脚本。</strong></p><p>公司阿里云ECS比较多，地区比较多，乃至有多个账号，一个个查看和统计比较费事费力。现编写一个统计阿里云ECS处于停机状态的主机的脚本，输出Excel文档后，我们可以区分，哪些还处于停机收费状态，哪些可以释放，哪些不可以释放。</p><h2 id="二、环境"><a href="#二、环境" class="headerlink" title="二、环境"></a>二、环境</h2><p>**Python3.7 + 阿里云ECS的SDK + pandas **</p><h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><pre><code class="bash"># -*- coding: utf-8 -*-from typing import Listimport pandas as pdfrom alibabacloud_tea_openapi import models as open_api_modelsfrom alibabacloud_ecs20140526.client import Client as Ecs20140526Clientfrom alibabacloud_ecs20140526 import models as ecs_20140526_modelsREGION_ID = [&#39;cn-beijing&#39;, &#39;cn-hangzhou&#39;, &#39;cn-zhangjiakou&#39;, &#39;cn-shenzhen&#39;, &#39;cn-shanghai&#39;, &#39;cn-hongkong&#39;,             &#39;ap-southeast-1&#39;, &#39;ap-northeast-1&#39;]class GetInstanceIdName:    def __init__(self):        pass    @staticmethod    def create_client(        access_key_id: str,        access_key_secret: str,    ) -&gt; Ecs20140526Client:        config = open_api_models.Config(            access_key_id=&#39;xxxxxxxxxx&#39;,            access_key_secret=&#39;xxxxxxxxxx&#39;        )        config.endpoint = &#39;ecs-cn-hangzhou.aliyuncs.com&#39;        return Ecs20140526Client(config)    @staticmethod    def main(        args: List[str],    ) -&gt; None:        client = GetInstanceIdName.create_client(&#39;accessKeyId&#39;, &#39;accessKeySecret&#39;)        describe_instances_request = ecs_20140526_models.DescribeInstancesRequest(            region_id=args[1],            next_token=args[0],            max_results=50        )        res = client.describe_instances(describe_instances_request)        return res.bodydef get_instance_status():    instance_dict = &#123;&#125;    for i in range(0, len(REGION_ID)):        token = &#39;init_data&#39;        while token:            result = GetInstanceIdName.main([token, REGION_ID[i]])            token = result.next_token            info_list = result.instances.instance            # print(dir(info_list[0]))            for j in range(0, len(info_list)):                if info_list[j].status == &#39;Stopped&#39;:                    instance_dict[info_list[j].instance_id] = []            for k in range(0, len(info_list)):                if info_list[k].status == &#39;Stopped&#39;:                    instance_dict[info_list[k].instance_id].append(info_list[k].instance_name)                    instance_dict[info_list[k].instance_id].append(info_list[k].status)                    instance_dict[info_list[k].instance_id].append(info_list[k].stopped_mode)                    instance_dict[info_list[k].instance_id].append(info_list[k].description)    return instance_dictdef write_to_execl(data):    df = pd.DataFrame.from_dict(data, orient=&#39;index&#39;, columns=[&#39;实例名称&#39;, &#39;实例状态&#39;, &#39;停机收费状态&#39;, &#39;实例备注&#39;])    df.to_excel(&#39;ecs_stopped.xlsx&#39;)if __name__ == &#39;__main__&#39;:    instance_info = get_instance_status()    write_to_execl(instance_info)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、缘由&quot;&gt;&lt;a href=&quot;#一、缘由&quot; class=&quot;headerlink&quot; title=&quot;一、缘由&quot;&gt;&lt;/a&gt;一、缘由&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;本脚本是阿里云资源利用率定期统计方案中的其中一个脚本。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;公司阿里云ECS比较</summary>
      
    
    
    
    <category term="脚本" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/"/>
    
    <category term="Python" scheme="https://www.langxw.com/categories/%E8%84%9A%E6%9C%AC/Python/"/>
    
    
    <category term="python" scheme="https://www.langxw.com/tags/python/"/>
    
  </entry>
  
</feed>
